Only in holmes2/holmes: __pycache__
Only in holmes2/holmes/clients: __pycache__
diff -ur baseline-holmes/holmes/clients/robusta_client.py holmes2/holmes/clients/robusta_client.py
--- baseline-holmes/holmes/clients/robusta_client.py	2025-11-05 16:43:28.202636864 -0800
+++ holmes2/holmes/clients/robusta_client.py	2025-10-17 15:09:28.614898043 -0700
@@ -1,5 +1,5 @@
 import logging
-from typing import Optional, Dict, Any
+from typing import List, Optional
 import requests  # type: ignore
 from functools import cache
 from pydantic import BaseModel, ConfigDict
@@ -14,31 +14,18 @@
     latest_version: Optional[str] = None
 
 
-class RobustaModel(BaseModel):
-    model_config = ConfigDict(extra="ignore")
-    model: str
-    holmes_args: Optional[dict[str, Any]] = None
-    is_default: bool = False
-
-
-class RobustaModelsResponse(BaseModel):
-    models: Dict[str, RobustaModel]
-
-
 @cache
-def fetch_robusta_models(
-    account_id: str, token: str
-) -> Optional[RobustaModelsResponse]:
+def fetch_robusta_models(account_id, token) -> Optional[List[str]]:
     try:
         session_request = {"session_token": token, "account_id": account_id}
         resp = requests.post(
-            f"{ROBUSTA_API_ENDPOINT}/api/llm/models/v2",
+            f"{ROBUSTA_API_ENDPOINT}/api/llm/models",
             json=session_request,
             timeout=10,
         )
         resp.raise_for_status()
         response_json = resp.json()
-        return RobustaModelsResponse(**{"models": response_json})
+        return response_json.get("models")
     except Exception:
         logging.exception("Failed to fetch robusta models")
         return None
Only in holmes2/holmes/common: __pycache__
diff -ur baseline-holmes/holmes/common/env_vars.py holmes2/holmes/common/env_vars.py
--- baseline-holmes/holmes/common/env_vars.py	2025-11-05 16:43:28.202865031 -0800
+++ holmes2/holmes/common/env_vars.py	2025-10-17 15:09:28.859266016 -0700
@@ -2,16 +2,6 @@
 import json
 from typing import Optional
 
-# Recommended models for different providers
-RECOMMENDED_OPENAI_MODEL = "gpt-4.1"
-RECOMMENDED_ANTHROPIC_MODEL = "anthropic/claude-opus-4-1-20250805"
-
-# Default model for HolmesGPT
-DEFAULT_MODEL = RECOMMENDED_OPENAI_MODEL
-FALLBACK_CONTEXT_WINDOW_SIZE = (
-    200000  # Fallback context window size if it can't be determined from the model
-)
-
 
 def load_bool(env_var, default: Optional[bool]) -> Optional[bool]:
     env_value = os.environ.get(env_var)
@@ -48,7 +38,6 @@
 SENTRY_DSN = os.environ.get("SENTRY_DSN", "")
 SENTRY_TRACES_SAMPLE_RATE = float(os.environ.get("SENTRY_TRACES_SAMPLE_RATE", "0.0"))
 
-EXTRA_HEADERS = os.environ.get("EXTRA_HEADERS", "")
 THINKING = os.environ.get("THINKING", "")
 REASONING_EFFORT = os.environ.get("REASONING_EFFORT", "").strip().lower()
 TEMPERATURE = float(os.environ.get("TEMPERATURE", "0.00000001"))
@@ -80,32 +69,3 @@
 BASH_TOOL_UNSAFE_ALLOW_ALL = load_bool("BASH_TOOL_UNSAFE_ALLOW_ALL", False)
 
 LOG_LLM_USAGE_RESPONSE = load_bool("LOG_LLM_USAGE_RESPONSE", False)
-
-# For CLI only, enable user approval for potentially sensitive commands that would otherwise be rejected
-ENABLE_CLI_TOOL_APPROVAL = load_bool("ENABLE_CLI_TOOL_APPROVAL", True)
-
-MAX_GRAPH_POINTS = float(os.environ.get("MAX_GRAPH_POINTS", 100))
-
-# Limit each tool response to N% of the total context window.
-# Number between 0 and 100
-# Setting to either 0 or any number above 100 disables the logic that limits tool response size
-TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_PCT = float(
-    os.environ.get("TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_PCT", 15)
-)
-
-# Absolute max tokens to allocate for a single tool response
-TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_TOKENS = 25000
-
-MAX_EVIDENCE_DATA_CHARACTERS_BEFORE_TRUNCATION = int(
-    os.environ.get("MAX_EVIDENCE_DATA_CHARACTERS_BEFORE_TRUNCATION", 3000)
-)
-
-ENABLE_CONVERSATION_HISTORY_COMPACTION = load_bool(
-    "ENABLE_CONVERSATION_HISTORY_COMPACTION", default=True
-)
-
-DISABLE_PROMETHEUS_TOOLSET = load_bool("DISABLE_PROMETHEUS_TOOLSET", False)
-
-RESET_REPEATED_TOOL_CALL_CHECK_AFTER_COMPACTION = load_bool(
-    "RESET_REPEATED_TOOL_CALL_CHECK_AFTER_COMPACTION", True
-)
diff -ur baseline-holmes/holmes/config.py holmes2/holmes/config.py
--- baseline-holmes/holmes/config.py	2025-11-05 16:43:28.203076363 -0800
+++ holmes2/holmes/config.py	2025-10-17 15:09:28.615727578 -0700
@@ -1,3 +1,4 @@
+import json
 import logging
 import os
 import os.path
@@ -5,12 +6,18 @@
 from pathlib import Path
 from typing import TYPE_CHECKING, Any, List, Optional, Union
 
-import sentry_sdk
 import yaml  # type: ignore
-from pydantic import BaseModel, ConfigDict, FilePath, PrivateAttr, SecretStr
+from pydantic import BaseModel, ConfigDict, FilePath, SecretStr
 
-from holmes.common.env_vars import ROBUSTA_CONFIG_PATH
-from holmes.core.llm import DefaultLLM, LLMModelRegistry
+
+from holmes.clients.robusta_client import fetch_robusta_models
+from holmes.core.llm import DefaultLLM
+from holmes.common.env_vars import (
+    ROBUSTA_AI,
+    LOAD_ALL_ROBUSTA_MODELS,
+    ROBUSTA_API_ENDPOINT,
+    ROBUSTA_CONFIG_PATH,
+)
 from holmes.core.tools_utils.tool_executor import ToolExecutor
 from holmes.core.toolset_manager import ToolsetManager
 from holmes.plugins.runbooks import (
@@ -22,6 +29,7 @@
 
 # Source plugin imports moved to their respective create methods to speed up startup
 if TYPE_CHECKING:
+    from holmes.core.llm import LLM
     from holmes.core.tool_calling_llm import IssueInvestigator, ToolCallingLLM
     from holmes.plugins.destinations.slack import SlackDestination
     from holmes.plugins.sources.github import GitHubSource
@@ -30,12 +38,18 @@
     from holmes.plugins.sources.pagerduty import PagerDutySource
     from holmes.plugins.sources.prometheus.plugin import AlertManagerSource
 
-from holmes.core.config import config_path_dir
 from holmes.core.supabase_dal import SupabaseDal
+from holmes.core.config import config_path_dir
 from holmes.utils.definitions import RobustaConfig
+from holmes.utils.env import replace_env_vars_values
+from holmes.utils.file_utils import load_yaml_file
 from holmes.utils.pydantic_utils import RobustaBaseConfig, load_model_from_file
 
 DEFAULT_CONFIG_LOCATION = os.path.join(config_path_dir, "config.yaml")
+MODEL_LIST_FILE_LOCATION = os.environ.get(
+    "MODEL_LIST_FILE_LOCATION", "/etc/holmes/config/model_list.yaml"
+)
+ROBUSTA_AI_MODEL_NAME = "Robusta"
 
 
 class SupportedTicketSources(str, Enum):
@@ -43,14 +57,32 @@
     PAGERDUTY = "pagerduty"
 
 
+def is_old_toolset_config(
+    toolsets: Union[dict[str, dict[str, Any]], List[dict[str, Any]]],
+) -> bool:
+    # old config is a list of toolsets
+    if isinstance(toolsets, list):
+        return True
+    return False
+
+
+def parse_models_file(path: str):
+    models = load_yaml_file(path, raise_error=False, warn_not_found=False)
+
+    for _, params in models.items():
+        params = replace_env_vars_values(params)
+
+    return models
+
+
 class Config(RobustaBaseConfig):
-    model: Optional[str] = None
     api_key: Optional[SecretStr] = (
         None  # if None, read from OPENAI_API_KEY or AZURE_OPENAI_ENDPOINT env var
     )
-    api_base: Optional[str] = None
-    api_version: Optional[str] = None
-    fast_model: Optional[str] = None
+    account_id: Optional[str] = None
+    session_token: Optional[SecretStr] = None
+
+    model: Optional[str] = "gpt-4o"
     max_steps: int = 40
     cluster_name: Optional[str] = None
 
@@ -91,19 +123,14 @@
     # custom_toolsets_from_cli is passed from CLI option `--custom-toolsets` as 'experimental' custom toolsets.
     # The status of toolset here won't be cached, so the toolset from cli will always be loaded when specified in the CLI.
     custom_toolsets_from_cli: Optional[List[FilePath]] = None
-    # if True, we will try to load the Robusta AI model, in cli we aren't trying to load it.
-    should_try_robusta_ai: bool = False
+    should_try_robusta_ai: bool = False  # if True, we will try to load the Robusta AI model, in cli we aren't trying to load it.
 
     toolsets: Optional[dict[str, dict[str, Any]]] = None
     mcp_servers: Optional[dict[str, dict[str, Any]]] = None
 
     _server_tool_executor: Optional[ToolExecutor] = None
-    _agui_tool_executor: Optional[ToolExecutor] = None
 
-    # TODO: Separate those fields to facade class, this shouldn't be part of the config.
-    _toolset_manager: Optional[ToolsetManager] = PrivateAttr(None)
-    _llm_model_registry: Optional[LLMModelRegistry] = PrivateAttr(None)
-    _dal: Optional[SupabaseDal] = PrivateAttr(None)
+    _toolset_manager: Optional[ToolsetManager] = None
 
     @property
     def toolset_manager(self) -> ToolsetManager:
@@ -113,29 +140,80 @@
                 mcp_servers=self.mcp_servers,
                 custom_toolsets=self.custom_toolsets,
                 custom_toolsets_from_cli=self.custom_toolsets_from_cli,
-                global_fast_model=self.fast_model,
             )
         return self._toolset_manager
 
-    @property
-    def dal(self) -> SupabaseDal:
-        if not self._dal:
-            self._dal = SupabaseDal(self.cluster_name)  # type: ignore
-        return self._dal
+    def model_post_init(self, __context: Any) -> None:
+        self._model_list = parse_models_file(MODEL_LIST_FILE_LOCATION)
 
-    @property
-    def llm_model_registry(self) -> LLMModelRegistry:
-        if not self._llm_model_registry:
-            self._llm_model_registry = LLMModelRegistry(self, dal=self.dal)
-        return self._llm_model_registry
+        if not self._should_load_robusta_ai():
+            return
 
-    def log_useful_info(self):
-        if self.llm_model_registry.models:
-            logging.info(
-                f"Loaded models: {list(self.llm_model_registry.models.keys())}"
+        self.configure_robusta_ai_model()
+
+    def configure_robusta_ai_model(self) -> None:
+        try:
+            if not self.cluster_name or not LOAD_ALL_ROBUSTA_MODELS:
+                self._load_default_robusta_config()
+                return
+
+            if not self.api_key:
+                dal = SupabaseDal(self.cluster_name)
+                self.load_robusta_api_key(dal)
+
+            if not self.account_id or not self.session_token:
+                self._load_default_robusta_config()
+                return
+
+            models = fetch_robusta_models(
+                self.account_id, self.session_token.get_secret_value()
             )
-        else:
-            logging.warning("No llm models were loaded")
+            if not models:
+                self._load_default_robusta_config()
+                return
+
+            for model in models:
+                logging.info(f"Loading Robusta AI model: {model}")
+                self._model_list[model] = {
+                    "base_url": f"{ROBUSTA_API_ENDPOINT}/llm/{model}",
+                    "is_robusta_model": True,
+                }
+
+        except Exception:
+            logging.exception("Failed to get all robusta models")
+            # fallback to default behavior
+            self._load_default_robusta_config()
+
+    def _load_default_robusta_config(self):
+        if self._should_load_robusta_ai() and self.api_key:
+            logging.info("Loading default Robusta AI model")
+            self._model_list[ROBUSTA_AI_MODEL_NAME] = {
+                "base_url": ROBUSTA_API_ENDPOINT,
+                "is_robusta_model": True,
+            }
+
+    def _should_load_robusta_ai(self) -> bool:
+        if not self.should_try_robusta_ai:
+            return False
+
+        # ROBUSTA_AI were set in the env vars, so we can use it directly
+        if ROBUSTA_AI is not None:
+            return ROBUSTA_AI
+
+        # MODEL is set in the env vars, e.g. the user is using a custom model
+        # so we don't need to load the robusta AI model and keep the behavior backward compatible
+        if "MODEL" in os.environ:
+            return False
+
+        # if the user has provided a model list, we don't need to load the robusta AI model
+        if self._model_list:
+            return False
+
+        return True
+
+    def log_useful_info(self):
+        if self._model_list:
+            logging.info(f"loaded models: {list(self._model_list.keys())}")
 
     @classmethod
     def load_from_file(cls, config_file: Optional[Path], **kwargs) -> "Config":
@@ -149,7 +227,6 @@
         Returns:
             Config instance with merged settings
         """
-
         config_from_file: Optional[Config] = None
         if config_file is not None and config_file.exists():
             logging.debug(f"Loading config from {config_file}")
@@ -173,10 +250,7 @@
         kwargs = {}
         for field_name in [
             "model",
-            "fast_model",
             "api_key",
-            "api_base",
-            "api_version",
             "max_steps",
             "alertmanager_url",
             "alertmanager_username",
@@ -223,9 +297,10 @@
 
         return None
 
-    def get_runbook_catalog(self) -> Optional[RunbookCatalog]:
+    @staticmethod
+    def get_runbook_catalog() -> Optional[RunbookCatalog]:
         # TODO(mainred): besides the built-in runbooks, we need to allow the user to bring their own runbooks
-        runbook_catalog = load_runbook_catalog(dal=self.dal)
+        runbook_catalog = load_runbook_catalog()
         return runbook_catalog
 
     def create_console_tool_executor(
@@ -245,23 +320,6 @@
         )
         return ToolExecutor(cli_toolsets)
 
-    def create_agui_tool_executor(self, dal: Optional["SupabaseDal"]) -> ToolExecutor:
-        """
-        Creates ToolExecutor for the AG-UI server endpoints
-        """
-
-        if self._agui_tool_executor:
-            return self._agui_tool_executor
-
-        # Use same toolset as CLI for AG-UI front-end.
-        agui_toolsets = self.toolset_manager.list_console_toolsets(
-            dal=dal, refresh_status=True
-        )
-
-        self._agui_tool_executor = ToolExecutor(agui_toolsets)
-
-        return self._agui_tool_executor
-
     def create_tool_executor(self, dal: Optional["SupabaseDal"]) -> ToolExecutor:
         """
         Creates ToolExecutor for the server endpoints
@@ -293,19 +351,6 @@
             tool_executor, self.max_steps, self._get_llm(tracer=tracer)
         )
 
-    def create_agui_toolcalling_llm(
-        self,
-        dal: Optional["SupabaseDal"] = None,
-        model: Optional[str] = None,
-        tracer=None,
-    ) -> "ToolCallingLLM":
-        tool_executor = self.create_agui_tool_executor(dal)
-        from holmes.core.tool_calling_llm import ToolCallingLLM
-
-        return ToolCallingLLM(
-            tool_executor, self.max_steps, self._get_llm(model, tracer)
-        )
-
     def create_toolcalling_llm(
         self,
         dal: Optional["SupabaseDal"] = None,
@@ -471,54 +516,37 @@
             raise ValueError("--slack-channel must be specified")
         return SlackDestination(self.slack_token.get_secret_value(), self.slack_channel)
 
-    # TODO: move this to the llm model registry
-    def _get_llm(self, model_key: Optional[str] = None, tracer=None) -> "DefaultLLM":
-        sentry_sdk.set_tag("requested_model", model_key)
-        model_entry = self.llm_model_registry.get_model_params(model_key)
-        model_params = model_entry.model_dump(exclude_none=True)
-        api_base = self.api_base
-        api_version = self.api_version
-
-        is_robusta_model = model_params.pop("is_robusta_model", False)
-        sentry_sdk.set_tag("is_robusta_model", is_robusta_model)
-        if is_robusta_model:
-            # we set here the api_key since it is being refresh when exprided and not as part of the model loading.
-            account_id, token = self.dal.get_ai_credentials()
-            api_key = f"{account_id} {token}"
-        else:
-            api_key = model_params.pop("api_key", None)
-            if api_key is not None:
-                api_key = api_key.get_secret_value()
-
-        model = model_params.pop("model")
-        # It's ok if the model does not have api base and api version, which are defaults to None.
-        # Handle both api_base and base_url - api_base takes precedence
-        model_api_base = model_params.pop("api_base", None)
-        model_base_url = model_params.pop("base_url", None)
-        api_base = model_api_base or model_base_url or api_base
-        api_version = model_params.pop("api_version", api_version)
-        model_name = model_params.pop("name", None) or model_key or model
-        sentry_sdk.set_tag("model_name", model_name)
-        llm = DefaultLLM(
-            model=model,
-            api_key=api_key,
-            api_base=api_base,
-            api_version=api_version,
-            args=model_params,
-            tracer=tracer,
-            name=model_name,
-            is_robusta_model=is_robusta_model,
-        )  # type: ignore
-        logging.info(
-            f"Using model: {model_name} ({llm.get_context_window_size():,} total tokens, {llm.get_maximum_output_token():,} output tokens)"
-        )
-        return llm
+    def _get_llm(self, model_key: Optional[str] = None, tracer=None) -> "LLM":
+        api_key: Optional[str] = None
+        model = self.model
+        model_params = {}
+        if self._model_list:
+            # get requested model or the first credentials if no model requested.
+            model_params = (
+                self._model_list.get(model_key, {}).copy()
+                if model_key
+                else next(iter(self._model_list.values())).copy()
+            )
+            if model_params.get("is_robusta_model") and self.api_key:
+                api_key = self.api_key.get_secret_value()
+            else:
+                api_key = model_params.pop("api_key", api_key)
+            model = model_params.pop("model", model)
+
+        return DefaultLLM(model, api_key, model_params, tracer)  # type: ignore
 
     def get_models_list(self) -> List[str]:
-        if self.llm_model_registry and self.llm_model_registry.models:
-            return list(self.llm_model_registry.models.keys())
+        if self._model_list:
+            return json.dumps(list(self._model_list.keys()))  # type: ignore
+
+        return json.dumps([self.model])  # type: ignore
 
-        return []
+    def load_robusta_api_key(self, dal: SupabaseDal):
+        if ROBUSTA_AI:
+            account_id, token = dal.get_ai_credentials()
+            self.api_key = SecretStr(f"{account_id} {token}")
+            self.account_id = account_id
+            self.session_token = SecretStr(token)
 
 
 class TicketSource(BaseModel):
Only in holmes2/holmes/core: __pycache__
diff -ur baseline-holmes/holmes/core/conversations.py holmes2/holmes/core/conversations.py
--- baseline-holmes/holmes/core/conversations.py	2025-11-05 16:43:28.203419571 -0800
+++ holmes2/holmes/core/conversations.py	2025-10-17 15:09:28.641176547 -0700
@@ -1,6 +1,7 @@
 from typing import Dict, List, Optional
 
 import sentry_sdk
+
 from holmes.config import Config
 from holmes.core.models import (
     ToolCallConversationResult,
@@ -9,10 +10,9 @@
 )
 from holmes.plugins.prompts import load_and_render_prompt
 from holmes.core.tool_calling_llm import ToolCallingLLM
-from holmes.plugins.runbooks import RunbookCatalog
 from holmes.utils.global_instructions import (
     Instructions,
-    add_runbooks_to_user_prompt,
+    add_global_instructions_to_user_prompt,
 )
 
 DEFAULT_TOOL_SIZE = 10000
@@ -26,8 +26,7 @@
         return DEFAULT_TOOL_SIZE
 
     context_window = ai.llm.get_context_window_size()
-    tokens = ai.llm.count_tokens(messages_without_tools)
-    message_size_without_tools = tokens.total_tokens
+    message_size_without_tools = ai.llm.count_tokens_for_message(messages_without_tools)
     maximum_output_token = ai.llm.get_maximum_output_token()
 
     tool_size = min(
@@ -64,7 +63,6 @@
     ai: ToolCallingLLM,
     config: Config,
     global_instructions: Optional[Instructions] = None,
-    runbooks: Optional[RunbookCatalog] = None,
 ):
     """
     This function generates a list of messages for issue conversation and ensures that the message sequence adheres to the model's context window limitations
@@ -121,10 +119,8 @@
     tools_for_investigation = issue_chat_request.investigation_result.tools
 
     if not conversation_history or len(conversation_history) == 0:
-        user_prompt = add_runbooks_to_user_prompt(
-            user_prompt=user_prompt,
-            runbook_catalog=runbooks,
-            global_instructions=global_instructions,
+        user_prompt = add_global_instructions_to_user_prompt(
+            user_prompt, global_instructions
         )
 
         number_of_tools_for_investigation = len(tools_for_investigation)  # type: ignore
@@ -137,7 +133,6 @@
                     "issue": issue_chat_request.issue_type,
                     "toolsets": ai.tool_executor.toolsets,
                     "cluster_name": config.cluster_name,
-                    "runbooks_enabled": True if runbooks else False,
                 },
             )
             messages = [
@@ -158,7 +153,6 @@
             "issue": issue_chat_request.issue_type,
             "toolsets": ai.tool_executor.toolsets,
             "cluster_name": config.cluster_name,
-            "runbooks_enabled": True if runbooks else False,
         }
         system_prompt_without_tools = load_and_render_prompt(
             template_path, template_context_without_tools
@@ -192,7 +186,6 @@
             "issue": issue_chat_request.issue_type,
             "toolsets": ai.tool_executor.toolsets,
             "cluster_name": config.cluster_name,
-            "runbooks_enabled": True if runbooks else False,
         }
         system_prompt_with_truncated_tools = load_and_render_prompt(
             template_path, truncated_template_context
@@ -208,10 +201,8 @@
             },
         ]
 
-    user_prompt = add_runbooks_to_user_prompt(
-        user_prompt=user_prompt,
-        runbook_catalog=runbooks,
-        global_instructions=global_instructions,
+    user_prompt = add_global_instructions_to_user_prompt(
+        user_prompt, global_instructions
     )
 
     conversation_history.append(
@@ -236,7 +227,6 @@
         "issue": issue_chat_request.issue_type,
         "toolsets": ai.tool_executor.toolsets,
         "cluster_name": config.cluster_name,
-        "runbooks_enabled": True if runbooks else False,
     }
     system_prompt_without_tools = load_and_render_prompt(
         template_path, template_context_without_tools
@@ -260,7 +250,6 @@
         "issue": issue_chat_request.issue_type,
         "toolsets": ai.tool_executor.toolsets,
         "cluster_name": config.cluster_name,
-        "runbooks_enabled": True if runbooks else False,
     }
     system_prompt_with_truncated_tools = load_and_render_prompt(
         template_path, template_context
@@ -273,11 +262,7 @@
 
 
 def add_or_update_system_prompt(
-    conversation_history: List[Dict[str, str]],
-    ai: ToolCallingLLM,
-    config: Config,
-    additional_system_prompt: Optional[str] = None,
-    runbooks: Optional[RunbookCatalog] = None,
+    conversation_history: List[Dict[str, str]], ai: ToolCallingLLM, config: Config
 ):
     """Either add the system prompt or replace an existing system prompt.
     As a 'defensive' measure, this code will only replace an existing system prompt if it is the
@@ -289,12 +274,9 @@
     context = {
         "toolsets": ai.tool_executor.toolsets,
         "cluster_name": config.cluster_name,
-        "runbooks_enabled": True if runbooks else False,
     }
 
     system_prompt = load_and_render_prompt(template_path, context)
-    if additional_system_prompt:
-        system_prompt = system_prompt + "\n" + additional_system_prompt
 
     if not conversation_history or len(conversation_history) == 0:
         conversation_history.append({"role": "system", "content": system_prompt})
@@ -321,8 +303,6 @@
     ai: ToolCallingLLM,
     config: Config,
     global_instructions: Optional[Instructions] = None,
-    additional_system_prompt: Optional[str] = None,
-    runbooks: Optional[RunbookCatalog] = None,
 ) -> List[dict]:
     """
     This function generates a list of messages for general chat conversation and ensures that the message sequence adheres to the model's context window limitations
@@ -378,18 +358,10 @@
         conversation_history = conversation_history.copy()
 
     conversation_history = add_or_update_system_prompt(
-        conversation_history=conversation_history,
-        ai=ai,
-        config=config,
-        additional_system_prompt=additional_system_prompt,
-        runbooks=runbooks,
+        conversation_history=conversation_history, ai=ai, config=config
     )
 
-    ask = add_runbooks_to_user_prompt(
-        user_prompt=ask,
-        runbook_catalog=runbooks,
-        global_instructions=global_instructions,
-    )
+    ask = add_global_instructions_to_user_prompt(ask, global_instructions)
 
     conversation_history.append(  # type: ignore
         {
@@ -397,7 +369,6 @@
             "content": ask,
         },
     )
-
     number_of_tools = len(
         [message for message in conversation_history if message.get("role") == "tool"]  # type: ignore
     )
@@ -422,7 +393,6 @@
     ai: ToolCallingLLM,
     config: Config,
     global_instructions: Optional[Instructions] = None,
-    runbooks: Optional[RunbookCatalog] = None,
 ):
     """
     This function generates a list of messages for workload health conversation and ensures that the message sequence adheres to the model's context window limitations
@@ -481,10 +451,8 @@
     resource = workload_health_chat_request.resource
 
     if not conversation_history or len(conversation_history) == 0:
-        user_prompt = add_runbooks_to_user_prompt(
-            user_prompt=user_prompt,
-            runbook_catalog=runbooks,
-            global_instructions=global_instructions,
+        user_prompt = add_global_instructions_to_user_prompt(
+            user_prompt, global_instructions
         )
 
         number_of_tools_for_workload = len(tools_for_workload)  # type: ignore
@@ -497,7 +465,6 @@
                     "resource": resource,
                     "toolsets": ai.tool_executor.toolsets,
                     "cluster_name": config.cluster_name,
-                    "runbooks_enabled": True if runbooks else False,
                 },
             )
             messages = [
@@ -518,7 +485,6 @@
             "resource": resource,
             "toolsets": ai.tool_executor.toolsets,
             "cluster_name": config.cluster_name,
-            "runbooks_enabled": True if runbooks else False,
         }
         system_prompt_without_tools = load_and_render_prompt(
             template_path, template_context_without_tools
@@ -552,7 +518,6 @@
             "resource": resource,
             "toolsets": ai.tool_executor.toolsets,
             "cluster_name": config.cluster_name,
-            "runbooks_enabled": True if runbooks else False,
         }
         system_prompt_with_truncated_tools = load_and_render_prompt(
             template_path, truncated_template_context
@@ -568,10 +533,8 @@
             },
         ]
 
-    user_prompt = add_runbooks_to_user_prompt(
-        user_prompt=user_prompt,
-        runbook_catalog=runbooks,
-        global_instructions=global_instructions,
+    user_prompt = add_global_instructions_to_user_prompt(
+        user_prompt, global_instructions
     )
 
     conversation_history.append(
@@ -596,7 +559,6 @@
         "resource": resource,
         "toolsets": ai.tool_executor.toolsets,
         "cluster_name": config.cluster_name,
-        "runbooks_enabled": True if runbooks else False,
     }
     system_prompt_without_tools = load_and_render_prompt(
         template_path, template_context_without_tools
@@ -620,7 +582,6 @@
         "resource": resource,
         "toolsets": ai.tool_executor.toolsets,
         "cluster_name": config.cluster_name,
-        "runbooks_enabled": True if runbooks else False,
     }
     system_prompt_with_truncated_tools = load_and_render_prompt(
         template_path, template_context
Only in baseline-holmes/holmes/core: feedback.py
diff -ur baseline-holmes/holmes/core/investigation.py holmes2/holmes/core/investigation.py
--- baseline-holmes/holmes/core/investigation.py	2025-11-05 16:43:28.203766903 -0800
+++ holmes2/holmes/core/investigation.py	2025-10-17 15:09:28.640017306 -0700
@@ -1,7 +1,6 @@
 import logging
 from typing import Optional
 
-
 from holmes.common.env_vars import HOLMES_POST_PROCESSING_PROMPT
 from holmes.config import Config
 from holmes.core.investigation_structured_output import process_response_into_sections
@@ -9,8 +8,7 @@
 from holmes.core.models import InvestigateRequest, InvestigationResult
 from holmes.core.supabase_dal import SupabaseDal
 from holmes.core.tracing import DummySpan, SpanType
-from holmes.plugins.runbooks import RunbookCatalog
-from holmes.utils.global_instructions import add_runbooks_to_user_prompt
+from holmes.utils.global_instructions import add_global_instructions_to_user_prompt
 
 from holmes.core.investigation_structured_output import (
     DEFAULT_SECTIONS,
@@ -27,8 +25,8 @@
     config: Config,
     model: Optional[str] = None,
     trace_span=DummySpan(),
-    runbooks: Optional[RunbookCatalog] = None,
 ) -> InvestigationResult:
+    config.load_robusta_api_key(dal=dal)
     context = dal.get_issue_data(investigate_request.context.get("robusta_issue_id"))
 
     resource_instructions = dal.get_resource_instructions(
@@ -63,7 +61,6 @@
         global_instructions=global_instructions,
         sections=investigate_request.sections,
         trace_span=trace_span,
-        runbooks=runbooks,
     )
 
     (text_response, sections) = process_response_into_sections(investigation.result)
@@ -74,7 +71,6 @@
         sections=sections,
         tool_calls=investigation.tool_calls or [],
         instructions=investigation.instructions,
-        metadata=investigation.metadata,
     )
 
 
@@ -84,6 +80,7 @@
     config: Config,
     request_structured_output_from_llm: Optional[bool] = None,
 ):
+    config.load_robusta_api_key(dal=dal)
     ai = config.create_issue_investigator(dal=dal, model=investigate_request.model)
 
     raw_data = investigate_request.model_dump()
@@ -99,11 +96,18 @@
         raw=raw_data,
     )
 
-    issue_instructions = ai.runbook_manager.get_instructions_for_issue(issue)
+    runbooks = ai.runbook_manager.get_instructions_for_issue(issue)
 
-    resource_instructions = dal.get_resource_instructions(
+    instructions = dal.get_resource_instructions(
         "alert", investigate_request.context.get("issue_type")
     )
+    if instructions is not None and instructions.instructions:
+        runbooks.extend(instructions.instructions)
+    if instructions is not None and len(instructions.documents) > 0:
+        docPrompts = []
+        for document in instructions.documents:
+            docPrompts.append(f"* fetch information from this URL: {document.url}\n")
+        runbooks.extend(docPrompts)
 
     # This section is about setting vars to request the LLM to return structured output.
     # It does not mean that Holmes will not return structured sections for investigation as it is
@@ -128,7 +132,6 @@
     else:
         logging.info("Structured output is disabled for this request")
 
-    runbook_catalog = config.get_runbook_catalog()
     system_prompt = load_and_render_prompt(
         investigate_request.prompt_template,
         {
@@ -137,20 +140,21 @@
             "structured_output": request_structured_output_from_llm,
             "toolsets": ai.tool_executor.toolsets,
             "cluster_name": config.cluster_name,
-            "runbooks_enabled": True if runbook_catalog else False,
         },
     )
+
     user_prompt = ""
+    if runbooks:
+        for runbook_str in runbooks:
+            user_prompt += f"* {runbook_str}\n"
+
+        user_prompt = f'My instructions to check \n"""{user_prompt}"""'
 
     global_instructions = dal.get_global_instructions_for_account()
-    user_prompt = add_runbooks_to_user_prompt(
-        user_prompt=user_prompt,
-        runbook_catalog=runbook_catalog,
-        global_instructions=global_instructions,
-        issue_instructions=issue_instructions,
-        resource_instructions=resource_instructions,
+    user_prompt = add_global_instructions_to_user_prompt(
+        user_prompt, global_instructions
     )
 
-    user_prompt = f"{user_prompt}\n #This is context from the issue:\n{issue.raw}"
+    user_prompt = f"{user_prompt}\n This is context from the issue {issue.raw}"
 
-    return ai, system_prompt, user_prompt, response_format, sections, issue_instructions
+    return ai, system_prompt, user_prompt, response_format, sections, runbooks
diff -ur baseline-holmes/holmes/core/llm.py holmes2/holmes/core/llm.py
--- baseline-holmes/holmes/core/llm.py	2025-11-05 16:43:28.204194527 -0800
+++ holmes2/holmes/core/llm.py	2025-10-17 15:09:28.621875447 -0700
@@ -1,92 +1,32 @@
 import json
 import logging
-import os
 from abc import abstractmethod
-from math import floor
-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Type, Union
+from typing import Any, Dict, List, Optional, Type, Union
 
-import litellm
-from litellm.litellm_core_utils.streaming_handler import CustomStreamWrapper
-from litellm.types.utils import ModelResponse, TextCompletionResponse
+from litellm.types.utils import ModelResponse
 import sentry_sdk
-from pydantic import BaseModel, ConfigDict, SecretStr
-from typing_extensions import Self
-
-from holmes.clients.robusta_client import (
-    RobustaModel,
-    RobustaModelsResponse,
-    fetch_robusta_models,
-)
 
+from litellm.litellm_core_utils.streaming_handler import CustomStreamWrapper
+from pydantic import BaseModel
+import litellm
+import os
 from holmes.common.env_vars import (
-    FALLBACK_CONTEXT_WINDOW_SIZE,
-    LOAD_ALL_ROBUSTA_MODELS,
     REASONING_EFFORT,
-    ROBUSTA_AI,
-    ROBUSTA_API_ENDPOINT,
     THINKING,
-    EXTRA_HEADERS,
-    TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_PCT,
-    TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_TOKENS,
 )
-from holmes.core.supabase_dal import SupabaseDal
-from holmes.utils.env import environ_get_safe_int, replace_env_vars_values
-from holmes.utils.file_utils import load_yaml_file
 
-if TYPE_CHECKING:
-    from holmes.config import Config
 
-MODEL_LIST_FILE_LOCATION = os.environ.get(
-    "MODEL_LIST_FILE_LOCATION", "/etc/holmes/config/model_list.yaml"
-)
+def environ_get_safe_int(env_var, default="0"):
+    try:
+        return max(int(os.environ.get(env_var, default)), 0)
+    except ValueError:
+        return int(default)
 
 
 OVERRIDE_MAX_OUTPUT_TOKEN = environ_get_safe_int("OVERRIDE_MAX_OUTPUT_TOKEN")
 OVERRIDE_MAX_CONTENT_SIZE = environ_get_safe_int("OVERRIDE_MAX_CONTENT_SIZE")
 
 
-def get_context_window_compaction_threshold_pct() -> int:
-    """Get the compaction threshold percentage at runtime to support test overrides."""
-    return environ_get_safe_int("CONTEXT_WINDOW_COMPACTION_THRESHOLD_PCT", default="95")
-
-
-ROBUSTA_AI_MODEL_NAME = "Robusta"
-
-
-class TokenCountMetadata(BaseModel):
-    total_tokens: int
-    tools_tokens: int
-    system_tokens: int
-    user_tokens: int
-    tools_to_call_tokens: int
-    assistant_tokens: int
-    other_tokens: int
-
-
-class ModelEntry(BaseModel):
-    """ModelEntry represents a single LLM model configuration."""
-
-    model: str
-    # TODO: the name field seems to be redundant, can we remove it?
-    name: Optional[str] = None
-    api_key: Optional[SecretStr] = None
-    base_url: Optional[str] = None
-    is_robusta_model: Optional[bool] = None
-    custom_args: Optional[Dict[str, Any]] = None
-
-    # LLM configurations used services like Azure OpenAI Service
-    api_base: Optional[str] = None
-    api_version: Optional[str] = None
-
-    model_config = ConfigDict(
-        extra="allow",
-    )
-
-    @classmethod
-    def load_from_dict(cls, data: dict) -> Self:
-        return cls.model_validate(data)
-
-
 class LLM:
     @abstractmethod
     def __init__(self):
@@ -100,23 +40,8 @@
     def get_maximum_output_token(self) -> int:
         pass
 
-    def get_max_token_count_for_single_tool(self) -> int:
-        if (
-            0 < TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_PCT
-            and TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_PCT <= 100
-        ):
-            context_window_size = self.get_context_window_size()
-            calculated_max_tokens = int(
-                context_window_size * TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_PCT // 100
-            )
-            return min(calculated_max_tokens, TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_TOKENS)
-        else:
-            return TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_TOKENS
-
     @abstractmethod
-    def count_tokens(
-        self, messages: list[dict], tools: Optional[list[dict[str, Any]]] = None
-    ) -> TokenCountMetadata:
+    def count_tokens_for_message(self, messages: list[dict]) -> int:
         pass
 
     @abstractmethod
@@ -136,55 +61,31 @@
 class DefaultLLM(LLM):
     model: str
     api_key: Optional[str]
-    api_base: Optional[str]
-    api_version: Optional[str]
+    base_url: Optional[str]
     args: Dict
-    is_robusta_model: bool
 
     def __init__(
         self,
         model: str,
         api_key: Optional[str] = None,
-        api_base: Optional[str] = None,
-        api_version: Optional[str] = None,
         args: Optional[Dict] = None,
-        tracer: Optional[Any] = None,
-        name: Optional[str] = None,
-        is_robusta_model: bool = False,
+        tracer=None,
     ):
         self.model = model
         self.api_key = api_key
-        self.api_base = api_base
-        self.api_version = api_version
         self.args = args or {}
         self.tracer = tracer
-        self.name = name
-        self.is_robusta_model = is_robusta_model
-        self.update_custom_args()
-        self.check_llm(
-            self.model, self.api_key, self.api_base, self.api_version, self.args
-        )
 
-    def update_custom_args(self):
-        self.max_context_size = self.args.get("custom_args", {}).get("max_context_size")
-        self.args.pop("custom_args", None)
+        if not self.args:
+            self.check_llm(self.model, self.api_key)
 
-    def check_llm(
-        self,
-        model: str,
-        api_key: Optional[str],
-        api_base: Optional[str],
-        api_version: Optional[str],
-        args: Optional[dict] = None,
-    ):
-        if self.is_robusta_model:
-            # The model is assumed correctly configured if it is a robusta model
-            # For robusta models, this code would fail because Holmes has no knowledge of the API keys
-            # to azure or bedrock as all completion API calls go through robusta's LLM proxy
-            return
-        args = args or {}
+    def check_llm(self, model: str, api_key: Optional[str]):
         logging.debug(f"Checking LiteLLM model {model}")
-        lookup = litellm.get_llm_provider(model)
+        # TODO: this WAS a hack to get around the fact that we can't pass in an api key to litellm.validate_environment
+        # so without this hack it always complains that the environment variable for the api key is missing
+        # to fix that, we always set an api key in the standard format that litellm expects (which is ${PROVIDER}_API_KEY)
+        # TODO: we can now handle this better - see https://github.com/BerriAI/litellm/issues/4375#issuecomment-2223684750
+        lookup = litellm.get_llm_provider(self.model)
         if not lookup:
             raise Exception(f"Unknown provider for model {model}")
         provider = lookup[1]
@@ -218,151 +119,85 @@
                     "environment variable for proper functionality. For more information, refer to the documentation: "
                     "https://docs.litellm.ai/docs/providers/watsonx#usage---models-in-deployment-spaces"
                 )
-        elif provider == "bedrock":
-            if os.environ.get("AWS_PROFILE") or os.environ.get(
-                "AWS_BEARER_TOKEN_BEDROCK"
-            ):
-                model_requirements = {"keys_in_environment": True, "missing_keys": []}
-            elif args.get("aws_access_key_id") and args.get("aws_secret_access_key"):
-                return  # break fast.
-            else:
-                model_requirements = litellm.validate_environment(
-                    model=model, api_key=api_key, api_base=api_base
-                )
+        elif provider == "bedrock" and (
+            os.environ.get("AWS_PROFILE") or os.environ.get("AWS_BEARER_TOKEN_BEDROCK")
+        ):
+            model_requirements = {"keys_in_environment": True, "missing_keys": []}
         else:
-            model_requirements = litellm.validate_environment(
-                model=model, api_key=api_key, api_base=api_base
-            )
-            # validate_environment does not accept api_version, and as a special case for Azure OpenAI Service,
-            # when all the other AZURE environments are set expect AZURE_API_VERSION, validate_environment complains
-            # the missing of it even after the api_version is set.
-            # TODO: There's an open PR in litellm to accept api_version in validate_environment, we can leverage this
-            # change if accepted to ignore the following check.
-            # https://github.com/BerriAI/litellm/pull/13808
-            if (
-                provider == "azure"
-                and ["AZURE_API_VERSION"] == model_requirements["missing_keys"]
-                and api_version is not None
-            ):
-                model_requirements["missing_keys"] = []
-                model_requirements["keys_in_environment"] = True
+            #
+            api_key_env_var = f"{provider.upper()}_API_KEY"
+            if api_key:
+                os.environ[api_key_env_var] = api_key
+            model_requirements = litellm.validate_environment(model=model)
 
         if not model_requirements["keys_in_environment"]:
             raise Exception(
                 f"model {model} requires the following environment variables: {model_requirements['missing_keys']}"
             )
 
-    def _get_model_name_variants_for_lookup(self) -> list[str]:
+    def _strip_model_prefix(self) -> str:
         """
-        Generate model name variants to try when looking up in litellm.model_cost.
-        Returns a list of names to try in order: exact, lowercase, without prefix, etc.
+        Helper function to strip 'openai/' prefix from model name if it exists.
+        model cost is taken from here which does not have the openai prefix
+        https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json
         """
-        names_to_try = [self.model, self.model.lower()]
-
-        # If there's a prefix, also try without it
-        if "/" in self.model:
-            base_model = self.model.split("/", 1)[1]
-            names_to_try.extend([base_model, base_model.lower()])
+        model_name = self.model
+        prefixes = ["openai/", "bedrock/", "vertex_ai/", "anthropic/"]
 
-        # Remove duplicates while preserving order (dict.fromkeys maintains insertion order in Python 3.7+)
-        return list(dict.fromkeys(names_to_try))
+        for prefix in prefixes:
+            if model_name.startswith(prefix):
+                return model_name[len(prefix) :]
+
+        return model_name
+
+        # this unfortunately does not seem to work for azure if the deployment name is not a well-known model name
+        # if not litellm.supports_function_calling(model=model):
+        #    raise Exception(f"model {model} does not support function calling. You must use HolmesGPT with a model that supports function calling.")
 
     def get_context_window_size(self) -> int:
-        if self.max_context_size:
-            return self.max_context_size
-
         if OVERRIDE_MAX_CONTENT_SIZE:
             logging.debug(
                 f"Using override OVERRIDE_MAX_CONTENT_SIZE {OVERRIDE_MAX_CONTENT_SIZE}"
             )
             return OVERRIDE_MAX_CONTENT_SIZE
 
-        # Try each name variant
-        for name in self._get_model_name_variants_for_lookup():
-            try:
-                return litellm.model_cost[name]["max_input_tokens"]
-            except Exception:
-                continue
-
-        # Log which lookups we tried
-        logging.warning(
-            f"Couldn't find model {self.model} in litellm's model list (tried: {', '.join(self._get_model_name_variants_for_lookup())}), "
-            f"using default {FALLBACK_CONTEXT_WINDOW_SIZE} tokens for max_input_tokens. "
-            f"To override, set OVERRIDE_MAX_CONTENT_SIZE environment variable to the correct value for your model."
-        )
-        return FALLBACK_CONTEXT_WINDOW_SIZE
+        model_name = os.environ.get("MODEL_TYPE", self._strip_model_prefix())
+        try:
+            return litellm.model_cost[model_name]["max_input_tokens"]
+        except Exception:
+            logging.warning(
+                f"Couldn't find model's name {model_name} in litellm's model list, fallback to 128k tokens for max_input_tokens"
+            )
+            return 128000
 
     @sentry_sdk.trace
-    def count_tokens(
-        self, messages: list[dict], tools: Optional[list[dict[str, Any]]] = None
-    ) -> TokenCountMetadata:
-        # TODO: Add a recount:bool flag to save time. When the flag is false, reuse 'message["token_count"]' for individual messages.
-        # It's only necessary to recount message tokens at the beginning of a session because the LLM model may have changed.
-        # Changing the model requires recounting tokens because the tokenizer may be different
-        total_tokens = 0
-        tools_tokens = 0
-        system_tokens = 0
-        assistant_tokens = 0
-        user_tokens = 0
-        other_tokens = 0
-        tools_to_call_tokens = 0
+    def count_tokens_for_message(self, messages: list[dict]) -> int:
+        total_token_count = 0
         for message in messages:
-            # count message tokens individually because it gives us fine grain information about each tool call/message etc.
-            # However be aware that the sum of individual message tokens is not equal to the overall messages token
-            token_count = litellm.token_counter(  # type: ignore
-                model=self.model, messages=[message]
-            )
-            message["token_count"] = token_count
-            role = message.get("role")
-            if role == "system":
-                system_tokens += token_count
-            elif role == "user":
-                user_tokens += token_count
-            elif role == "tool":
-                tools_tokens += token_count
-            elif role == "assistant":
-                assistant_tokens += token_count
+            if "token_count" in message and message["token_count"]:
+                total_token_count += message["token_count"]
             else:
-                # although this should not be needed,
-                # it is defensive code so that all tokens are accounted for
-                # and can potentially make debugging easier
-                other_tokens += token_count
-
-        messages_token_count_without_tools = litellm.token_counter(  # type: ignore
-            model=self.model, messages=messages
-        )
-
-        total_tokens = litellm.token_counter(  # type: ignore
-            model=self.model,
-            messages=messages,
-            tools=tools,  # type: ignore
-        )
-        tools_to_call_tokens = max(0, total_tokens - messages_token_count_without_tools)
-
-        return TokenCountMetadata(
-            total_tokens=total_tokens,
-            system_tokens=system_tokens,
-            user_tokens=user_tokens,
-            tools_tokens=tools_tokens,
-            tools_to_call_tokens=tools_to_call_tokens,
-            other_tokens=other_tokens,
-            assistant_tokens=assistant_tokens,
-        )
-
-    def get_litellm_corrected_name_for_robusta_ai(self) -> str:
-        if self.is_robusta_model:
-            # For robusta models, self.model is the underlying provider/model used by Robusta AI
-            # To avoid litellm modifying the API URL according to the provider, the provider name
-            # is replaced with 'openai/' just before doing a completion() call
-            # Cf. https://docs.litellm.ai/docs/providers/openai_compatible
-            split_model_name = self.model.split("/")
-            return (
-                split_model_name[0]
-                if len(split_model_name) == 1
-                else f"openai/{split_model_name[1]}"
-            )
-        else:
-            return self.model
+                # message can be counted by this method only if message contains a "content" key
+                if "content" in message:
+                    if isinstance(message["content"], str):
+                        message_to_count = [
+                            {"type": "text", "text": message["content"]}
+                        ]
+                    elif isinstance(message["content"], list):
+                        message_to_count = [
+                            {"type": "text", "text": json.dumps(message["content"])}
+                        ]
+                    elif isinstance(message["content"], dict):
+                        if "type" not in message["content"]:
+                            message_to_count = [
+                                {"type": "text", "text": json.dumps(message["content"])}
+                            ]
+                    token_count = litellm.token_counter(
+                        model=self.model, messages=message_to_count
+                    )
+                    message["token_count"] = token_count
+                    total_token_count += token_count
+        return total_token_count
 
     def completion(
         self,
@@ -377,16 +212,13 @@
         tools_args = {}
         allowed_openai_params = None
 
-        if tools and len(tools) > 0 and tool_choice == "auto":
+        if tools and len(tools) > 0 and tool_choice:
             tools_args["tools"] = tools
             tools_args["tool_choice"] = tool_choice  # type: ignore
 
         if THINKING:
             self.args.setdefault("thinking", json.loads(THINKING))
 
-        if EXTRA_HEADERS:
-            self.args.setdefault("extra_headers", json.loads(EXTRA_HEADERS))
-
         if self.args.get("thinking", None):
             litellm.modify_params = True
 
@@ -402,13 +234,9 @@
 
         # Get the litellm module to use (wrapped or unwrapped)
         litellm_to_use = self.tracer.wrap_llm(litellm) if self.tracer else litellm
-
-        litellm_model_name = self.get_litellm_corrected_name_for_robusta_ai()
         result = litellm_to_use.completion(
-            model=litellm_model_name,
+            model=self.model,
             api_key=self.api_key,
-            base_url=self.api_base,
-            api_version=self.api_version,
             messages=messages,
             response_format=response_format,
             drop_params=drop_params,
@@ -426,33 +254,20 @@
             raise Exception(f"Unexpected type returned by the LLM {type(result)}")
 
     def get_maximum_output_token(self) -> int:
-        max_output_tokens = floor(min(64000, self.get_context_window_size() / 5))
-
         if OVERRIDE_MAX_OUTPUT_TOKEN:
             logging.debug(
                 f"Using OVERRIDE_MAX_OUTPUT_TOKEN {OVERRIDE_MAX_OUTPUT_TOKEN}"
             )
             return OVERRIDE_MAX_OUTPUT_TOKEN
 
-        # Try each name variant
-        for name in self._get_model_name_variants_for_lookup():
-            try:
-                litellm_max_output_tokens = litellm.model_cost[name][
-                    "max_output_tokens"
-                ]
-                if litellm_max_output_tokens < max_output_tokens:
-                    max_output_tokens = litellm_max_output_tokens
-                return max_output_tokens
-            except Exception:
-                continue
-
-        # Log which lookups we tried
-        logging.warning(
-            f"Couldn't find model {self.model} in litellm's model list (tried: {', '.join(self._get_model_name_variants_for_lookup())}), "
-            f"using {max_output_tokens} tokens for max_output_tokens. "
-            f"To override, set OVERRIDE_MAX_OUTPUT_TOKEN environment variable to the correct value for your model."
-        )
-        return max_output_tokens
+        model_name = os.environ.get("MODEL_TYPE", self._strip_model_prefix())
+        try:
+            return litellm.model_cost[model_name]["max_output_tokens"]
+        except Exception:
+            logging.warning(
+                f"Couldn't find model's name {model_name} in litellm's model list, fallback to 4096 tokens for max_output_tokens"
+            )
+            return 4096
 
     def _add_cache_control_to_last_message(
         self, messages: List[Dict[str, Any]]
@@ -461,12 +276,6 @@
         Add cache_control to the last non-user message for Anthropic prompt caching.
         Removes any existing cache_control from previous messages to avoid accumulation.
         """
-        # Skip cache_control for VertexAI/Gemini models as they don't support it with tools
-        if self.model and (
-            "vertex" in self.model.lower() or "gemini" in self.model.lower()
-        ):
-            return
-
         # First, remove any existing cache_control from all messages
         for msg in messages:
             content = msg.get("content")
@@ -496,7 +305,7 @@
         if content is None:
             return
 
-        if isinstance(content, str) and content:
+        if isinstance(content, str):
             # Convert string to structured format with cache_control
             target_msg["content"] = [
                 {
@@ -516,213 +325,3 @@
                 logging.debug(
                     f"Added cache_control to {target_msg.get('role')} message (structured content)"
                 )
-
-
-class LLMModelRegistry:
-    def __init__(self, config: "Config", dal: SupabaseDal) -> None:
-        self.config = config
-        self._llms: dict[str, ModelEntry] = {}
-        self._default_robusta_model = None
-        self.dal = dal
-
-        self._init_models()
-
-    @property
-    def default_robusta_model(self) -> Optional[str]:
-        return self._default_robusta_model
-
-    def _init_models(self):
-        self._llms = self._parse_models_file(MODEL_LIST_FILE_LOCATION)
-
-        if self._should_load_robusta_ai():
-            self.configure_robusta_ai_model()
-
-        if self._should_load_config_model():
-            self._llms[self.config.model] = self._create_model_entry(
-                model=self.config.model,
-                model_name=self.config.model,
-                base_url=self.config.api_base,
-                is_robusta_model=False,
-                api_key=self.config.api_key,
-                api_version=self.config.api_version,
-            )
-
-    def _should_load_config_model(self) -> bool:
-        if self.config.model is not None:
-            return True
-
-        # backward compatibility - in the past config.model was set by default to gpt-4o.
-        # so we need to check if the user has set an OPENAI_API_KEY to load the config model.
-        has_openai_key = os.environ.get("OPENAI_API_KEY")
-        if has_openai_key:
-            self.config.model = "gpt-4.1"
-            return True
-
-        return False
-
-    def configure_robusta_ai_model(self) -> None:
-        try:
-            if not self.config.cluster_name or not LOAD_ALL_ROBUSTA_MODELS:
-                self._load_default_robusta_config()
-                return
-
-            if not self.dal.account_id or not self.dal.enabled:
-                self._load_default_robusta_config()
-                return
-
-            account_id, token = self.dal.get_ai_credentials()
-
-            robusta_models: RobustaModelsResponse | None = fetch_robusta_models(
-                account_id, token
-            )
-            if not robusta_models or not robusta_models.models:
-                self._load_default_robusta_config()
-                return
-
-            default_model = None
-            for model_name, model_data in robusta_models.models.items():
-                logging.info(f"Loading Robusta AI model: {model_name}")
-                self._llms[model_name] = self._create_robusta_model_entry(
-                    model_name=model_name, model_data=model_data
-                )
-                if model_data.is_default:
-                    default_model = model_name
-
-            if default_model:
-                logging.info(f"Setting default Robusta AI model to: {default_model}")
-                self._default_robusta_model: str = default_model  # type: ignore
-
-        except Exception:
-            logging.exception("Failed to get all robusta models")
-            # fallback to default behavior
-            self._load_default_robusta_config()
-
-    def _load_default_robusta_config(self):
-        if self._should_load_robusta_ai():
-            logging.info("Loading default Robusta AI model")
-            self._llms[ROBUSTA_AI_MODEL_NAME] = ModelEntry(
-                name=ROBUSTA_AI_MODEL_NAME,
-                model="gpt-4o",  # TODO: tech debt, this isn't really
-                base_url=ROBUSTA_API_ENDPOINT,
-                is_robusta_model=True,
-            )
-            self._default_robusta_model = ROBUSTA_AI_MODEL_NAME
-
-    def _should_load_robusta_ai(self) -> bool:
-        if not self.config.should_try_robusta_ai:
-            return False
-
-        # ROBUSTA_AI were set in the env vars, so we can use it directly
-        if ROBUSTA_AI is not None:
-            return ROBUSTA_AI
-
-        # MODEL is set in the env vars, e.g. the user is using a custom model
-        # so we don't need to load the robusta AI model and keep the behavior backward compatible
-        if "MODEL" in os.environ:
-            return False
-
-        # if the user has provided a model list, we don't need to load the robusta AI model
-        if self._llms:
-            return False
-
-        return True
-
-    def get_model_params(self, model_key: Optional[str] = None) -> ModelEntry:
-        if not self._llms:
-            raise Exception("No llm models were loaded")
-
-        if model_key:
-            model_params = self._llms.get(model_key)
-            if model_params is not None:
-                logging.info(f"Using selected model: {model_key}")
-                return model_params.copy()
-
-            logging.error(f"Couldn't find model: {model_key} in model list")
-
-        if self._default_robusta_model:
-            model_params = self._llms.get(self._default_robusta_model)
-            if model_params is not None:
-                logging.info(
-                    f"Using default Robusta AI model: {self._default_robusta_model}"
-                )
-                return model_params.copy()
-
-            logging.error(
-                f"Couldn't find default Robusta AI model: {self._default_robusta_model} in model list"
-            )
-
-        model_key, first_model_params = next(iter(self._llms.items()))
-        logging.debug(f"Using first available model: {model_key}")
-        return first_model_params.copy()
-
-    def get_llm(self, name: str) -> LLM:  # TODO: fix logic
-        return self._llms[name]  # type: ignore
-
-    @property
-    def models(self) -> dict[str, ModelEntry]:
-        return self._llms
-
-    def _parse_models_file(self, path: str) -> dict[str, ModelEntry]:
-        models = load_yaml_file(path, raise_error=False, warn_not_found=False)
-        for _, params in models.items():
-            params = replace_env_vars_values(params)
-
-        llms = {}
-        for model_name, params in models.items():
-            llms[model_name] = ModelEntry.model_validate(params)
-
-        return llms
-
-    def _create_robusta_model_entry(
-        self, model_name: str, model_data: RobustaModel
-    ) -> ModelEntry:
-        entry = self._create_model_entry(
-            model=model_data.model,
-            model_name=model_name,
-            base_url=f"{ROBUSTA_API_ENDPOINT}/llm/{model_name}",
-            is_robusta_model=True,
-        )
-        entry.custom_args = model_data.holmes_args or {}  # type: ignore[assignment]
-        return entry
-
-    def _create_model_entry(
-        self,
-        model: str,
-        model_name: str,
-        base_url: Optional[str] = None,
-        is_robusta_model: Optional[bool] = None,
-        api_key: Optional[SecretStr] = None,
-        api_base: Optional[str] = None,
-        api_version: Optional[str] = None,
-    ) -> ModelEntry:
-        return ModelEntry(
-            name=model_name,
-            model=model,
-            base_url=base_url,
-            is_robusta_model=is_robusta_model,
-            api_key=api_key,
-            api_base=api_base,
-            api_version=api_version,
-        )
-
-
-def get_llm_usage(
-    llm_response: Union[ModelResponse, CustomStreamWrapper, TextCompletionResponse],
-) -> dict:
-    usage: dict = {}
-    if (
-        (
-            isinstance(llm_response, ModelResponse)
-            or isinstance(llm_response, TextCompletionResponse)
-        )
-        and hasattr(llm_response, "usage")
-        and llm_response.usage
-    ):  # type: ignore
-        usage["prompt_tokens"] = llm_response.usage.prompt_tokens  # type: ignore
-        usage["completion_tokens"] = llm_response.usage.completion_tokens  # type: ignore
-        usage["total_tokens"] = llm_response.usage.total_tokens  # type: ignore
-    elif isinstance(llm_response, CustomStreamWrapper):
-        complete_response = litellm.stream_chunk_builder(chunks=llm_response)  # type: ignore
-        if complete_response:
-            return get_llm_usage(complete_response)
-    return usage
diff -ur baseline-holmes/holmes/core/models.py holmes2/holmes/core/models.py
--- baseline-holmes/holmes/core/models.py	2025-11-05 16:43:28.204438693 -0800
+++ holmes2/holmes/core/models.py	2025-10-17 15:09:28.618591806 -0700
@@ -1,94 +1,15 @@
-import json
 from holmes.core.investigation_structured_output import InputSectionsDataType
+from holmes.core.tool_calling_llm import ToolCallResult
 from typing import Optional, List, Dict, Any, Union
 from pydantic import BaseModel, model_validator, Field
 from enum import Enum
 
-from holmes.core.tools import StructuredToolResult, StructuredToolResultStatus
-
-
-class TruncationMetadata(BaseModel):
-    tool_call_id: str
-    start_index: int
-    end_index: int
-    tool_name: str
-    original_token_count: int
-
-
-class TruncationResult(BaseModel):
-    truncated_messages: list[dict]
-    truncations: list[TruncationMetadata]
-
-
-class ToolCallResult(BaseModel):
-    tool_call_id: str
-    tool_name: str
-    description: str
-    result: StructuredToolResult
-    size: Optional[int] = None
-
-    def as_tool_call_message(self):
-        return {
-            "tool_call_id": self.tool_call_id,
-            "role": "tool",
-            "name": self.tool_name,
-            "content": format_tool_result_data(self.result),
-        }
-
-    def as_tool_result_response(self):
-        result_dump = self.result.model_dump()
-        result_dump["data"] = self.result.get_stringified_data()
-
-        return {
-            "tool_call_id": self.tool_call_id,
-            "tool_name": self.tool_name,
-            "description": self.description,
-            "role": "tool",
-            "result": result_dump,
-        }
-
-    def as_streaming_tool_result_response(self):
-        result_dump = self.result.model_dump()
-        result_dump["data"] = self.result.get_stringified_data()
-
-        return {
-            "tool_call_id": self.tool_call_id,
-            "role": "tool",
-            "description": self.description,
-            "name": self.tool_name,
-            "result": result_dump,
-        }
-
-
-def format_tool_result_data(tool_result: StructuredToolResult) -> str:
-    tool_response = tool_result.data
-    if isinstance(tool_result.data, str):
-        tool_response = tool_result.data
-    else:
-        try:
-            if isinstance(tool_result.data, BaseModel):
-                tool_response = tool_result.data.model_dump_json(indent=2)
-            else:
-                tool_response = json.dumps(tool_result.data, indent=2)
-        except Exception:
-            tool_response = str(tool_result.data)
-    if tool_result.status == StructuredToolResultStatus.ERROR:
-        tool_response = f"{tool_result.error or 'Tool execution failed'}:\n\n{tool_result.data or ''}".strip()
-
-    if tool_result.params:
-        tool_response = (
-            f"Params used for the tool call: {json.dumps(tool_result.params)}. The tool call output follows on the next line.\n"
-            + tool_response
-        )
-    return tool_response
-
 
 class InvestigationResult(BaseModel):
     analysis: Optional[str] = None
     sections: Optional[Dict[str, Union[str, None]]] = None
     tool_calls: List[ToolCallResult] = []
     instructions: List[str] = []
-    metadata: Optional[Dict[Any, Any]] = None
 
 
 class InvestigateRequest(BaseModel):
@@ -165,31 +86,10 @@
     include_tool_call_results: bool = False
 
 
-class PendingToolApproval(BaseModel):
-    """Represents a tool call that requires user approval."""
-
-    tool_call_id: str
-    tool_name: str
-    description: str
-    params: Dict[str, Any]
-
-
-class ToolApprovalDecision(BaseModel):
-    """Represents a user's decision on a tool approval."""
-
-    tool_call_id: str
-    approved: bool
-
-
 class ChatRequestBaseModel(BaseModel):
     conversation_history: Optional[list[dict]] = None
     model: Optional[str] = None
     stream: bool = Field(default=False)
-    enable_tool_approval: Optional[bool] = (
-        False  # Optional boolean for backwards compatibility
-    )
-    tool_decisions: Optional[List[ToolApprovalDecision]] = None
-    additional_system_prompt: Optional[str] = None
 
     # In our setup with litellm, the first message in conversation_history
     # should follow the structure [{"role": "system", "content": ...}],
@@ -245,8 +145,6 @@
     conversation_history: list[dict]
     tool_calls: Optional[List[ToolCallResult]] = []
     follow_up_actions: Optional[List[FollowUpAction]] = []
-    pending_approvals: Optional[List[PendingToolApproval]] = None
-    metadata: Optional[Dict[Any, Any]] = None
 
 
 class WorkloadHealthInvestigationResult(BaseModel):
diff -ur baseline-holmes/holmes/core/openai_formatting.py holmes2/holmes/core/openai_formatting.py
--- baseline-holmes/holmes/core/openai_formatting.py	2025-11-05 16:43:28.204631067 -0800
+++ holmes2/holmes/core/openai_formatting.py	2025-10-17 15:09:28.616777612 -0700
@@ -80,19 +80,6 @@
         )
         if param_attributes.description is not None:
             tool_properties[param_name]["description"] = param_attributes.description
-        # Add enum constraint if specified
-        if hasattr(param_attributes, "enum") and param_attributes.enum:
-            enum_values = list(
-                param_attributes.enum
-            )  # Create a copy to avoid modifying original
-            # In strict mode, optional parameters need None in their enum to match the type allowing null
-            if (
-                strict_mode
-                and not param_attributes.required
-                and None not in enum_values
-            ):
-                enum_values.append(None)
-            tool_properties[param_name]["enum"] = enum_values
 
     result: dict[str, Any] = {
         "type": "function",
Only in holmes2/holmes/core: performance_timing.py
diff -ur baseline-holmes/holmes/core/prompt.py holmes2/holmes/core/prompt.py
--- baseline-holmes/holmes/core/prompt.py	2025-11-05 16:43:28.204728692 -0800
+++ holmes2/holmes/core/prompt.py	2025-10-17 15:09:28.640494469 -0700
@@ -55,10 +55,29 @@
     """
     # Load and render system prompt internally
     system_prompt_template = "builtin://generic_ask.jinja2"
+    
+    # Add KAITO-specific anti-JSON and conciseness system prompt addition
+    anti_json_prompt = """
+
+KAITO ANTI-JSON ENFORCEMENT:
+- You are strictly PROHIBITED from outputting raw JSON as your final response
+- Tool calls are internal operations - the user should never see JSON tool call syntax
+- After tools execute, you MUST provide natural language answers
+- Example: If asked "how many pods?" respond "There are X pods" NOT {"name": "kubectl_get_by_kind_in_namespace", "arguments": {...}}
+- Your final response must be conversational and helpful, never JSON
+
+KAITO CONCISENESS ENFORCEMENT:
+- Keep diagnostic answers direct and concise
+- Lead with the core issue, then brief details if needed
+- For "what's wrong?" questions, state the problem immediately: "The pod was killed due to out of memory"
+- Avoid verbose explanations unless specifically requested  
+- Be actionable and specific rather than lengthy
+"""
+    
     template_context = {
         "toolsets": tool_executor.toolsets,
-        "runbooks_enabled": True if runbooks else False,
-        "system_prompt_additions": system_prompt_additions or "",
+        "runbooks": runbooks or {},
+        "system_prompt_additions": (system_prompt_additions or "") + anti_json_prompt,
     }
     system_prompt_rendered = load_and_render_prompt(
         system_prompt_template, template_context
@@ -69,7 +88,8 @@
         console, initial_user_prompt, file_paths
     )
 
-    user_prompt_with_files += get_tasks_management_system_reminder()
+    # KAITO PATCH: Comment out TodoWrite system reminder to restore Wednesday behavior
+    # user_prompt_with_files += get_tasks_management_system_reminder()
     messages = [
         {"role": "system", "content": system_prompt_rendered},
         {"role": "user", "content": user_prompt_with_files},
diff -ur baseline-holmes/holmes/core/safeguards.py holmes2/holmes/core/safeguards.py
--- baseline-holmes/holmes/core/safeguards.py	2025-11-05 16:43:28.205202524 -0800
+++ holmes2/holmes/core/safeguards.py	2025-10-17 15:09:28.620168502 -0700
@@ -5,7 +5,7 @@
 
 from holmes.common.env_vars import TOOL_CALL_SAFEGUARDS_ENABLED
 from holmes.plugins.toolsets.logging_utils.logging_api import POD_LOGGING_TOOL_NAME
-from holmes.core.tools import StructuredToolResult, StructuredToolResultStatus
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 from holmes.plugins.toolsets.logging_utils.logging_api import FetchPodLogsParams
 
 
@@ -39,7 +39,7 @@
             result = tool_call.get("result", {})
             if (
                 tool_call.get("tool_name") == POD_LOGGING_TOOL_NAME
-                and result.get("status") == StructuredToolResultStatus.NO_DATA
+                and result.get("status") == ToolResultStatus.NO_DATA
                 and result.get("params")
             ):
                 params = FetchPodLogsParams(**result.get("params"))
@@ -94,7 +94,7 @@
                 For example if Holmes checks if a resource is deployed, runs a command to deploy it and then checks again if it has deployed properly.
             """
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=(
                     "Refusing to run this tool call because it has already been called during this session with the exact same parameters.\n"
                     "Move on with your investigation to a different tool or change the parameter values."
@@ -106,7 +106,7 @@
             tool_name=tool_name, tool_params=tool_params, tool_calls=tool_calls
         ):
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=(
                     f"Refusing to run this tool call because the exact same {POD_LOGGING_TOOL_NAME} tool call without filter has already run and returned no data.\n"
                     "This tool call would also have returned no data.\n"
diff -ur baseline-holmes/holmes/core/supabase_dal.py holmes2/holmes/core/supabase_dal.py
--- baseline-holmes/holmes/core/supabase_dal.py	2025-11-05 16:43:28.205327482 -0800
+++ holmes2/holmes/core/supabase_dal.py	2025-10-17 15:09:28.639682892 -0700
@@ -1,16 +1,14 @@
 import base64
 import binascii
-import gzip
 import json
 import logging
 import os
 import threading
 from datetime import datetime, timedelta
-from enum import Enum
 from typing import Dict, List, Optional, Tuple
 from uuid import uuid4
+import gzip
 
-from postgrest.base_request_builder import QueryArgs
 import yaml  # type: ignore
 from cachetools import TTLCache  # type: ignore
 from postgrest._sync.request_builder import SyncQueryRequestBuilder
@@ -32,19 +30,13 @@
     ResourceInstructionDocument,
     ResourceInstructions,
 )
-from holmes.core.truncation.dal_truncation_utils import (
-    truncate_evidences_entities_if_necessary,
-)
-from holmes.plugins.runbooks import RobustaRunbookInstruction
 from holmes.utils.definitions import RobustaConfig
 from holmes.utils.env import get_env_replacement
 from holmes.utils.global_instructions import Instructions
-from postgrest._sync import request_builder as supabase_request_builder
 
 SUPABASE_TIMEOUT_SECONDS = int(os.getenv("SUPABASE_TIMEOUT_SECONDS", 3600))
 
 ISSUES_TABLE = "Issues"
-GROUPED_ISSUES_TABLE = "GroupedIssues"
 EVIDENCE_TABLE = "Evidence"
 RUNBOOKS_TABLE = "HolmesRunbooks"
 SESSION_TOKENS_TABLE = "AuthTokens"
@@ -53,31 +45,6 @@
 SCANS_META_TABLE = "ScansMeta"
 SCANS_RESULTS_TABLE = "ScansResults"
 
-ENRICHMENT_BLACKLIST = ["text_file", "graph", "ai_analysis", "holmes"]
-ENRICHMENT_BLACKLIST_SET = set(ENRICHMENT_BLACKLIST)
-
-
-logging.info("Patching supabase_request_builder.pre_select")
-original_pre_select = supabase_request_builder.pre_select
-
-
-def pre_select_patched(*args, **kwargs):
-    query_args: QueryArgs = original_pre_select(*args, **kwargs)
-    if not query_args.json:
-        query_args = QueryArgs(
-            query_args.method, query_args.params, query_args.headers, None
-        )
-
-    return query_args
-
-
-supabase_request_builder.pre_select = pre_select_patched
-
-
-class FindingType(str, Enum):
-    ISSUE = "issue"
-    CONFIGURATION_CHANGE = "configuration_change"
-
 
 class RobustaToken(BaseModel):
     store_url: str
@@ -92,7 +59,7 @@
         self.enabled = self.__init_config()
         self.cluster = cluster
         if not self.enabled:
-            logging.debug(
+            logging.info(
                 "Not connecting to Robusta platform - robusta token not provided - using ROBUSTA_AI will not be possible"
             )
             return
@@ -150,7 +117,7 @@
                 )
 
         if not os.path.exists(config_file_path):
-            logging.debug(f"No robusta config in {config_file_path}")
+            logging.info(f"No robusta config in {config_file_path}")
             return None
 
         logging.info(f"loading config {config_file_path}")
@@ -164,7 +131,7 @@
                         raise Exception(
                             "No robusta token provided to Holmes.\n"
                             "Please set a valid Robusta UI token.\n "
-                            "See https://holmesgpt.dev/ai-providers/ for instructions."
+                            "See https://docs.robusta.dev/master/configuration/ai-analysis.html#choosing-and-configuring-an-ai-provider for instructions."
                         )
                     env_replacement_token = get_env_replacement(token)
                     if env_replacement_token:
@@ -176,7 +143,7 @@
                             "Ensure your Helm chart or environment variables are set correctly.\n "
                             "If you store the token in a secret, you must also pass "
                             "the environment variable ROBUSTA_UI_TOKEN to Holmes.\n "
-                            "See https://holmesgpt.dev/data-sources/builtin-toolsets/robusta/ for instructions."
+                            "See https://docs.robusta.dev/master/configuration/ai-analysis.html#configuring-holmesgpt-access-to-saas-data for instructions."
                         )
                     try:
                         decoded = base64.b64decode(token)
@@ -263,64 +230,67 @@
             logging.exception("Supabase error while retrieving efficiency data")
             return None
 
-    def get_issues_metadata(
-        self,
-        start_datetime: str,
-        end_datetime: str,
-        limit: int = 100,
-        workload: Optional[str] = None,
-        ns: Optional[str] = None,
-        cluster: Optional[str] = None,
-        finding_type: FindingType = FindingType.CONFIGURATION_CHANGE,
+    def get_configuration_changes(
+        self, start_datetime: str, end_datetime: str
     ) -> Optional[List[Dict]]:
         if not self.enabled:
             return []
-        if not cluster:
-            cluster = self.cluster
+
         try:
-            query = (
+            changes_response = (
                 self.client.table(ISSUES_TABLE)
-                .select(
-                    "id",
-                    "title",
-                    "subject_name",
-                    "subject_namespace",
-                    "subject_type",
-                    "description",
-                    "starts_at",
-                    "ends_at",
-                )
+                .select("id", "subject_name", "subject_namespace", "description")
                 .eq("account_id", self.account_id)
-                .eq("cluster", cluster)
+                .eq("cluster", self.cluster)
+                .eq("finding_type", "configuration_change")
                 .gte("creation_date", start_datetime)
                 .lte("creation_date", end_datetime)
-                .limit(limit)
+                .execute()
             )
+            if not len(changes_response.data):
+                return None
 
-            query = query.eq("finding_type", finding_type.value)
-            if workload:
-                query.eq("subject_name", workload)
-            if ns:
-                query.eq("subject_namespace", ns)
+        except Exception:
+            logging.exception("Supabase error while retrieving change data")
+            return None
 
-            res = query.execute()
-            if not res.data:
+        changes_ids = [change["id"] for change in changes_response.data]
+        try:
+            change_data_response = (
+                self.client.table(EVIDENCE_TABLE)
+                .select("*")
+                .eq("account_id", self.account_id)
+                .in_("issue_id", changes_ids)
+                .execute()
+            )
+            if not len(change_data_response.data):
                 return None
 
         except Exception:
-            logging.exception("Supabase error while retrieving change data")
+            logging.exception("Supabase error while retrieving change content")
             return None
 
+        changes_data = []
+        change_data_map = {
+            change["issue_id"]: change for change in change_data_response.data
+        }
+
+        for change in changes_response.data:
+            change_content = change_data_map.get(change["id"])
+            if change_content:
+                changes_data.append(
+                    {
+                        "change": change_content["data"],
+                        "evidence_id": change_content["id"],
+                        **change,
+                    }
+                )
+
         logging.debug(
-            "Change history metadata for %s-%s workload %s in ns %s: %s",
-            start_datetime,
-            end_datetime,
-            workload,
-            ns,
-            res.data,
+            "Change history for %s-%s: %s", start_datetime, end_datetime, changes_data
         )
 
-        return res.data
+        return changes_data
 
     def unzip_evidence_file(self, data):
         try:
@@ -352,30 +322,22 @@
             return data
 
     def extract_relevant_issues(self, evidence):
+        enrichment_blacklist = {"text_file", "graph", "ai_analysis", "holmes"}
         data = [
             enrich
             for enrich in evidence.data
-            if enrich.get("enrichment_type") not in ENRICHMENT_BLACKLIST_SET
+            if enrich.get("enrichment_type") not in enrichment_blacklist
         ]
 
         unzipped_files = [
             self.unzip_evidence_file(enrich)
             for enrich in evidence.data
             if enrich.get("enrichment_type") == "text_file"
-            or enrich.get("enrichment_type") == "alert_raw_data"
         ]
 
         data.extend(unzipped_files)
         return data
 
-    def get_issue_from_db(self, issue_id: str, table: str) -> Optional[Dict]:
-        issue_response = (
-            self.client.table(table).select("*").filter("id", "eq", issue_id).execute()
-        )
-        if len(issue_response.data):
-            return issue_response.data[0]
-        return None
-
     def get_issue_data(self, issue_id: Optional[str]) -> Optional[Dict]:
         # TODO this could be done in a single atomic SELECT, but there is no
         # foreign key relation between Issues and Evidence.
@@ -385,11 +347,14 @@
             return None
         issue_data = None
         try:
-            issue_data = self.get_issue_from_db(issue_id, ISSUES_TABLE)
-            if issue_data and issue_data["source"] == "prometheus":
-                logging.debug("Getting alert %s from GroupedIssuesTable", issue_id)
-                # This issue will have the complete alert duration information
-                issue_data = self.get_issue_from_db(issue_id, GROUPED_ISSUES_TABLE)
+            issue_response = (
+                self.client.table(ISSUES_TABLE)
+                .select("*")
+                .filter("id", "eq", issue_id)
+                .execute()
+            )
+            if len(issue_response.data):
+                issue_data = issue_response.data[0]
 
         except Exception:  # e.g. invalid id format
             logging.exception("Supabase error while retrieving issue data")
@@ -399,14 +364,12 @@
         evidence = (
             self.client.table(EVIDENCE_TABLE)
             .select("*")
-            .eq("issue_id", issue_id)
-            .not_.in_("enrichment_type", ENRICHMENT_BLACKLIST)
+            .filter("issue_id", "eq", issue_id)
             .execute()
         )
-        relevant_evidence = self.extract_relevant_issues(evidence)
-        truncate_evidences_entities_if_necessary(relevant_evidence)
+        data = self.extract_relevant_issues(evidence)
 
-        issue_data["evidence"] = relevant_evidence
+        issue_data["evidence"] = data
 
         # build issue investigation dates
         started_at = issue_data.get("starts_at")
@@ -430,79 +393,6 @@
 
         return issue_data
 
-    def get_runbook_catalog(self) -> Optional[List[RobustaRunbookInstruction]]:
-        if not self.enabled:
-            return None
-
-        try:
-            res = (
-                self.client.table(RUNBOOKS_TABLE)
-                .select("*")
-                .eq("account_id", self.account_id)
-                .eq("subject_type", "RunbookCatalog")
-                .execute()
-            )
-            if not res.data:
-                return None
-
-            instructions = []
-            for row in res.data:
-                id = row.get("runbook_id")
-                symptom = row.get("symptoms")
-                title = row.get("subject_name")
-                if not symptom:
-                    logging.warning("Skipping runbook with empty symptom: %s", id)
-                    continue
-                instructions.append(
-                    RobustaRunbookInstruction(id=id, symptom=symptom, title=title)
-                )
-            return instructions
-        except Exception:
-            logging.exception("Failed to fetch RunbookCatalog", exc_info=True)
-            return None
-
-    def get_runbook_content(
-        self, runbook_id: str
-    ) -> Optional[RobustaRunbookInstruction]:
-        if not self.enabled:
-            return None
-
-        res = (
-            self.client.table(RUNBOOKS_TABLE)
-            .select("*")
-            .eq("account_id", self.account_id)
-            .eq("subject_type", "RunbookCatalog")
-            .eq("runbook_id", runbook_id)
-            .execute()
-        )
-        if not res.data or len(res.data) != 1:
-            return None
-
-        row = res.data[0]
-        id = row.get("runbook_id")
-        symptom = row.get("symptoms")
-        title = row.get("subject_name")
-        raw_instruction = row.get("runbook").get("instructions")
-        # TODO: remove in the future when we migrate the table data
-        if isinstance(raw_instruction, list) and len(raw_instruction) == 1:
-            instruction = raw_instruction[0]
-        elif isinstance(raw_instruction, list) and len(raw_instruction) > 1:
-            # not currently used, but will be used in the future
-            instruction = "\n - ".join(raw_instruction)
-        elif isinstance(raw_instruction, str):
-            # not supported by the current UI, but will be supported in the future
-            instruction = raw_instruction
-        else:
-            # in case the format is unexpected, convert to string
-            logging.error(
-                f"Unexpected runbook instruction format for runbook_id={runbook_id}: {raw_instruction}"
-            )
-            instruction = str(raw_instruction)
-
-        return RobustaRunbookInstruction(
-            id=id, symptom=symptom, instruction=instruction, title=title
-        )
-
     def get_resource_instructions(
         self, type: str, name: Optional[str]
     ) -> Optional[ResourceInstructions]:
@@ -622,13 +512,10 @@
                 self.client.table(EVIDENCE_TABLE)
                 .select("data, enrichment_type")
                 .in_("issue_id", unique_issues)
-                .not_.in_("enrichment_type", ENRICHMENT_BLACKLIST)
                 .execute()
             )
 
-            relevant_issues = self.extract_relevant_issues(res)
-            truncate_evidences_entities_if_necessary(relevant_issues)
-            return relevant_issues
+            return self.extract_relevant_issues(res)
 
         except Exception:
             logging.exception("failed to fetch workload issues data", exc_info=True)
diff -ur baseline-holmes/holmes/core/tool_calling_llm.py holmes2/holmes/core/tool_calling_llm.py
--- baseline-holmes/holmes/core/tool_calling_llm.py	2025-11-05 16:43:28.205585022 -0800
+++ holmes2/holmes/core/tool_calling_llm.py	2025-10-17 15:09:28.634551807 -0700
@@ -1,14 +1,9 @@
 import concurrent.futures
 import json
 import logging
+import os
 import textwrap
-from typing import Dict, List, Optional, Type, Union, Callable, Any
-
-from holmes.core.models import (
-    ToolApprovalDecision,
-    ToolCallResult,
-    PendingToolApproval,
-)
+from typing import Dict, List, Optional, Type, Union
 
 import sentry_sdk
 from openai import BadRequestError
@@ -19,8 +14,8 @@
 from rich.console import Console
 
 from holmes.common.env_vars import (
-    RESET_REPEATED_TOOL_CALL_CHECK_AFTER_COMPACTION,
     TEMPERATURE,
+    MAX_OUTPUT_TOKEN_RESERVATION,
     LOG_LLM_USAGE_RESPONSE,
 )
 
@@ -33,37 +28,21 @@
 )
 from holmes.core.issue import Issue
 from holmes.core.llm import LLM
+from holmes.core.performance_timing import PerformanceTiming
 from holmes.core.resource_instruction import ResourceInstructions
 from holmes.core.runbooks import RunbookManager
 from holmes.core.safeguards import prevent_overly_repeated_tool_call
-from holmes.core.tools import (
-    StructuredToolResult,
-    StructuredToolResultStatus,
-    ToolInvokeContext,
-)
-from holmes.core.tools_utils.tool_context_window_limiter import (
-    prevent_overly_big_tool_response,
-)
-from holmes.core.truncation.input_context_window_limiter import (
-    limit_input_context_window,
-)
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 from holmes.plugins.prompts import load_and_render_prompt
-from holmes.plugins.runbooks import RunbookCatalog
-from holmes.utils import sentry_helper
 from holmes.utils.global_instructions import (
     Instructions,
-    add_runbooks_to_user_prompt,
+    add_global_instructions_to_user_prompt,
 )
 from holmes.utils.tags import format_tags_in_string, parse_messages_tags
 from holmes.core.tools_utils.tool_executor import ToolExecutor
 from holmes.core.tracing import DummySpan
 from holmes.utils.colors import AI_COLOR
-from holmes.utils.stream import (
-    StreamEvents,
-    StreamMessage,
-    add_token_count_to_metadata,
-    build_stream_event_token_count,
-)
+from holmes.utils.stream import StreamEvents, StreamMessage
 
 # Create a named logger for cost tracking
 cost_logger = logging.getLogger("holmes.costs")
@@ -140,6 +119,148 @@
         logging.debug(f"Could not extract cost information: {e}")
 
 
+def format_tool_result_data(tool_result: StructuredToolResult) -> str:
+    tool_response = tool_result.data
+    if isinstance(tool_result.data, str):
+        tool_response = tool_result.data
+    else:
+        try:
+            if isinstance(tool_result.data, BaseModel):
+                tool_response = tool_result.data.model_dump_json(indent=2)
+            else:
+                tool_response = json.dumps(tool_result.data, indent=2)
+        except Exception:
+            tool_response = str(tool_result.data)
+    if tool_result.status == ToolResultStatus.ERROR:
+        tool_response = f"{tool_result.error or 'Tool execution failed'}:\n\n{tool_result.data or ''}".strip()
+    return tool_response
+
+
+# TODO: I think there's a bug here because we don't account for the 'role' or json structure like '{...}' when counting tokens
+# However, in practice it works because we reserve enough space for the output tokens that the minor inconsistency does not matter
+# We should fix this in the future
+# TODO: we truncate using character counts not token counts - this means we're overly agressive with truncation - improve it by considering
+# token truncation and not character truncation
+def truncate_messages_to_fit_context(
+    messages: list, max_context_size: int, maximum_output_token: int, count_tokens_fn
+) -> list:
+    """
+    Helper function to truncate tool messages to fit within context limits.
+
+    Args:
+        messages: List of message dictionaries with roles and content
+        max_context_size: Maximum context window size for the model
+        maximum_output_token: Maximum tokens reserved for model output
+        count_tokens_fn: Function to count tokens for a list of messages
+
+    Returns:
+        Modified list of messages with truncated tool responses
+
+    Raises:
+        Exception: If non-tool messages exceed available context space
+    """
+    messages_except_tools = [
+        message for message in messages if message["role"] != "tool"
+    ]
+    message_size_without_tools = count_tokens_fn(messages_except_tools)
+
+    tool_call_messages = [message for message in messages if message["role"] == "tool"]
+
+    reserved_for_output_tokens = min(maximum_output_token, MAX_OUTPUT_TOKEN_RESERVATION)
+    if message_size_without_tools >= (max_context_size - reserved_for_output_tokens):
+        logging.error(
+            f"The combined size of system_prompt and user_prompt ({message_size_without_tools} tokens) exceeds the model's context window for input."
+        )
+        raise Exception(
+            f"The combined size of system_prompt and user_prompt ({message_size_without_tools} tokens) exceeds the maximum context size of {max_context_size - reserved_for_output_tokens} tokens available for input."
+        )
+
+    if len(tool_call_messages) == 0:
+        return messages
+
+    available_space = (
+        max_context_size - message_size_without_tools - maximum_output_token
+    )
+    remaining_space = available_space
+    tool_call_messages.sort(key=lambda x: len(x["content"]))
+
+    # Allocate space starting with small tools and going to larger tools, while maintaining fairness
+    # Small tools can often get exactly what they need, while larger tools may need to be truncated
+    # We ensure fairness (no tool gets more than others that need it) and also maximize utilization (we don't leave space unused)
+    for i, msg in enumerate(tool_call_messages):
+        remaining_tools = len(tool_call_messages) - i
+        max_allocation = remaining_space // remaining_tools
+        needed_space = len(msg["content"])
+        allocated_space = min(needed_space, max_allocation)
+
+        if needed_space > allocated_space:
+            truncation_notice = "\n\n[TRUNCATED]"
+            # Ensure the indicator fits in the allocated space
+            if allocated_space > len(truncation_notice):
+                msg["content"] = (
+                    msg["content"][: allocated_space - len(truncation_notice)]
+                    + truncation_notice
+                )
+                logging.info(
+                    f"Truncating tool message '{msg['name']}' from {needed_space} to {allocated_space-len(truncation_notice)} tokens"
+                )
+            else:
+                msg["content"] = truncation_notice[:allocated_space]
+                logging.info(
+                    f"Truncating tool message '{msg['name']}' from {needed_space} to {allocated_space} tokens"
+                )
+            msg.pop("token_count", None)  # Remove token_count if present
+
+        remaining_space -= allocated_space
+    return messages
+
+
+class ToolCallResult(BaseModel):
+    tool_call_id: str
+    tool_name: str
+    description: str
+    result: StructuredToolResult
+    size: Optional[int] = None
+
+    def as_tool_call_message(self):
+        content = format_tool_result_data(self.result)
+        if self.result.params:
+            content = (
+                f"Params used for the tool call: {json.dumps(self.result.params)}. The tool call output follows on the next line.\n"
+                + content
+            )
+        return {
+            "tool_call_id": self.tool_call_id,
+            "role": "tool",
+            "name": self.tool_name,
+            "content": content,
+        }
+
+    def as_tool_result_response(self):
+        result_dump = self.result.model_dump()
+        result_dump["data"] = self.result.get_stringified_data()
+
+        return {
+            "tool_call_id": self.tool_call_id,
+            "tool_name": self.tool_name,
+            "description": self.description,
+            "role": "tool",
+            "result": result_dump,
+        }
+
+    def as_streaming_tool_result_response(self):
+        result_dump = self.result.model_dump()
+        result_dump["data"] = self.result.get_stringified_data()
+
+        return {
+            "tool_call_id": self.tool_call_id,
+            "role": "tool",
+            "description": self.description,
+            "name": self.tool_name,
+            "result": result_dump,
+        }
+
+
 class LLMResult(LLMCosts):
     tool_calls: Optional[List[ToolCallResult]] = None
     result: Optional[str] = None
@@ -148,7 +269,6 @@
     # TODO: clean up these two
     prompt: Optional[str] = None
     messages: Optional[List[dict]] = None
-    metadata: Optional[Dict[Any, Any]] = None
 
     def get_tool_usage_summary(self):
         return "AI used info from issue and " + ",".join(
@@ -156,12 +276,6 @@
         )
 
 
-class ToolCallWithDecision(BaseModel):
-    message_index: int
-    tool_call: ChatCompletionMessageToolCall
-    decision: Optional[ToolApprovalDecision]
-
-
 class ToolCallingLLM:
     llm: LLM
 
@@ -172,98 +286,6 @@
         self.max_steps = max_steps
         self.tracer = tracer
         self.llm = llm
-        self.approval_callback: Optional[
-            Callable[[StructuredToolResult], tuple[bool, Optional[str]]]
-        ] = None
-
-    def process_tool_decisions(
-        self, messages: List[Dict[str, Any]], tool_decisions: List[ToolApprovalDecision]
-    ) -> tuple[List[Dict[str, Any]], list[StreamMessage]]:
-        """
-        Process tool approval decisions and execute approved tools.
-
-        Args:
-            messages: Current conversation messages
-            tool_decisions: List of ToolApprovalDecision objects
-
-        Returns:
-            Updated messages list with tool execution results
-        """
-        events: list[StreamMessage] = []
-        if not tool_decisions:
-            return messages, events
-
-        # Create decision lookup
-        decisions_by_tool_call_id = {
-            decision.tool_call_id: decision for decision in tool_decisions
-        }
-
-        pending_tool_calls: list[ToolCallWithDecision] = []
-
-        for i in reversed(range(len(messages))):
-            msg = messages[i]
-            if msg.get("role") == "assistant" and msg.get("tool_calls"):
-                message_tool_calls = msg.get("tool_calls", [])
-                for tool_call in message_tool_calls:
-                    decision = decisions_by_tool_call_id.get(tool_call.get("id"), None)
-                    if tool_call.get("pending_approval"):
-                        del tool_call[
-                            "pending_approval"
-                        ]  # Cleanup so that a pending approval is not tagged on message in a future response
-                        pending_tool_calls.append(
-                            ToolCallWithDecision(
-                                tool_call=ChatCompletionMessageToolCall(**tool_call),
-                                decision=decision,
-                                message_index=i,
-                            )
-                        )
-
-        if not pending_tool_calls:
-            error_message = f"Received {len(tool_decisions)} tool decisions but no pending approvals found"
-            logging.error(error_message)
-            raise Exception(error_message)
-        for tool_call_with_decision in pending_tool_calls:
-            tool_call_message: dict
-            tool_call = tool_call_with_decision.tool_call
-            decision = tool_call_with_decision.decision
-            tool_result: Optional[ToolCallResult] = None
-            if decision and decision.approved:
-                tool_result = self._invoke_llm_tool_call(
-                    tool_to_call=tool_call,
-                    previous_tool_calls=[],
-                    trace_span=DummySpan(),  # TODO: replace with proper span
-                    tool_number=None,
-                    user_approved=True,
-                )
-            else:
-                # Tool was rejected or no decision found, add rejection message
-                tool_result = ToolCallResult(
-                    tool_call_id=tool_call.id,
-                    tool_name=tool_call.function.name,
-                    description=tool_call.function.name,
-                    result=StructuredToolResult(
-                        status=StructuredToolResultStatus.ERROR,
-                        error="Tool execution was denied by the user.",
-                    ),
-                )
-
-            events.append(
-                StreamMessage(
-                    event=StreamEvents.TOOL_RESULT,
-                    data=tool_result.as_streaming_tool_result_response(),
-                )
-            )
-
-            tool_call_message = tool_result.as_tool_call_message()
-
-            # It is expected that the tool call result directly follows the tool call request from the LLM
-            # The API call may contain a user ask which is appended to the messages so we can't just append
-            # tool call results; they need to be inserted right after the llm's message requesting tool calls
-            messages.insert(
-                tool_call_with_decision.message_index + 1, tool_call_message
-            )
-
-        return messages, events
 
     def prompt_call(
         self,
@@ -309,35 +331,37 @@
         trace_span=DummySpan(),
         tool_number_offset: int = 0,
     ) -> LLMResult:
-        tool_calls: list[
-            dict
-        ] = []  # Used for preventing repeated tool calls. potentially reset after compaction
-        all_tool_calls = []  # type: ignore
+        perf_timing = PerformanceTiming("tool_calling_llm.call")
+        tool_calls = []  # type: ignore
         costs = LLMCosts()
+
         tools = self.tool_executor.get_all_tools_openai_format(
             target_model=self.llm.model
         )
+        perf_timing.measure("get_all_tools_openai_format")
         max_steps = self.max_steps
         i = 0
-        metadata: Dict[Any, Any] = {}
+
         while i < max_steps:
             i += 1
+            perf_timing.measure(f"start iteration {i}")
             logging.debug(f"running iteration {i}")
             # on the last step we don't allow tools - we want to force a reply, not a request to run another tool
             tools = None if i == max_steps else tools
-            tool_choice = "auto" if tools else None
+            tool_choice = os.environ.get("HOLMES_TOOL_CHOICE", "auto") if tools else None
+            logging.debug(f"HOLMES_TOOL_CHOICE env var: {os.environ.get('HOLMES_TOOL_CHOICE', 'NOT_SET')}, using tool_choice: {tool_choice}")
 
-            limit_result = limit_input_context_window(
-                llm=self.llm, messages=messages, tools=tools
-            )
-            messages = limit_result.messages
-            metadata = metadata | limit_result.metadata
-
-            if (
-                limit_result.conversation_history_compacted
-                and RESET_REPEATED_TOOL_CALL_CHECK_AFTER_COMPACTION
-            ):
-                tool_calls = []
+            total_tokens = self.llm.count_tokens_for_message(messages)
+            max_context_size = self.llm.get_context_window_size()
+            maximum_output_token = self.llm.get_maximum_output_token()
+            perf_timing.measure("count tokens")
+
+            if (total_tokens + maximum_output_token) > max_context_size:
+                logging.warning("Token limit exceeded. Truncating tool responses.")
+                messages = self.truncate_messages_to_fit_context(
+                    messages, max_context_size, maximum_output_token
+                )
+                perf_timing.measure("truncate_messages_to_fit_context")
 
             logging.debug(f"sending messages={messages}\n\ntools={tools}")
 
@@ -355,6 +379,7 @@
                 # Extract and accumulate cost information
                 _process_cost_info(full_response, costs, "LLM call")
 
+                perf_timing.measure("llm.completion")
             # catch a known error that occurs with Azure and replace the error message with something more obvious to the user
             except BadRequestError as e:
                 if "Unrecognized request arguments supplied: tool_choice, tools" in str(
@@ -378,10 +403,9 @@
 
                 if incorrect_tool_call:
                     logging.warning(
-                        "Detected incorrect tool call. Structured output will be disabled. This can happen on models that do not support tool calling. For Azure AI, make sure the model name contains 'gpt-4.1' or other structured output compatible models. To disable this holmes behaviour, set REQUEST_STRUCTURED_OUTPUT_FROM_LLM to `false`."
+                        "Detected incorrect tool call. Structured output will be disabled. This can happen on models that do not support tool calling. For Azure AI, make sure the model name contains 'gpt-4o'. To disable this holmes behaviour, set REQUEST_STRUCTURED_OUTPUT_FROM_LLM to `false`."
                     )
                     # disable structured output going forward and and retry
-                    sentry_helper.capture_structured_output_incorrect_tool_call()
                     response_format = None
                     max_steps = max_steps + 1
                     continue
@@ -398,8 +422,8 @@
                 hasattr(response_message, "reasoning_content")
                 and response_message.reasoning_content
             ):
-                logging.info(
-                    f"[italic dim]AI reasoning:\n\n{response_message.reasoning_content}[/italic dim]\n"
+                logging.debug(
+                    f"[bold {AI_COLOR}]AI (reasoning) :[/bold {AI_COLOR}] {response_message.reasoning_content}\n"
                 )
 
             if not tools_to_call:
@@ -417,33 +441,23 @@
                     )
                     costs.total_cost += post_processing_cost
 
-                    tokens = self.llm.count_tokens(messages=messages, tools=tools)
-
-                    add_token_count_to_metadata(
-                        tokens=tokens,
-                        full_llm_response=full_response,
-                        max_context_size=limit_result.max_context_size,
-                        maximum_output_token=limit_result.maximum_output_token,
-                        metadata=metadata,
-                    )
-
+                    perf_timing.end(f"- completed in {i} iterations -")
                     return LLMResult(
                         result=post_processed_response,
                         unprocessed_result=raw_response,
-                        tool_calls=all_tool_calls,
+                        tool_calls=tool_calls,
                         prompt=json.dumps(messages, indent=2),
                         messages=messages,
                         **costs.model_dump(),  # Include all cost fields
-                        metadata=metadata,
                     )
 
+                perf_timing.end(f"- completed in {i} iterations -")
                 return LLMResult(
                     result=text_response,
-                    tool_calls=all_tool_calls,
+                    tool_calls=tool_calls,
                     prompt=json.dumps(messages, indent=2),
                     messages=messages,
                     **costs.model_dump(),  # Include all cost fields
-                    metadata=metadata,
                 )
 
             if text_response and text_response.strip():
@@ -451,55 +465,28 @@
             logging.info(
                 f"The AI requested [bold]{len(tools_to_call) if tools_to_call else 0}[/bold] tool call(s)."
             )
+            perf_timing.measure("pre-tool-calls")
             with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
                 futures = []
-                futures_tool_numbers: dict[
-                    concurrent.futures.Future, Optional[int]
-                ] = {}
-                tool_number: Optional[int]
                 for tool_index, t in enumerate(tools_to_call, 1):
                     logging.debug(f"Tool to call: {t}")
-                    tool_number = tool_number_offset + tool_index
-
-                    future = executor.submit(
-                        self._invoke_llm_tool_call,
-                        tool_to_call=t,
-                        previous_tool_calls=tool_calls,
-                        trace_span=trace_span,
-                        tool_number=tool_number,
+                    futures.append(
+                        executor.submit(
+                            self._invoke_tool,
+                            tool_to_call=t,
+                            previous_tool_calls=tool_calls,
+                            trace_span=trace_span,
+                            tool_number=tool_number_offset + tool_index,
+                        )
                     )
-                    futures_tool_numbers[future] = tool_number
-                    futures.append(future)
 
                 for future in concurrent.futures.as_completed(futures):
                     tool_call_result: ToolCallResult = future.result()
 
-                    tool_number = (
-                        futures_tool_numbers[future]
-                        if future in futures_tool_numbers
-                        else None
-                    )
-
-                    if (
-                        tool_call_result.result.status
-                        == StructuredToolResultStatus.APPROVAL_REQUIRED
-                    ):
-                        with trace_span.start_span(type="tool") as tool_span:
-                            tool_call_result = self._handle_tool_call_approval(
-                                tool_call_result=tool_call_result,
-                                tool_number=tool_number,
-                            )
-                            ToolCallingLLM._log_tool_call_result(
-                                tool_span, tool_call_result
-                            )
-
-                    tool_result_response_dict = (
-                        tool_call_result.as_tool_result_response()
-                    )
-                    tool_calls.append(tool_result_response_dict)
-                    all_tool_calls.append(tool_result_response_dict)
+                    tool_calls.append(tool_call_result.as_tool_result_response())
                     messages.append(tool_call_result.as_tool_call_message())
-                    tokens = self.llm.count_tokens(messages=messages, tools=tools)
+
+                    perf_timing.measure(f"tool completed {tool_call_result.tool_name}")
 
                 # Update the tool number offset for the next iteration
                 tool_number_offset += len(tools_to_call)
@@ -510,206 +497,122 @@
 
         raise Exception(f"Too many LLM calls - exceeded max_steps: {i}/{max_steps}")
 
-    def _directly_invoke_tool_call(
-        self,
-        tool_name: str,
-        tool_params: dict,
-        user_approved: bool,
-        tool_number: Optional[int] = None,
-    ) -> StructuredToolResult:
-        tool = self.tool_executor.get_tool_by_name(tool_name)
-        if not tool:
-            logging.warning(
-                f"Skipping tool execution for {tool_name}: args: {tool_params}"
-            )
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=f"Failed to find tool {tool_name}",
-                params=tool_params,
-            )
-
-        try:
-            invoke_context = ToolInvokeContext(
-                tool_number=tool_number,
-                user_approved=user_approved,
-                llm=self.llm,
-                max_token_count=self.llm.get_max_token_count_for_single_tool(),
-            )
-            tool_response = tool.invoke(tool_params, context=invoke_context)
-        except Exception as e:
-            logging.error(
-                f"Tool call to {tool_name} failed with an Exception", exc_info=True
-            )
-            tool_response = StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=f"Tool call failed: {e}",
-                params=tool_params,
-            )
-        return tool_response
-
-    def _get_tool_call_result(
+    def _invoke_tool(
         self,
-        tool_call_id: str,
-        tool_name: str,
-        tool_arguments: str,
-        user_approved: bool,
+        tool_to_call: ChatCompletionMessageToolCall,
         previous_tool_calls: list[dict],
-        tool_number: Optional[int] = None,
+        trace_span=DummySpan(),
+        tool_number=None,
     ) -> ToolCallResult:
-        tool_params = {}
+        # Handle the union type - ChatCompletionMessageToolCall can be either
+        # ChatCompletionMessageFunctionToolCall (with 'function' field and type='function')
+        # or ChatCompletionMessageCustomToolCall (with 'custom' field and type='custom').
+        # We use hasattr to check for the 'function' attribute as it's more flexible
+        # and doesn't require importing the specific type.
+        if hasattr(tool_to_call, "function"):
+            tool_name = tool_to_call.function.name
+            tool_arguments = tool_to_call.function.arguments
+        else:
+            # This is a custom tool call - we don't support these currently
+            logging.error(f"Unsupported custom tool call: {tool_to_call}")
+            return ToolCallResult(
+                tool_call_id=tool_to_call.id,
+                tool_name="unknown",
+                description="NA",
+                result=StructuredToolResult(
+                    status=ToolResultStatus.ERROR,
+                    error="Custom tool calls are not supported",
+                    params=None,
+                ),
+            )
+
+        tool_params = None
         try:
             tool_params = json.loads(tool_arguments)
         except Exception:
             logging.warning(
                 f"Failed to parse arguments for tool: {tool_name}. args: {tool_arguments}"
             )
+        tool_call_id = tool_to_call.id
+        tool = self.tool_executor.get_tool_by_name(tool_name)
+
+        if (not tool) or (tool_params is None):
+            logging.warning(
+                f"Skipping tool execution for {tool_name}: args: {tool_arguments}"
+            )
+            return ToolCallResult(
+                tool_call_id=tool_call_id,
+                tool_name=tool_name,
+                description="NA",
+                result=StructuredToolResult(
+                    status=ToolResultStatus.ERROR,
+                    error=f"Failed to find tool {tool_name}",
+                    params=tool_params,
+                ),
+            )
 
         tool_response = None
-        if not user_approved:
+
+        # Create tool span if tracing is enabled
+        tool_span = trace_span.start_span(name=tool_name, type="tool")
+
+        try:
             tool_response = prevent_overly_repeated_tool_call(
-                tool_name=tool_name,
+                tool_name=tool.name,
                 tool_params=tool_params,
                 tool_calls=previous_tool_calls,
             )
+            if not tool_response:
+                tool_response = tool.invoke(tool_params, tool_number=tool_number)
 
-        if not tool_response:
-            tool_response = self._directly_invoke_tool_call(
-                tool_name=tool_name,
-                tool_params=tool_params,
-                user_approved=user_approved,
-                tool_number=tool_number,
+            if not isinstance(tool_response, StructuredToolResult):
+                # Should never be needed but ensure Holmes does not crash if one of the tools does not return the right type
+                logging.error(
+                    f"Tool {tool.name} return type is not StructuredToolResult. Nesting the tool result into StructuredToolResult..."
+                )
+                tool_response = StructuredToolResult(
+                    status=ToolResultStatus.SUCCESS,
+                    data=tool_response,
+                    params=tool_params,
+                )
+
+            # Log tool execution to trace span
+            tool_span.log(
+                input=tool_params,
+                output=tool_response.data,
+                metadata={
+                    "status": tool_response.status.value,
+                    "error": tool_response.error,
+                    "description": tool.get_parameterized_one_liner(tool_params),
+                    "structured_tool_result": tool_response,
+                },
             )
 
-        if not isinstance(tool_response, StructuredToolResult):
-            # Should never be needed but ensure Holmes does not crash if one of the tools does not return the right type
+        except Exception as e:
             logging.error(
-                f"Tool {tool_name} return type is not StructuredToolResult. Nesting the tool result into StructuredToolResult..."
+                f"Tool call to {tool_name} failed with an Exception", exc_info=True
             )
             tool_response = StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=tool_response,
+                status=ToolResultStatus.ERROR,
+                error=f"Tool call failed: {e}",
                 params=tool_params,
             )
 
-        tool = self.tool_executor.get_tool_by_name(tool_name)
-
+            # Log error to trace span
+            tool_span.log(
+                input=tool_params, output=str(e), metadata={"status": "ERROR"}
+            )
+        finally:
+            # End tool span
+            tool_span.end()
         return ToolCallResult(
             tool_call_id=tool_call_id,
             tool_name=tool_name,
-            description=str(tool.get_parameterized_one_liner(tool_params))
-            if tool
-            else "",
+            description=tool.get_parameterized_one_liner(tool_params),
             result=tool_response,
         )
 
     @staticmethod
-    def _log_tool_call_result(tool_span, tool_call_result: ToolCallResult):
-        tool_span.set_attributes(name=tool_call_result.tool_name)
-        tool_span.log(
-            input=tool_call_result.result.params,
-            output=tool_call_result.result.data,
-            error=tool_call_result.result.error,
-            metadata={
-                "status": tool_call_result.result.status,
-                "description": tool_call_result.description,
-            },
-        )
-
-    def _invoke_llm_tool_call(
-        self,
-        tool_to_call: ChatCompletionMessageToolCall,
-        previous_tool_calls: list[dict],
-        trace_span=None,
-        tool_number=None,
-        user_approved: bool = False,
-    ) -> ToolCallResult:
-        if trace_span is None:
-            trace_span = DummySpan()
-        with trace_span.start_span(type="tool") as tool_span:
-            if not hasattr(tool_to_call, "function"):
-                # Handle the union type - ChatCompletionMessageToolCall can be either
-                # ChatCompletionMessageFunctionToolCall (with 'function' field and type='function')
-                # or ChatCompletionMessageCustomToolCall (with 'custom' field and type='custom').
-                # We use hasattr to check for the 'function' attribute as it's more flexible
-                # and doesn't require importing the specific type.
-                tool_name = "Unknown_Custom_Tool"
-                logging.error(f"Unsupported custom tool call: {tool_to_call}")
-                tool_call_result = ToolCallResult(
-                    tool_call_id=tool_to_call.id,
-                    tool_name=tool_name,
-                    description="NA",
-                    result=StructuredToolResult(
-                        status=StructuredToolResultStatus.ERROR,
-                        error="Custom tool calls are not supported",
-                        params=None,
-                    ),
-                )
-            else:
-                tool_name = tool_to_call.function.name
-                tool_arguments = tool_to_call.function.arguments
-                tool_id = tool_to_call.id
-                tool_call_result = self._get_tool_call_result(
-                    tool_id,
-                    tool_name,
-                    tool_arguments,
-                    previous_tool_calls=previous_tool_calls,
-                    tool_number=tool_number,
-                    user_approved=user_approved,
-                )
-
-            prevent_overly_big_tool_response(
-                tool_call_result=tool_call_result, llm=self.llm
-            )
-
-            ToolCallingLLM._log_tool_call_result(tool_span, tool_call_result)
-            return tool_call_result
-
-    def _handle_tool_call_approval(
-        self,
-        tool_call_result: ToolCallResult,
-        tool_number: Optional[int],
-    ) -> ToolCallResult:
-        """
-        Handle approval for a single tool call if required.
-
-        Args:
-            tool_call_result: A single tool call result that may require approval
-            tool_number: The tool call number
-
-        Returns:
-            Updated tool call result with approved/denied status
-        """
-
-        # If no approval callback, convert to ERROR because it is assumed the client may not be able to handle approvals
-        if not self.approval_callback:
-            tool_call_result.result.status = StructuredToolResultStatus.ERROR
-            return tool_call_result
-
-        # Get approval from user
-        approved, feedback = self.approval_callback(tool_call_result.result)
-
-        if approved:
-            logging.debug(
-                f"User approved command: {tool_call_result.result.invocation}"
-            )
-            new_response = self._directly_invoke_tool_call(
-                tool_name=tool_call_result.tool_name,
-                tool_params=tool_call_result.result.params or {},
-                user_approved=True,
-                tool_number=tool_number,
-            )
-            tool_call_result.result = new_response
-        else:
-            # User denied - update to error
-            feedback_text = f" User feedback: {feedback}" if feedback else ""
-            tool_call_result.result.status = StructuredToolResultStatus.ERROR
-            tool_call_result.result.error = (
-                f"User denied command execution.{feedback_text}"
-            )
-
-        return tool_call_result
-
-    @staticmethod
     def __load_post_processing_user_prompt(
         input_prompt, investigation, user_prompt: Optional[str] = None
     ) -> str:
@@ -757,6 +660,17 @@
             logging.exception("Failed to run post processing", exc_info=True)
             return investigation, 0.0
 
+    @sentry_sdk.trace
+    def truncate_messages_to_fit_context(
+        self, messages: list, max_context_size: int, maximum_output_token: int
+    ) -> list:
+        return truncate_messages_to_fit_context(
+            messages,
+            max_context_size,
+            maximum_output_token,
+            self.llm.count_tokens_for_message,
+        )
+
     def call_stream(
         self,
         system_prompt: str = "",
@@ -764,55 +678,48 @@
         response_format: Optional[Union[dict, Type[BaseModel]]] = None,
         sections: Optional[InputSectionsDataType] = None,
         msgs: Optional[list[dict]] = None,
-        enable_tool_approval: bool = False,
-        tool_decisions: List[ToolApprovalDecision] | None = None,
     ):
         """
         This function DOES NOT call llm.completion(stream=true).
         This function streams holmes one iteration at a time instead of waiting for all iterations to complete.
         """
-
-        # Process tool decisions if provided
-        if msgs and tool_decisions:
-            logging.info(f"Processing {len(tool_decisions)} tool decisions")
-            msgs, events = self.process_tool_decisions(msgs, tool_decisions)
-            yield from events
-
-        messages: list[dict] = []
+        messages = []
         if system_prompt:
             messages.append({"role": "system", "content": system_prompt})
         if user_prompt:
             messages.append({"role": "user", "content": user_prompt})
         if msgs:
             messages.extend(msgs)
+        perf_timing = PerformanceTiming("tool_calling_llm.call")
         tool_calls: list[dict] = []
         tools = self.tool_executor.get_all_tools_openai_format(
             target_model=self.llm.model
         )
+        perf_timing.measure("get_all_tools_openai_format")
         max_steps = self.max_steps
-        metadata: Dict[Any, Any] = {}
         i = 0
         tool_number_offset = 0
 
         while i < max_steps:
             i += 1
+            perf_timing.measure(f"start iteration {i}")
             logging.debug(f"running iteration {i}")
 
             tools = None if i == max_steps else tools
-            tool_choice = "auto" if tools else None
-
-            limit_result = limit_input_context_window(
-                llm=self.llm, messages=messages, tools=tools
-            )
-            yield from limit_result.events
-            messages = limit_result.messages
-            metadata = metadata | limit_result.metadata
+            tool_choice = os.environ.get("HOLMES_TOOL_CHOICE", "auto") if tools else None
+            logging.debug(f"HOLMES_TOOL_CHOICE env var: {os.environ.get('HOLMES_TOOL_CHOICE', 'NOT_SET')}, using tool_choice: {tool_choice}")
 
-            if (
-                limit_result.conversation_history_compacted
-                and RESET_REPEATED_TOOL_CALL_CHECK_AFTER_COMPACTION
-            ):
-                tool_calls = []
+            total_tokens = self.llm.count_tokens_for_message(messages)  # type: ignore
+            max_context_size = self.llm.get_context_window_size()
+            maximum_output_token = self.llm.get_maximum_output_token()
+            perf_timing.measure("count tokens")
+
+            if (total_tokens + maximum_output_token) > max_context_size:
+                logging.warning("Token limit exceeded. Truncating tool responses.")
+                messages = self.truncate_messages_to_fit_context(
+                    messages, max_context_size, maximum_output_token
+                )
+                perf_timing.measure("truncate_messages_to_fit_context")
 
             logging.debug(f"sending messages={messages}\n\ntools={tools}")
             try:
@@ -829,6 +736,7 @@
                 # Log cost information for this iteration (no accumulation in streaming)
                 _process_cost_info(full_response, log_prefix="LLM iteration")
 
+                perf_timing.measure("llm.completion")
             # catch a known error that occurs with Azure and replace the error message with something more obvious to the user
             except BadRequestError as e:
                 if "Unrecognized request arguments supplied: tool_choice, tools" in str(
@@ -850,10 +758,9 @@
 
                 if incorrect_tool_call:
                     logging.warning(
-                        "Detected incorrect tool call. Structured output will be disabled. This can happen on models that do not support tool calling. For Azure AI, make sure the model name contains 'gpt-4.1' or other structured output compatible models. To disable this holmes behaviour, set REQUEST_STRUCTURED_OUTPUT_FROM_LLM to `false`."
+                        "Detected incorrect tool call. Structured output will be disabled. This can happen on models that do not support tool calling. For Azure AI, make sure the model name contains 'gpt-4o'. To disable this holmes behaviour, set REQUEST_STRUCTURED_OUTPUT_FROM_LLM to `false`."
                     )
                     # disable structured output going forward and and retry
-                    sentry_helper.capture_structured_output_incorrect_tool_call()
                     response_format = None
                     max_steps = max_steps + 1
                     continue
@@ -864,25 +771,11 @@
                 )
             )
 
-            tokens = self.llm.count_tokens(messages=messages, tools=tools)
-            add_token_count_to_metadata(
-                tokens=tokens,
-                full_llm_response=full_response,
-                max_context_size=limit_result.max_context_size,
-                maximum_output_token=limit_result.maximum_output_token,
-                metadata=metadata,
-            )
-            yield build_stream_event_token_count(metadata=metadata)
-
             tools_to_call = getattr(response_message, "tool_calls", None)
             if not tools_to_call:
                 yield StreamMessage(
                     event=StreamEvents.ANSWER_END,
-                    data={
-                        "content": response_message.content,
-                        "messages": messages,
-                        "metadata": metadata,
-                    },
+                    data={"content": response_message.content, "messages": messages},
                 )
                 return
 
@@ -891,30 +784,22 @@
             if reasoning or message:
                 yield StreamMessage(
                     event=StreamEvents.AI_MESSAGE,
-                    data={
-                        "content": message,
-                        "reasoning": reasoning,
-                        "metadata": metadata,
-                    },
+                    data={"content": message, "reasoning": reasoning},
                 )
 
-            # Check if any tools require approval first
-            pending_approvals = []
-            approval_required_tools = []
-
+            perf_timing.measure("pre-tool-calls")
             with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
                 futures = []
                 for tool_index, t in enumerate(tools_to_call, 1):  # type: ignore
-                    tool_number = tool_number_offset + tool_index
-
-                    future = executor.submit(
-                        self._invoke_llm_tool_call,
-                        tool_to_call=t,  # type: ignore
-                        previous_tool_calls=tool_calls,
-                        trace_span=DummySpan(),  # Streaming mode doesn't support tracing yet
-                        tool_number=tool_number,
+                    futures.append(
+                        executor.submit(
+                            self._invoke_tool,
+                            tool_to_call=t,  # type: ignore
+                            previous_tool_calls=tool_calls,
+                            trace_span=DummySpan(),  # Streaming mode doesn't support tracing yet
+                            tool_number=tool_number_offset + tool_index,
+                        )
                     )
-                    futures.append(future)
                     yield StreamMessage(
                         event=StreamEvents.START_TOOL,
                         data={"tool_name": t.function.name, "id": t.id},
@@ -923,72 +808,15 @@
                 for future in concurrent.futures.as_completed(futures):
                     tool_call_result: ToolCallResult = future.result()
 
-                    if (
-                        tool_call_result.result.status
-                        == StructuredToolResultStatus.APPROVAL_REQUIRED
-                    ):
-                        if enable_tool_approval:
-                            pending_approvals.append(
-                                PendingToolApproval(
-                                    tool_call_id=tool_call_result.tool_call_id,
-                                    tool_name=tool_call_result.tool_name,
-                                    description=tool_call_result.description,
-                                    params=tool_call_result.result.params or {},
-                                )
-                            )
-                            approval_required_tools.append(tool_call_result)
-
-                            yield StreamMessage(
-                                event=StreamEvents.TOOL_RESULT,
-                                data=tool_call_result.as_streaming_tool_result_response(),
-                            )
-                        else:
-                            tool_call_result.result.status = (
-                                StructuredToolResultStatus.ERROR
-                            )
-                            tool_call_result.result.error = f"Tool call rejected for security reasons: {tool_call_result.result.error}"
-
-                            tool_calls.append(
-                                tool_call_result.as_tool_result_response()
-                            )
-                            messages.append(tool_call_result.as_tool_call_message())
-
-                            yield StreamMessage(
-                                event=StreamEvents.TOOL_RESULT,
-                                data=tool_call_result.as_streaming_tool_result_response(),
-                            )
-
-                    else:
-                        tool_calls.append(tool_call_result.as_tool_result_response())
-                        messages.append(tool_call_result.as_tool_call_message())
-
-                        yield StreamMessage(
-                            event=StreamEvents.TOOL_RESULT,
-                            data=tool_call_result.as_streaming_tool_result_response(),
-                        )
+                    tool_calls.append(tool_call_result.as_tool_result_response())
+                    messages.append(tool_call_result.as_tool_call_message())
 
-                # If we have approval required tools, end the stream with pending approvals
-                if pending_approvals:
-                    # Add assistant message with pending tool calls
-                    for result in approval_required_tools:
-                        tool_call = self.find_assistant_tool_call_request(
-                            tool_call_id=result.tool_call_id, messages=messages
-                        )
-                        tool_call["pending_approval"] = True
+                    perf_timing.measure(f"tool completed {tool_call_result.tool_name}")
 
-                    # End stream with approvals required
                     yield StreamMessage(
-                        event=StreamEvents.APPROVAL_REQUIRED,
-                        data={
-                            "content": None,
-                            "messages": messages,
-                            "pending_approvals": [
-                                approval.model_dump() for approval in pending_approvals
-                            ],
-                            "requires_approval": True,
-                        },
+                        event=StreamEvents.TOOL_RESULT,
+                        data=tool_call_result.as_streaming_tool_result_response(),
                     )
-                    return
 
                 # Update the tool number offset for the next iteration
                 tool_number_offset += len(tools_to_call)
@@ -997,21 +825,6 @@
             f"Too many LLM calls - exceeded max_steps: {i}/{self.max_steps}"
         )
 
-    def find_assistant_tool_call_request(
-        self, tool_call_id: str, messages: list[dict[str, Any]]
-    ) -> dict[str, Any]:
-        for message in messages:
-            if message.get("role") == "assistant":
-                for tool_call in message.get("tool_calls", []):
-                    if tool_call.get("id") == tool_call_id:
-                        return tool_call
-
-        # Should not happen unless there is a bug.
-        # If we are here
-        raise Exception(
-            f"Failed to find assistant request for a tool_call in conversation history. tool_call_id={tool_call_id}"
-        )
-
 
 # TODO: consider getting rid of this entirely and moving templating into the cmds in holmes_cli.py
 class IssueInvestigator(ToolCallingLLM):
@@ -1044,9 +857,8 @@
         post_processing_prompt: Optional[str] = None,
         sections: Optional[InputSectionsDataType] = None,
         trace_span=DummySpan(),
-        runbooks: Optional[RunbookCatalog] = None,
     ) -> LLMResult:
-        issue_runbooks = self.runbook_manager.get_instructions_for_issue(issue)
+        runbooks = self.runbook_manager.get_instructions_for_issue(issue)
 
         request_structured_output_from_llm = True
         response_format = None
@@ -1074,9 +886,12 @@
         else:
             logging.info("Structured output is disabled for this request")
 
+        if instructions is not None and instructions.instructions:
+            runbooks.extend(instructions.instructions)
+
         if console and runbooks:
             console.print(
-                f"[bold]Analyzing with {len(issue_runbooks)} runbooks: {issue_runbooks}[/bold]"
+                f"[bold]Analyzing with {len(runbooks)} runbooks: {runbooks}[/bold]"
             )
         elif console:
             console.print(
@@ -1091,20 +906,29 @@
                 "structured_output": request_structured_output_from_llm,
                 "toolsets": self.tool_executor.toolsets,
                 "cluster_name": self.cluster_name,
-                "runbooks_enabled": True if runbooks else False,
             },
         )
 
+        if instructions is not None and len(instructions.documents) > 0:
+            docPrompts = []
+            for document in instructions.documents:
+                docPrompts.append(
+                    f"* fetch information from this URL: {document.url}\n"
+                )
+            runbooks.extend(docPrompts)
+
         user_prompt = ""
+        if runbooks:
+            for runbook_str in runbooks:
+                user_prompt += f"* {runbook_str}\n"
 
-        user_prompt = add_runbooks_to_user_prompt(
-            user_prompt,
-            runbook_catalog=runbooks,
-            global_instructions=global_instructions,
-            issue_instructions=issue_runbooks,
-            resource_instructions=instructions,
+            user_prompt = f'My instructions to check \n"""{user_prompt}"""'
+
+        user_prompt = add_global_instructions_to_user_prompt(
+            user_prompt, global_instructions
         )
-        user_prompt = f"{user_prompt}\n #This is context from the issue:\n{issue.raw}"
+        user_prompt = f"{user_prompt}\n This is context from the issue {issue.raw}"
+
         logging.debug(
             "Rendered system prompt:\n%s", textwrap.indent(system_prompt, "    ")
         )
@@ -1118,5 +942,5 @@
             sections=sections,
             trace_span=trace_span,
         )
-        res.instructions = issue_runbooks
+        res.instructions = runbooks
         return res
Only in holmes2/holmes/core/tools_utils: __pycache__
Only in baseline-holmes/holmes/core/tools_utils: token_counting.py
Only in baseline-holmes/holmes/core/tools_utils: tool_context_window_limiter.py
diff -ur baseline-holmes/holmes/core/tools_utils/tool_executor.py holmes2/holmes/core/tools_utils/tool_executor.py
--- baseline-holmes/holmes/core/tools_utils/tool_executor.py	2025-11-05 16:43:28.206140937 -0800
+++ holmes2/holmes/core/tools_utils/tool_executor.py	2025-10-17 15:09:28.638547193 -0700
@@ -6,10 +6,9 @@
 from holmes.core.tools import (
     StructuredToolResult,
     Tool,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     Toolset,
     ToolsetStatusEnum,
-    ToolInvokeContext,
 )
 from holmes.core.tools_utils.toolset_utils import filter_out_default_logging_toolset
 
@@ -47,20 +46,16 @@
                     )
                 self.tools_by_name[tool.name] = tool
 
-    def invoke(
-        self, tool_name: str, params: dict, context: ToolInvokeContext
-    ) -> StructuredToolResult:
-        """TODO: remove this function as it seems unused.
-        We call tool_executor.get_tool_by_name() and then tool.invoke() directly instead of this invoke function
-        """
+    def invoke(self, tool_name: str, params: dict) -> StructuredToolResult:
         tool = self.get_tool_by_name(tool_name)
-        if not tool:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+        return (
+            tool.invoke(params)
+            if tool
+            else StructuredToolResult(
+                status=ToolResultStatus.ERROR,
                 error=f"Could not find tool named {tool_name}",
             )
-
-        return tool.invoke(params, context)
+        )
 
     def get_tool_by_name(self, name: str) -> Optional[Tool]:
         if name in self.tools_by_name:
diff -ur baseline-holmes/holmes/core/tools.py holmes2/holmes/core/tools.py
--- baseline-holmes/holmes/core/tools.py	2025-11-05 16:43:28.205719689 -0800
+++ holmes2/holmes/core/tools.py	2025-10-17 15:09:28.619662923 -0700
@@ -8,77 +8,43 @@
 from abc import ABC, abstractmethod
 from datetime import datetime
 from enum import Enum
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    Dict,
-    List,
-    Optional,
-    OrderedDict,
-    Tuple,
-    Union,
-)
+from typing import Any, Callable, Dict, List, Optional, OrderedDict, Tuple, Union
 
 from jinja2 import Template
-from pydantic import (
-    BaseModel,
-    ConfigDict,
-    Field,
-    FilePath,
-    model_validator,
-    PrivateAttr,
-)
+from pydantic import BaseModel, ConfigDict, Field, FilePath, model_validator
 from rich.console import Console
 
-from holmes.core.llm import LLM
 from holmes.core.openai_formatting import format_tool_to_open_ai_standard
 from holmes.plugins.prompts import load_and_render_prompt
-from holmes.core.transformers import (
-    registry,
-    TransformerError,
-    Transformer,
-)
-
-if TYPE_CHECKING:
-    from holmes.core.transformers import BaseTransformer
-from holmes.utils.config_utils import merge_transformers
 import time
 from rich.table import Table
 
-logger = logging.getLogger(__name__)
 
-
-class StructuredToolResultStatus(str, Enum):
+class ToolResultStatus(str, Enum):
     SUCCESS = "success"
     ERROR = "error"
     NO_DATA = "no_data"
-    APPROVAL_REQUIRED = "approval_required"
 
     def to_color(self) -> str:
-        if self == StructuredToolResultStatus.SUCCESS:
+        if self == ToolResultStatus.SUCCESS:
             return "green"
-        elif self == StructuredToolResultStatus.ERROR:
+        elif self == ToolResultStatus.ERROR:
             return "red"
-        elif self == StructuredToolResultStatus.APPROVAL_REQUIRED:
-            return "yellow"
         else:
             return "white"
 
     def to_emoji(self) -> str:
-        if self == StructuredToolResultStatus.SUCCESS:
+        if self == ToolResultStatus.SUCCESS:
             return ""
-        elif self == StructuredToolResultStatus.ERROR:
+        elif self == ToolResultStatus.ERROR:
             return ""
-        elif self == StructuredToolResultStatus.APPROVAL_REQUIRED:
-            return ""
         else:
             return ""
 
 
 class StructuredToolResult(BaseModel):
     schema_version: str = "robusta:v1.0.0"
-    status: StructuredToolResultStatus
+    status: ToolResultStatus
     error: Optional[str] = None
     return_code: Optional[int] = None
     data: Optional[Any] = None
@@ -158,16 +124,6 @@
     required: bool = True
     properties: Optional[Dict[str, "ToolParameter"]] = None  # For object types
     items: Optional["ToolParameter"] = None  # For array item schemas
-    enum: Optional[List[str]] = None  # For restricting to specific values
-
-
-class ToolInvokeContext(BaseModel):
-    model_config = ConfigDict(arbitrary_types_allowed=True)
-
-    tool_number: Optional[int] = None
-    user_approved: bool = False
-    llm: LLM
-    max_token_count: int
 
 
 class Tool(ABC, BaseModel):
@@ -182,48 +138,6 @@
         default=None,
         description="The URL of the icon for the tool, if None will get toolset icon",
     )
-    transformers: Optional[List[Transformer]] = None
-
-    # Private attribute to store initialized transformer instances for performance
-    _transformer_instances: Optional[List["BaseTransformer"]] = PrivateAttr(
-        default=None
-    )
-
-    def model_post_init(self, __context) -> None:
-        """Initialize transformer instances once during tool creation for better performance."""
-        logger.debug(
-            f"Tool '{self.name}' model_post_init: creating transformer instances"
-        )
-
-        if self.transformers:
-            logger.debug(
-                f"Tool '{self.name}' has {len(self.transformers)} transformers to initialize"
-            )
-            self._transformer_instances = []
-            for transformer in self.transformers:
-                if not transformer:
-                    continue
-                logger.debug(
-                    f"  Initializing transformer '{transformer.name}' with config: {transformer.config}"
-                )
-                try:
-                    # Create transformer instance once and cache it
-                    transformer_instance = registry.create_transformer(
-                        transformer.name, transformer.config
-                    )
-                    self._transformer_instances.append(transformer_instance)
-                    logger.debug(
-                        f"Initialized transformer '{transformer.name}' for tool '{self.name}'"
-                    )
-                except Exception as e:
-                    logger.warning(
-                        f"Failed to initialize transformer '{transformer.name}' for tool '{self.name}': {e}"
-                    )
-                    # Continue with other transformers, don't fail the entire initialization
-                    continue
-        else:
-            logger.debug(f"Tool '{self.name}' has no transformers")
-            self._transformer_instances = None
 
     def get_openai_format(self, target_model: str):
         return format_tool_to_open_ai_standard(
@@ -234,130 +148,30 @@
         )
 
     def invoke(
-        self,
-        params: Dict,
-        context: ToolInvokeContext,
+        self, params: Dict, tool_number: Optional[int] = None
     ) -> StructuredToolResult:
-        tool_number_str = f"#{context.tool_number} " if context.tool_number else ""
-        logger.info(
+        tool_number_str = f"#{tool_number} " if tool_number else ""
+        logging.info(
             f"Running tool {tool_number_str}[bold]{self.name}[/bold]: {self.get_parameterized_one_liner(params)}"
         )
         start_time = time.time()
-        result = self._invoke(params=params, context=context)
+        result = self._invoke(params)
         result.icon_url = self.icon_url
-
-        # Apply transformers to the result
-        transformed_result = self._apply_transformers(result)
         elapsed = time.time() - start_time
         output_str = (
-            transformed_result.get_stringified_data()
-            if hasattr(transformed_result, "get_stringified_data")
-            else str(transformed_result)
+            result.get_stringified_data()
+            if hasattr(result, "get_stringified_data")
+            else str(result)
         )
-        show_hint = f"/show {context.tool_number}" if context.tool_number else "/show"
+        show_hint = f"/show {tool_number}" if tool_number else "/show"
         line_count = output_str.count("\n") + 1 if output_str else 0
-        logger.info(
+        logging.info(
             f"  [dim]Finished {tool_number_str}in {elapsed:.2f}s, output length: {len(output_str):,} characters ({line_count:,} lines) - {show_hint} to view contents[/dim]"
         )
-        return transformed_result
-
-    def _apply_transformers(self, result: StructuredToolResult) -> StructuredToolResult:
-        """
-        Apply configured transformers to the tool result.
-
-        Args:
-            result: The original tool result
-
-        Returns:
-            The tool result with transformed data, or original result if transformation fails
-        """
-        if (
-            not self._transformer_instances
-            or result.status != StructuredToolResultStatus.SUCCESS
-        ):
-            return result
-
-        # Get the output string to transform
-        original_data = result.get_stringified_data()
-        if not original_data:
-            return result
-
-        transformed_data = original_data
-        transformers_applied = []
-
-        # Use cached transformer instances instead of creating new ones
-        for transformer_instance in self._transformer_instances:
-            try:
-                # Check if transformer should be applied
-                if not transformer_instance.should_apply(transformed_data):
-                    logger.debug(
-                        f"Transformer '{transformer_instance.name}' skipped for tool '{self.name}' (conditions not met)"
-                    )
-                    continue
-
-                # Apply transformation
-                pre_transform_size = len(transformed_data)
-                transform_start_time = time.time()
-                original_data = transformed_data  # Keep a copy for potential reversion
-                transformed_data = transformer_instance.transform(transformed_data)
-                transform_elapsed = time.time() - transform_start_time
-
-                # Check if this is llm_summarize and revert if summary is not smaller
-                post_transform_size = len(transformed_data)
-                if (
-                    transformer_instance.name == "llm_summarize"
-                    and post_transform_size >= pre_transform_size
-                ):
-                    # Revert to original data if summary is not smaller
-                    transformed_data = original_data
-                    logger.debug(
-                        f"Transformer '{transformer_instance.name}' reverted for tool '{self.name}' "
-                        f"(output size {post_transform_size:,} >= input size {pre_transform_size:,})"
-                    )
-                    continue  # Don't mark as applied
-
-                transformers_applied.append(transformer_instance.name)
-
-                # Generic logging - transformers can override this with their own specific metrics
-                size_change = post_transform_size - pre_transform_size
-                logger.info(
-                    f"Applied transformer '{transformer_instance.name}' to tool '{self.name}' output "
-                    f"in {transform_elapsed:.2f}s (size: {pre_transform_size:,}  {post_transform_size:,} chars, "
-                    f"change: {size_change:+,})"
-                )
-
-            except TransformerError as e:
-                logger.warning(
-                    f"Transformer '{transformer_instance.name}' failed for tool '{self.name}': {e}"
-                )
-                # Continue with other transformers, don't fail the entire chain
-                continue
-            except Exception as e:
-                logger.error(
-                    f"Unexpected error applying transformer '{transformer_instance.name}' to tool '{self.name}': {e}"
-                )
-                # Continue with other transformers
-                continue
-
-        # If any transformers were applied, update the result
-        if transformers_applied:
-            # Create a copy of the result with transformed data
-            result_dict = result.model_dump(exclude={"data"})
-            result_dict["data"] = transformed_data
-            return StructuredToolResult(**result_dict)
-
         return result
 
     @abstractmethod
-    def _invoke(
-        self,
-        params: dict,
-        context: ToolInvokeContext,
-    ) -> StructuredToolResult:
-        """
-        params: the tool params
-        user_approved: whether the tool call is approved by the user. Can be used to confidently execute unsafe actions.
-        """
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         pass
 
     @abstractmethod
@@ -402,27 +216,21 @@
         context = {**params}
         return context
 
-    def _get_status(
-        self, return_code: int, raw_output: str
-    ) -> StructuredToolResultStatus:
+    def _get_status(self, return_code: int, raw_output: str) -> ToolResultStatus:
         if return_code != 0:
-            return StructuredToolResultStatus.ERROR
+            return ToolResultStatus.ERROR
         if raw_output == "":
-            return StructuredToolResultStatus.NO_DATA
-        return StructuredToolResultStatus.SUCCESS
+            return ToolResultStatus.NO_DATA
+        return ToolResultStatus.SUCCESS
 
-    def _invoke(
-        self,
-        params: dict,
-        context: ToolInvokeContext,
-    ) -> StructuredToolResult:
+    def _invoke(self, params) -> StructuredToolResult:
         if self.command is not None:
             raw_output, return_code, invocation = self.__invoke_command(params)
         else:
             raw_output, return_code, invocation = self.__invoke_script(params)  # type: ignore
 
         if self.additional_instructions and return_code == 0:
-            logger.info(
+            logging.info(
                 f"Applying additional instructions: {self.additional_instructions}"
             )
             output_with_instructions = self.__apply_additional_instructions(raw_output)
@@ -457,7 +265,7 @@
             )
             return result.stdout.strip()
         except subprocess.CalledProcessError as e:
-            logger.error(
+            logging.error(
                 f"Failed to apply additional instructions: {self.additional_instructions}. "
                 f"Error: {e.stderr}"
             )
@@ -492,7 +300,7 @@
 
     def __execute_subprocess(self, cmd) -> Tuple[str, int]:
         try:
-            logger.debug(f"Running `{cmd}`")
+            logging.debug(f"Running `{cmd}`")
             result = subprocess.run(
                 cmd,
                 shell=True,
@@ -505,7 +313,7 @@
 
             return result.stdout.strip(), result.returncode
         except Exception as e:
-            logger.error(
+            logging.error(
                 f"An unexpected error occurred while running '{cmd}': {e}",
                 exc_info=True,
             )
@@ -557,7 +365,6 @@
     config: Optional[Any] = None
     is_default: bool = False
     llm_instructions: Optional[str] = None
-    transformers: Optional[List[Transformer]] = None
 
     # warning! private attributes are not copied, which can lead to subtle bugs.
     # e.g. l.extend([some_tool]) will reset these private attribute to None
@@ -583,85 +390,13 @@
     @model_validator(mode="before")
     def preprocess_tools(cls, values):
         additional_instructions = values.get("additional_instructions", "")
-        transformers = values.get("transformers", None)
         tools_data = values.get("tools", [])
-
-        # Convert raw dict transformers to Transformer objects BEFORE merging
-        if transformers:
-            converted_transformers = []
-            for t in transformers:
-                if isinstance(t, dict):
-                    try:
-                        transformer_obj = Transformer(**t)
-                        # Check if transformer is registered
-                        from holmes.core.transformers import registry
-
-                        if not registry.is_registered(transformer_obj.name):
-                            logger.warning(
-                                f"Invalid toolset transformer configuration: Transformer '{transformer_obj.name}' is not registered"
-                            )
-                            continue  # Skip invalid transformer
-                        converted_transformers.append(transformer_obj)
-                    except Exception as e:
-                        # Log warning and skip invalid transformer
-                        logger.warning(
-                            f"Invalid toolset transformer configuration: {e}"
-                        )
-                        continue
-                else:
-                    # Already a Transformer object
-                    converted_transformers.append(t)
-            transformers = converted_transformers if converted_transformers else None
-
         tools = []
         for tool in tools_data:
             if isinstance(tool, dict):
                 tool["additional_instructions"] = additional_instructions
-
-                # Convert tool-level transformers to Transformer objects
-                tool_transformers = tool.get("transformers")
-                if tool_transformers:
-                    converted_tool_transformers = []
-                    for t in tool_transformers:
-                        if isinstance(t, dict):
-                            try:
-                                transformer_obj = Transformer(**t)
-                                # Check if transformer is registered
-                                from holmes.core.transformers import registry
-
-                                if not registry.is_registered(transformer_obj.name):
-                                    logger.warning(
-                                        f"Invalid tool transformer configuration: Transformer '{transformer_obj.name}' is not registered"
-                                    )
-                                    continue  # Skip invalid transformer
-                                converted_tool_transformers.append(transformer_obj)
-                            except Exception as e:
-                                # Log warning and skip invalid transformer
-                                logger.warning(
-                                    f"Invalid tool transformer configuration: {e}"
-                                )
-                                continue
-                        else:
-                            # Already a Transformer object
-                            converted_tool_transformers.append(t)
-                    tool_transformers = (
-                        converted_tool_transformers
-                        if converted_tool_transformers
-                        else None
-                    )
-
-                # Merge toolset-level transformers with tool-level configs
-                tool["transformers"] = merge_transformers(
-                    base_transformers=transformers,
-                    override_transformers=tool_transformers,
-                )
             if isinstance(tool, Tool):
                 tool.additional_instructions = additional_instructions
-                # Merge toolset-level transformers with tool-level configs
-                tool.transformers = merge_transformers(  # type: ignore
-                    base_transformers=transformers,
-                    override_transformers=tool.transformers,
-                )
             tools.append(tool)
         values["tools"] = tools
 
@@ -683,26 +418,7 @@
     def check_prerequisites(self):
         self.status = ToolsetStatusEnum.ENABLED
 
-        # Sort prerequisites by type to fail fast on missing env vars before
-        # running slow commands (e.g., ArgoCD checks that timeout):
-        # 1. Static checks (instant)
-        # 2. Environment variable checks (instant, often required by commands)
-        # 3. Callable checks (variable speed)
-        # 4. Command checks (slowest - may timeout or hang)
-        def prereq_priority(prereq):
-            if isinstance(prereq, StaticPrerequisite):
-                return 0
-            elif isinstance(prereq, ToolsetEnvironmentPrerequisite):
-                return 1
-            elif isinstance(prereq, CallablePrerequisite):
-                return 2
-            elif isinstance(prereq, ToolsetCommandPrerequisite):
-                return 3
-            return 4  # Unknown types go last
-
-        sorted_prereqs = sorted(self.prerequisites, key=prereq_priority)
-
-        for prereq in sorted_prereqs:
+        for prereq in self.prerequisites:
             if isinstance(prereq, ToolsetCommandPrerequisite):
                 try:
                     command = self.interpolate_command(prereq.command)
@@ -750,11 +466,11 @@
                 self.status == ToolsetStatusEnum.DISABLED
                 or self.status == ToolsetStatusEnum.FAILED
             ):
-                logger.info(f" Toolset {self.name}: {self.error}")
+                logging.info(f" Toolset {self.name}: {self.error}")
                 # no point checking further prerequisites if one failed
                 return
 
-        logger.info(f" Toolset {self.name}")
+        logging.info(f" Toolset {self.name}")
 
     @abstractmethod
     def get_example_config(self) -> Dict[str, Any]:
@@ -767,16 +483,6 @@
             context={"tool_names": tool_names, "config": self.config},
         )
 
-    def _load_llm_instructions_from_file(self, file_dir: str, filename: str) -> None:
-        """Helper method to load LLM instructions from a jinja2 template file.
-
-        Args:
-            file_dir: Directory where the template file is located (typically os.path.dirname(__file__))
-            filename: Name of the jinja2 template file (e.g., "toolset_grafana_dashboard.jinja2")
-        """
-        template_file_path = os.path.abspath(os.path.join(file_dir, filename))
-        self._load_llm_instructions(jinja_template=f"file://{template_file_path}")
-
 
 class YAMLToolset(Toolset):
     tools: List[YAMLTool]  # type: ignore
diff -ur baseline-holmes/holmes/core/toolset_manager.py holmes2/holmes/core/toolset_manager.py
--- baseline-holmes/holmes/core/toolset_manager.py	2025-11-05 16:43:28.206348312 -0800
+++ holmes2/holmes/core/toolset_manager.py	2025-10-17 15:09:28.641501586 -0700
@@ -2,7 +2,7 @@
 import json
 import logging
 import os
-from typing import Any, List, Optional, TYPE_CHECKING
+from typing import Any, List, Optional
 
 from benedict import benedict
 from pydantic import FilePath
@@ -13,9 +13,6 @@
 from holmes.plugins.toolsets import load_builtin_toolsets, load_toolsets_from_config
 from holmes.utils.definitions import CUSTOM_TOOLSET_LOCATION
 
-if TYPE_CHECKING:
-    pass
-
 DEFAULT_TOOLSET_STATUS_LOCATION = os.path.join(config_path_dir, "toolsets_status.json")
 
 
@@ -33,7 +30,6 @@
         custom_toolsets: Optional[List[FilePath]] = None,
         custom_toolsets_from_cli: Optional[List[FilePath]] = None,
         toolset_status_location: Optional[FilePath] = None,
-        global_fast_model: Optional[str] = None,
     ):
         self.toolsets = toolsets
         self.toolsets = toolsets or {}
@@ -42,7 +38,6 @@
                 mcp_server["type"] = ToolsetType.MCP.value
         self.toolsets.update(mcp_servers or {})
         self.custom_toolsets = custom_toolsets
-        self.global_fast_model = global_fast_model
 
         if toolset_status_location is None:
             toolset_status_location = FilePath(DEFAULT_TOOLSET_STATUS_LOCATION)
@@ -123,13 +118,9 @@
                 if any(tag in toolset_tags for tag in toolset.tags)
             }
 
-        # Inject global fast_model into all toolsets
-        final_toolsets = list(toolsets_by_name.values())
-        self._inject_fast_model_into_transformers(final_toolsets)
-
         # check_prerequisites against each enabled toolset
         if not check_prerequisites:
-            return final_toolsets
+            return list(toolsets_by_name.values())
 
         enabled_toolsets: List[Toolset] = []
         for _, toolset in toolsets_by_name.items():
@@ -139,7 +130,7 @@
                 toolset.status = ToolsetStatusEnum.DISABLED
         self.check_toolset_prerequisites(enabled_toolsets)
 
-        return final_toolsets
+        return list(toolsets_by_name.values())
 
     @classmethod
     def check_toolset_prerequisites(cls, toolsets: list[Toolset]):
@@ -275,11 +266,7 @@
                 toolset.path = cached_status.get("path", None)
             # check prerequisites for only enabled toolset when the toolset is loaded from cache. When the toolset is
             # not loaded from cache, the prerequisites are checked in the refresh_toolset_status method.
-            if toolset.enabled and (
-                toolset.status == ToolsetStatusEnum.ENABLED
-                or toolset.type == ToolsetType.MCP
-            ):
-                # MCP servers need to reload their tools even if previously failed, so rerun prerequisites
+            if toolset.enabled and toolset.status == ToolsetStatusEnum.ENABLED:
                 enabled_toolsets_from_cache.append(toolset)
         self.check_toolset_prerequisites(enabled_toolsets_from_cache)
 
@@ -289,10 +276,6 @@
             list(toolsets_status_by_name.keys()),
             check_conflict_default=True,
         )
-
-        # Inject fast_model into CLI custom toolsets
-        self._inject_fast_model_into_transformers(custom_toolsets_from_cli)
-
         # custom toolsets from cli as experimental toolset should not override custom toolsets from config
         enabled_toolsets_from_cli: List[Toolset] = []
         for custom_toolset_from_cli in custom_toolsets_from_cli:
@@ -455,137 +438,3 @@
             else:
                 existing_toolsets_by_name[new_toolset.name] = new_toolset
                 existing_toolsets_by_name[new_toolset.name] = new_toolset
-
-    def _inject_fast_model_into_transformers(self, toolsets: List[Toolset]) -> None:
-        """
-        Inject global fast_model setting into all llm_summarize transformers that don't already have fast_model.
-        This ensures --fast-model reaches all tools regardless of toolset-level transformer configuration.
-
-        IMPORTANT: This also forces recreation of transformer instances since they may already be created.
-        """
-        import logging
-        from holmes.core.transformers import registry
-
-        logger = logging.getLogger(__name__)
-
-        logger.debug(
-            f"Starting fast_model injection. global_fast_model={self.global_fast_model}"
-        )
-
-        if not self.global_fast_model:
-            logger.debug("No global_fast_model configured, skipping injection")
-            return
-
-        injected_count = 0
-        toolset_count = 0
-
-        for toolset in toolsets:
-            toolset_count += 1
-            toolset_injected = 0
-            logger.debug(
-                f"Processing toolset '{toolset.name}', has toolset transformers: {toolset.transformers is not None}"
-            )
-
-            # Inject into toolset-level transformers
-            if toolset.transformers:
-                logger.debug(
-                    f"Toolset '{toolset.name}' has {len(toolset.transformers)} toolset-level transformers"
-                )
-                for transformer in toolset.transformers:
-                    logger.debug(
-                        f"  Toolset transformer: name='{transformer.name}', config keys={list(transformer.config.keys())}"
-                    )
-                    if (
-                        transformer.name == "llm_summarize"
-                        and "fast_model" not in transformer.config
-                    ):
-                        transformer.config["global_fast_model"] = self.global_fast_model
-                        injected_count += 1
-                        toolset_injected += 1
-                        logger.info(
-                            f"   Injected global_fast_model into toolset '{toolset.name}' transformer"
-                        )
-                    elif transformer.name == "llm_summarize":
-                        logger.debug(
-                            f"  - Toolset transformer already has fast_model: {transformer.config.get('fast_model')}"
-                        )
-            else:
-                logger.debug(
-                    f"Toolset '{toolset.name}' has no toolset-level transformers"
-                )
-
-            # Inject into tool-level transformers
-            if hasattr(toolset, "tools") and toolset.tools:
-                logger.debug(f"Toolset '{toolset.name}' has {len(toolset.tools)} tools")
-                for tool in toolset.tools:
-                    logger.debug(
-                        f"  Processing tool '{tool.name}', has transformers: {tool.transformers is not None}"
-                    )
-                    if tool.transformers:
-                        logger.debug(
-                            f"    Tool '{tool.name}' has {len(tool.transformers)} transformers"
-                        )
-                        tool_updated = False
-                        for transformer in tool.transformers:
-                            logger.debug(
-                                f"      Tool transformer: name='{transformer.name}', config keys={list(transformer.config.keys())}"
-                            )
-                            if (
-                                transformer.name == "llm_summarize"
-                                and "fast_model" not in transformer.config
-                            ):
-                                transformer.config["global_fast_model"] = (
-                                    self.global_fast_model
-                                )
-                                injected_count += 1
-                                toolset_injected += 1
-                                tool_updated = True
-                                logger.info(
-                                    f"       Injected global_fast_model into tool '{tool.name}' transformer"
-                                )
-                            elif transformer.name == "llm_summarize":
-                                logger.debug(
-                                    f"      - Tool transformer already has fast_model: {transformer.config.get('fast_model')}"
-                                )
-
-                        # CRITICAL: Force recreation of transformer instances if we updated the config
-                        if tool_updated:
-                            logger.info(
-                                f"       Recreating transformer instances for tool '{tool.name}' after injection"
-                            )
-                            if tool.transformers:
-                                tool._transformer_instances = []
-                                for transformer in tool.transformers:
-                                    if not transformer:
-                                        continue
-                                    try:
-                                        # Create transformer instance with updated config
-                                        transformer_instance = (
-                                            registry.create_transformer(
-                                                transformer.name, transformer.config
-                                            )
-                                        )
-                                        tool._transformer_instances.append(
-                                            transformer_instance
-                                        )
-                                        logger.debug(
-                                            f"        Recreated transformer '{transformer.name}' for tool '{tool.name}' with config: {transformer.config}"
-                                        )
-                                    except Exception as e:
-                                        logger.warning(
-                                            f"        Failed to recreate transformer '{transformer.name}' for tool '{tool.name}': {e}"
-                                        )
-                                        continue
-                    else:
-                        logger.debug(f"    Tool '{tool.name}' has no transformers")
-            else:
-                logger.debug(f"Toolset '{toolset.name}' has no tools")
-
-            if toolset_injected > 0:
-                logger.info(
-                    f"Toolset '{toolset.name}': injected into {toolset_injected} transformers"
-                )
-
-        logger.info(
-            f"Fast_model injection complete: {injected_count} transformers updated across {toolset_count} toolsets"
-        )
diff -ur baseline-holmes/holmes/core/tracing.py holmes2/holmes/core/tracing.py
--- baseline-holmes/holmes/core/tracing.py	2025-11-05 16:43:28.206530686 -0800
+++ holmes2/holmes/core/tracing.py	2025-10-17 15:09:28.617375149 -0700
@@ -101,7 +101,7 @@
 class DummySpan:
     """A no-op span implementation for when tracing is disabled."""
 
-    def start_span(self, name: Optional[str] = None, span_type=None, **kwargs):
+    def start_span(self, name: str, span_type=None, **kwargs):
         return DummySpan()
 
     def log(self, *args, **kwargs):
@@ -110,11 +110,6 @@
     def end(self):
         pass
 
-    def set_attributes(
-        self, name: Optional[str] = None, type=None, span_attributes=None
-    ) -> None:
-        pass
-
     def __enter__(self):
         return self
 
Only in baseline-holmes/holmes/core: transformers
Only in baseline-holmes/holmes/core: truncation
diff -ur baseline-holmes/holmes/interactive.py holmes2/holmes/interactive.py
--- baseline-holmes/holmes/interactive.py	2025-11-05 16:43:28.208218973 -0800
+++ holmes2/holmes/interactive.py	2025-10-17 15:09:28.859837387 -0700
@@ -26,19 +26,11 @@
 from pygments.lexers import guess_lexer
 from rich.console import Console
 from rich.markdown import Markdown, Panel
-from rich.markup import escape
 
-from holmes.common.env_vars import ENABLE_CLI_TOOL_APPROVAL
 from holmes.core.config import config_path_dir
-from holmes.core.feedback import (
-    PRIVACY_NOTICE_BANNER,
-    Feedback,
-    FeedbackCallback,
-    UserFeedback,
-)
 from holmes.core.prompt import build_initial_ask_messages
 from holmes.core.tool_calling_llm import ToolCallingLLM, ToolCallResult
-from holmes.core.tools import StructuredToolResult, pretty_print_toolset_status
+from holmes.core.tools import pretty_print_toolset_status
 from holmes.core.tracing import DummyTracer
 from holmes.utils.colors import (
     AI_COLOR,
@@ -50,7 +42,6 @@
 )
 from holmes.utils.console.consts import agent_name
 from holmes.version import check_version_async
-import re
 
 
 class SlashCommands(Enum):
@@ -70,25 +61,19 @@
     )
     CONTEXT = ("/context", "Show conversation context size and token count")
     SHOW = ("/show", "Show specific tool output in scrollable view")
-    FEEDBACK = ("/feedback", "Provide feedback on the agent's response")
 
     def __init__(self, command, description):
         self.command = command
         self.description = description
 
 
+SLASH_COMMANDS_REFERENCE = {cmd.command: cmd.description for cmd in SlashCommands}
+ALL_SLASH_COMMANDS = [cmd.command for cmd in SlashCommands]
+
+
 class SlashCommandCompleter(Completer):
-    def __init__(self, unsupported_commands: Optional[List[str]] = None):
-        # Build commands dictionary, excluding unsupported commands
-        all_commands = {cmd.command: cmd.description for cmd in SlashCommands}
-        if unsupported_commands:
-            self.commands = {
-                cmd: desc
-                for cmd, desc in all_commands.items()
-                if cmd not in unsupported_commands
-            }
-        else:
-            self.commands = all_commands
+    def __init__(self):
+        self.commands = SLASH_COMMANDS_REFERENCE
 
     def get_completions(self, document, complete_event):
         text = document.text_before_cursor
@@ -247,13 +232,6 @@
     return f"{tool_call.description} (exit: q, nav: /j/k/g/G/d/u/f/b/space, wrap: w [{wrap_status}])"
 
 
-def strip_ansi_codes(text: str) -> str:
-    ansi_escape_pattern = re.compile(
-        r"\x1b\[[0-9;]*[a-zA-Z]|\033\[[0-9;]*[a-zA-Z]|\^\[\[[0-9;]*[a-zA-Z]"
-    )
-    return ansi_escape_pattern.sub("", text)
-
-
 def detect_lexer(content: str) -> Optional[PygmentsLexer]:
     """
     Detect appropriate lexer for content using Pygments' built-in detection.
@@ -335,7 +313,6 @@
     try:
         # Get the full output
         output = tool_call.result.get_stringified_data()
-        output = strip_ansi_codes(output)
         title = build_modal_title(tool_call, "off")  # Word wrap starts disabled
 
         # Detect appropriate syntax highlighting
@@ -489,14 +466,10 @@
         return
 
     # Calculate context statistics
-    tokens_metadata = ai.llm.count_tokens(
-        messages
-    )  # TODO: pass tools to also count tokens used by input tools
+    total_tokens = ai.llm.count_tokens_for_message(messages)
     max_context_size = ai.llm.get_context_window_size()
     max_output_tokens = ai.llm.get_maximum_output_token()
-    available_tokens = (
-        max_context_size - tokens_metadata.total_tokens - max_output_tokens
-    )
+    available_tokens = max_context_size - total_tokens - max_output_tokens
 
     # Analyze token distribution by role and tool calls
     role_token_usage: DefaultDict[str, int] = defaultdict(int)
@@ -505,21 +478,19 @@
 
     for msg in messages:
         role = msg.get("role", "unknown")
-        message_tokens = ai.llm.count_tokens(
-            [msg]
-        )  # TODO: pass tools to also count tokens used by input tools
-        role_token_usage[role] += message_tokens.total_tokens
+        msg_tokens = ai.llm.count_tokens_for_message([msg])
+        role_token_usage[role] += msg_tokens
 
         # Track individual tool usage
         if role == "tool":
             tool_name = msg.get("name", "unknown_tool")
-            tool_token_usage[tool_name] += message_tokens.total_tokens
+            tool_token_usage[tool_name] += msg_tokens
             tool_call_counts[tool_name] += 1
 
     # Display context information
     console.print(f"[bold {STATUS_COLOR}]Conversation Context:[/bold {STATUS_COLOR}]")
     console.print(
-        f"  Context used: {tokens_metadata.total_tokens:,} / {max_context_size:,} tokens ({(tokens_metadata.total_tokens / max_context_size) * 100:.1f}%)"
+        f"  Context used: {total_tokens:,} / {max_context_size:,} tokens ({(total_tokens / max_context_size) * 100:.1f}%)"
     )
     console.print(
         f"  Space remaining: {available_tokens:,} for input ({(available_tokens / max_context_size) * 100:.1f}%) + {max_output_tokens:,} reserved for output ({(max_output_tokens / max_context_size) * 100:.1f}%)"
@@ -530,11 +501,7 @@
     for role in ["system", "user", "assistant", "tool"]:
         if role in role_token_usage:
             tokens = role_token_usage[role]
-            percentage = (
-                (tokens / tokens_metadata.total_tokens) * 100
-                if tokens_metadata.total_tokens > 0
-                else 0
-            )
+            percentage = (tokens / total_tokens) * 100 if total_tokens > 0 else 0
             role_name = {
                 "system": "system prompt",
                 "user": "user messages",
@@ -617,53 +584,6 @@
     return None
 
 
-def handle_tool_approval(
-    command: Optional[str],
-    error_message: Optional[str],
-    style: Style,
-    console: Console,
-) -> tuple[bool, Optional[str]]:
-    """
-    Handle user approval for potentially sensitive commands.
-
-    Args:
-        command: The command that needs approval
-        error_message: The error message explaining why approval is needed
-        session: PromptSession for user input
-        style: Style for prompts
-        console: Rich console for output
-
-    Returns:
-        Tuple of (approved: bool, feedback: Optional[str])
-        - approved: True if user approves, False if denied
-        - feedback: User's optional feedback message when denying
-    """
-    console.print("\n[bold yellow]  Command Approval Required[/bold yellow]")
-    console.print(f"[yellow]Command:[/yellow] {command or 'unknown'}")
-    console.print(f"[yellow]Reason:[/yellow] {error_message or 'unknown'}")
-    console.print()
-
-    # Create a temporary session without history for approval prompts
-    temp_session = PromptSession(history=InMemoryHistory())  # type: ignore
-
-    approval_prompt = temp_session.prompt(
-        [("class:prompt", "Do you want to approve and execute this command? (y/N): ")],
-        style=style,
-    )
-
-    if approval_prompt.lower().startswith("y"):
-        return True, None
-    else:
-        # Ask for optional feedback when denying
-        feedback_prompt = temp_session.prompt(
-            [("class:prompt", "Optional feedback for the AI (press Enter to skip): ")],
-            style=style,
-        )
-
-        feedback = feedback_prompt.strip() if feedback_prompt.strip() else None
-        return False, feedback
-
-
 def handle_run_command(
     bash_command: str, session: PromptSession, style: Style, console: Console
 ) -> Optional[str]:
@@ -843,88 +763,6 @@
         )
 
 
-def handle_feedback_command(
-    style: Style,
-    console: Console,
-    feedback: Feedback,
-    feedback_callback: FeedbackCallback,
-) -> None:
-    """Handle the /feedback command to collect user feedback."""
-    try:
-        # Create a temporary session without history for feedback prompts
-        temp_session = PromptSession(history=InMemoryHistory())  # type: ignore
-        # Prominent privacy notice to users
-        console.print(
-            f"[bold {HELP_COLOR}]Privacy Notice:[/bold {HELP_COLOR}] {PRIVACY_NOTICE_BANNER}"
-        )
-        # A "Cancel" button of equal discoverability to "Sent" or "Submit" buttons must be made available
-        console.print(
-            "[bold yellow] Tip: Press Ctrl+C at any time to cancel feedback[/bold yellow]"
-        )
-
-        # Ask for thumbs up/down rating with validation
-        while True:
-            rating_prompt = temp_session.prompt(
-                [("class:prompt", "Was this response useful to you? (y)/(n): ")],
-                style=style,
-            )
-
-            rating_lower = rating_prompt.lower().strip()
-            if rating_lower in ["y", "n"]:
-                break
-            else:
-                console.print(
-                    "[bold red]Please enter only 'y' for yes or 'n' for no.[/bold red]"
-                )
-
-        # Determine rating
-        is_positive = rating_lower == "y"
-
-        # Ask for additional comments
-        comment_prompt = temp_session.prompt(
-            [
-                (
-                    "class:prompt",
-                    "Do you want to provide any additional comments for feedback? (press Enter to skip):\n",
-                )
-            ],
-            style=style,
-        )
-
-        comment = comment_prompt.strip() if comment_prompt.strip() else None
-
-        # Create UserFeedback object
-        user_feedback = UserFeedback(is_positive, comment)
-
-        if comment:
-            console.print(
-                f'[bold green] Feedback recorded (rating={user_feedback.rating_emoji}, "{escape(comment)}")[/bold green]'
-            )
-        else:
-            console.print(
-                f"[bold green] Feedback recorded (rating={user_feedback.rating_emoji}, no comment)[/bold green]"
-            )
-
-        # Final confirmation before submitting
-        final_confirmation = temp_session.prompt(
-            [("class:prompt", "\nDo you want to submit this feedback? (Y/n): ")],
-            style=style,
-        )
-
-        # If user says no, cancel the feedback
-        if final_confirmation.lower().strip().startswith("n"):
-            console.print("[dim]Feedback cancelled.[/dim]")
-            return
-
-        feedback.user_feedback = user_feedback
-        feedback_callback(feedback)
-        console.print("[bold green]Thank you for your feedback! [/bold green]")
-
-    except KeyboardInterrupt:
-        console.print("[dim]Feedback cancelled.[/dim]")
-        return
-
-
 def display_recent_tool_outputs(
     tool_calls: List[ToolCallResult],
     console: Console,
@@ -937,10 +775,7 @@
     for tool_call in tool_calls:
         tool_index = find_tool_index_in_history(tool_call, all_tool_calls_history)
         preview_output = format_tool_call_output(tool_call, tool_index)
-        title = (
-            f"{tool_call.result.status.to_emoji()} {tool_call.description} -> "
-            f"returned {tool_call.result.return_code}"
-        )
+        title = f"{tool_call.result.status.to_emoji()} {tool_call.description} -> returned {tool_call.result.return_code}"
 
         console.print(
             Panel(
@@ -963,7 +798,6 @@
     runbooks=None,
     system_prompt_additions: Optional[str] = None,
     check_version: bool = True,
-    feedback_callback: Optional[FeedbackCallback] = None,
 ) -> None:
     # Initialize tracer - use DummyTracer if no tracer provided
     if tracer is None:
@@ -977,26 +811,8 @@
         }
     )
 
-    # Set up approval callback for potentially sensitive commands
-    def approval_handler(
-        tool_call_result: StructuredToolResult,
-    ) -> tuple[bool, Optional[str]]:
-        return handle_tool_approval(
-            command=tool_call_result.invocation,
-            error_message=tool_call_result.error,
-            style=style,
-            console=console,
-        )
-
-    if ENABLE_CLI_TOOL_APPROVAL:
-        ai.approval_callback = approval_handler
-
     # Create merged completer with slash commands, conditional executables, show command, and smart paths
-    # TODO: remove unsupported_commands support once we implement feedback callback
-    unsupported_commands = []
-    if feedback_callback is None:
-        unsupported_commands.append(SlashCommands.FEEDBACK.command)
-    slash_completer = SlashCommandCompleter(unsupported_commands)
+    slash_completer = SlashCommandCompleter()
     executable_completer = ConditionalExecutableCompleter()
     show_completer = ShowCommandCompleter()
     path_completer = SmartPathCompleter()
@@ -1013,9 +829,6 @@
     if initial_user_input:
         history.append_string(initial_user_input)
 
-    feedback = Feedback()
-    feedback.metadata.update_llm(ai.llm)
-
     # Create custom key bindings for Ctrl+C behavior
     bindings = KeyBindings()
     status_message = ""
@@ -1088,15 +901,7 @@
 
     input_prompt = [("class:prompt", "User: ")]
 
-    # TODO: merge the /feedback command description to WELCOME_BANNER once we implement feedback callback
-    welcome_banner = WELCOME_BANNER
-    if feedback_callback:
-        welcome_banner = (
-            welcome_banner.rstrip(".")
-            + f", '{SlashCommands.FEEDBACK.command}' to share your thoughts."
-        )
-    console.print(welcome_banner)
-
+    console.print(WELCOME_BANNER)
     if initial_user_input:
         console.print(
             f"[bold {USER_COLOR}]User:[/bold {USER_COLOR}] {initial_user_input}"
@@ -1118,18 +923,14 @@
             if user_input.startswith("/"):
                 original_input = user_input.strip()
                 command = original_input.lower()
+
                 # Handle prefix matching for slash commands
-                matches = [
-                    cmd
-                    for cmd in slash_completer.commands.keys()
-                    if cmd.startswith(command)
-                ]
+                matches = [cmd for cmd in ALL_SLASH_COMMANDS if cmd.startswith(command)]
                 if len(matches) == 1:
                     command = matches[0]
                 elif len(matches) > 1:
                     console.print(
-                        f"[bold {ERROR_COLOR}]Ambiguous command '{command}'. "
-                        f"Matches: {', '.join(matches)}[/bold {ERROR_COLOR}]"
+                        f"[bold {ERROR_COLOR}]Ambiguous command '{command}'. Matches: {', '.join(matches)}[/bold {ERROR_COLOR}]"
                     )
                     continue
 
@@ -1139,20 +940,13 @@
                     console.print(
                         f"[bold {HELP_COLOR}]Available commands:[/bold {HELP_COLOR}]"
                     )
-                    for cmd, description in slash_completer.commands.items():
-                        # Only show feedback command if callback is available
-                        if (
-                            cmd == SlashCommands.FEEDBACK.command
-                            and feedback_callback is None
-                        ):
-                            continue
+                    for cmd, description in SLASH_COMMANDS_REFERENCE.items():
                         console.print(f"  [bold]{cmd}[/bold] - {description}")
                     continue
                 elif command == SlashCommands.CLEAR.command:
                     console.clear()
                     console.print(
-                        f"[bold {STATUS_COLOR}]Screen cleared and context reset. "
-                        f"You can now ask a new question.[/bold {STATUS_COLOR}]"
+                        f"[bold {STATUS_COLOR}]Screen cleared and context reset. You can now ask a new question.[/bold {STATUS_COLOR}]"
                     )
                     messages = None
                     last_response = None
@@ -1196,12 +990,6 @@
                     if shared_input is None:
                         continue  # User chose not to share or no output, continue to next input
                     user_input = shared_input
-                elif (
-                    command == SlashCommands.FEEDBACK.command
-                    and feedback_callback is not None
-                ):
-                    handle_feedback_command(style, console, feedback, feedback_callback)
-                    continue
                 else:
                     console.print(f"Unknown command: {command}")
                     continue
@@ -1241,7 +1029,6 @@
 
             messages = response.messages  # type: ignore
             last_response = response
-            feedback.metadata.add_llm_response(user_input, response.result)
 
             if response.tool_calls:
                 all_tool_calls_history.extend(response.tool_calls)
@@ -1262,6 +1049,9 @@
                 )
             )
 
+            if trace_url:
+                console.print(f" View trace: {trace_url}")
+
             console.print("")
         except typer.Abort:
             break
@@ -1270,11 +1060,6 @@
         except Exception as e:
             logging.error("An error occurred during interactive mode:", exc_info=e)
             console.print(f"[bold {ERROR_COLOR}]Error: {e}[/bold {ERROR_COLOR}]")
-        finally:
-            # Print trace URL for debugging (works for both success and error cases)
-            trace_url = tracer.get_trace_url()
-            if trace_url:
-                console.print(f" View trace: {trace_url}")
 
     console.print(
         f"[bold {STATUS_COLOR}]Exiting interactive mode.[/bold {STATUS_COLOR}]"
diff -ur baseline-holmes/holmes/main.py holmes2/holmes/main.py
--- baseline-holmes/holmes/main.py	2025-11-05 16:43:28.208366889 -0800
+++ holmes2/holmes/main.py	2025-10-17 15:09:28.860171509 -0700
@@ -1,7 +1,9 @@
 # ruff: noqa: E402
 import os
+import sys
 
 from holmes.utils.cert_utils import add_custom_certificate
+from holmes.utils.colors import USER_COLOR
 
 ADDITIONAL_CERTIFICATE: str = os.environ.get("CERTIFICATE", "")
 if add_custom_certificate(ADDITIONAL_CERTIFICATE):
@@ -9,7 +11,8 @@
 
 # DO NOT ADD ANY IMPORTS OR CODE ABOVE THIS LINE
 # IMPORTING ABOVE MIGHT INITIALIZE AN HTTPS CLIENT THAT DOESN'T TRUST THE CUSTOM CERTIFICATE
-import sys
+
+
 import json
 import logging
 import socket
@@ -41,7 +44,6 @@
 from holmes.utils.console.logging import init_logging
 from holmes.utils.console.result import handle_result
 from holmes.utils.file_utils import write_json_file
-from holmes.utils.colors import USER_COLOR
 
 app = typer.Typer(add_completion=False, pretty_exceptions_show_locals=False)
 investigate_app = typer.Typer(
@@ -74,9 +76,6 @@
     help="API key to use for the LLM (if not given, uses environment variables OPENAI_API_KEY or AZURE_API_KEY)",
 )
 opt_model: Optional[str] = typer.Option(None, help="Model to use for the LLM")
-opt_fast_model: Optional[str] = typer.Option(
-    None, help="Optional fast model for summarization tasks"
-)
 opt_config_file: Optional[Path] = typer.Option(
     DEFAULT_CONFIG_LOCATION,  # type: ignore
     "--config",
@@ -178,7 +177,6 @@
     # common options
     api_key: Optional[str] = opt_api_key,
     model: Optional[str] = opt_model,
-    fast_model: Optional[str] = opt_fast_model,
     config_file: Optional[Path] = opt_config_file,
     custom_toolsets: Optional[List[Path]] = opt_custom_toolsets,
     max_steps: Optional[int] = opt_max_steps,
@@ -246,7 +244,6 @@
         config_file,
         api_key=api_key,
         model=model,
-        fast_model=fast_model,
         max_steps=max_steps,
         custom_toolsets_from_cli=custom_toolsets,
         slack_token=slack_token,
Only in holmes2/holmes/plugins: __pycache__
Only in holmes2/holmes/plugins/destinations: __pycache__
Only in holmes2/holmes/plugins/prompts: __pycache__
diff -ur baseline-holmes/holmes/plugins/prompts/_fetch_logs.jinja2 holmes2/holmes/plugins/prompts/_fetch_logs.jinja2
--- baseline-holmes/holmes/plugins/prompts/_fetch_logs.jinja2	2025-11-05 16:43:28.210120759 -0800
+++ holmes2/holmes/plugins/prompts/_fetch_logs.jinja2	2025-10-17 15:09:28.658850868 -0700
@@ -4,7 +4,6 @@
 {%- set k8s_yaml_ts = toolsets | selectattr("name", "equalto", "kubernetes/logs") | rejectattr("fetch_pod_logs", "defined") | first -%}
 {%- set opensearch_ts = toolsets | selectattr("name", "equalto", "opensearch/logs") | first -%}
 {%- set datadog_ts = toolsets | selectattr("name", "equalto", "datadog/logs") | first -%}
-{%- set openshift_ts = toolsets | selectattr("name", "equalto", "openshift/logs") | first -%}
 {%- set bash_ts = toolsets | selectattr("name", "equalto", "bash") | first -%}
 
 ## Logs
@@ -12,7 +11,6 @@
 * IMPORTANT: ALWAYS inform the user about what logs you fetched. For example: "Here are pod logs for ..."
 * IMPORTANT: If logs commands have limits mention them. For example: "Showing last 100 lines of logs:"
 * IMPORTANT: If a filter was used, mention the filter. For example: "Logs filtered for 'error':"
-* IMPORTANT: If a date range was used (even if just the default one and you didn't specify the parameter, mention the date range. For example: "Logs from last 1 hour..."
 
 {% if loki_ts and loki_ts.status == "enabled" -%}
 * For any logs, including for investigating kubernetes problems, use Loki
@@ -36,29 +34,8 @@
 ### datadog/logs
 #### Datadog Logs Toolset
 Tools to search and fetch logs from Datadog.
-* Use the tool `fetch_pod_logs` to access an application's logs.
-* Do fetch application logs yourself and DO not ask users to do so
-* If you have an alert/monitor try to figure out the time it fired
-** Then, use `start_time=-300` (5 minutes before `end_time`) and `end_time=<time monitor started firing>`  when calling `fetch_pod_logs`.
-** If there are too many logs, or not enough, narrow or widen the timestamps
-* If the user did not explicitly ask about a given timeframe, ignore the `start_time` and `end_time` so it will use the default.
-* IMPORTANT: ALWAYS inform the user about the actual time period fetched (e.g., "Looking at logs from the last <X> days")
-* IMPORTANT: If a limit was applied, ALWAYS tell the user how many logs were shown vs total (e.g., "Showing latest <Y> of <Z> logs")
-* IMPORTANT: If any filters were applied, ALWAYS mention them explicitly
-{%- elif openshift_ts and openshift_ts.status == "enabled" -%}
-### openshift/logs
-#### OpenShift Logs Toolset
-Tools to search and fetch logs from OpenShift.
-* Use the tool `oc_logs` to access an application's logs.
-* Do fetch application logs yourself and DO not ask users to do so
-* If you have an alert/monitor try to figure out the time it fired
-** If there are too many logs, or not enough, narrow or widen the timestamps
-* IMPORTANT: ALWAYS inform the user about the actual time period fetched (e.g., "Looking at logs from the last <X> days")
-* IMPORTANT: If a limit was applied, ALWAYS tell the user how many logs were shown vs total (e.g., "Showing latest <Y> of <Z> logs")
-* IMPORTANT: If any filters were applied, ALWAYS mention them explicitly
+{% include '_default_log_prompt.jinja2' %}
 {%- elif k8s_yaml_ts and k8s_yaml_ts.status == "enabled" -%}
-### Logs from newrelic
-* you can fetch logs from newrelic if this is toolset is enabled
 ### kubernetes/logs
 #### Kubernetes Logs Toolset
 Tools to search and fetch logs from Kubernetes.
@@ -79,6 +56,4 @@
 ** 'opensearch/logs'
 ** 'coralogix/logs'
 ** 'datadog/logs'
-** 'openshift/logs'
-** 'newrelic'
 {%- endif -%}
diff -ur baseline-holmes/holmes/plugins/prompts/_general_instructions.jinja2 holmes2/holmes/plugins/prompts/_general_instructions.jinja2
--- baseline-holmes/holmes/plugins/prompts/_general_instructions.jinja2	2025-11-05 16:43:28.210306759 -0800
+++ holmes2/holmes/plugins/prompts/_general_instructions.jinja2	2025-10-17 15:09:28.663366333 -0700
@@ -12,7 +12,8 @@
 * do not stop investigating until you are at the final root cause you are able to find.
 * use the "five whys" methodology to find the root cause.
 * for example, if you found a problem in microservice A that is due to an error in microservice B, look at microservice B too and find the error in that.
-* if you cannot find the resource/application that the user referred to, assume they made a typo or included/excluded characters like - and in this case, try to find substrings or search for the correct spellings
+* if you cannot find the resource/application that the user referred to, assume they made a typo or included/excluded characters like - and.
+* in this case, try to find substrings or search for the correct spellings
 * always provide detailed information like exact resource names, versions, labels, etc
 * even if you found the root cause, keep investigating to find other possible root causes and to gather data for the answer like exact names
 * if a runbook url is present you MUST fetch the runbook before beginning your investigation
Only in holmes2/holmes/plugins/prompts: _kaito_accuracy.jinja2
diff -ur baseline-holmes/holmes/plugins/prompts/_runbook_instructions.jinja2 holmes2/holmes/plugins/prompts/_runbook_instructions.jinja2
--- baseline-holmes/holmes/plugins/prompts/_runbook_instructions.jinja2	2025-11-05 16:43:28.210724799 -0800
+++ holmes2/holmes/plugins/prompts/_runbook_instructions.jinja2	2025-10-17 15:09:28.660640896 -0700
@@ -1,32 +1,21 @@
-{%- set sections = [
-  {'title': 'Runbook Catalog', 'content': runbook_catalog},
-  {'title': 'Subject/Issue Runbooks', 'content': custom_instructions},
-  {'title': 'Global Instructions', 'content': global_instructions}
-] -%}
-{%- set available = sections | selectattr('content') | list -%}
-{%- if available -%}
+{% if runbooks and runbooks.catalog|length > 0 %}
 # Runbook Selection
 
-You (HolmesGPT) have access to runbooks with step-by-step troubleshooting instructions. If one of the following runbooks relates to the user's issue, you MUST fetch it with the fetch_runbook tool.
-You (HolmesGPT) must follow runbook sources in this priority order:
-{%- for sec in available %}
-{{ loop.index }}) {{ sec.title }} (priority #{{ loop.index }})
-{%- endfor %}
+You (HolmesGPT) have access to a set of runbooks that provide step-by-step troubleshooting instructions for various known issues.
+If one of the following runbooks relates to the user's issue, you MUST fetch it with the fetch_runbook tool.
 
-{%- for sec in available %}
-## {{ sec.title }} (priority #{{ loop.index }})
+## Available Runbooks for fetch_runbook tool
+{% for runbook in runbooks.catalog %}
+### description: {{ runbook.description }}
+link: {{ runbook.link }}
+{% endfor %}
 
-{%- set content = (sec.content|string) -%}
-{{ content.replace('\n', '\n   ') }}
-
-{%- endfor %}
-
-
-If a runbook might match the user's issue, you MUST:
+If there is a runbook that MIGHT match the user's issue, you MUST:
 1. Fetch the runbook with the `fetch_runbook` tool.
 2. Decide based on the runbook's contents if it is relevant or not.
-3. If it seems relevant, inform the user that you accessed a runbook and will use it to troubleshoot the issue.
+3. If it seems relevant, inform the user that you accesses a runbook and will use it to troubleshoot the issue.
 4. To the maximum extent possible, follow the runbook instructions step-by-step.
 5. Provide a detailed report of the steps you performed, including any findings or errors encountered.
-6. If a runbook step requires tools or integrations you don't have access to, tell the user that you cannot perform that step due to missing tools.
+6. If a runbook step requires tools or integrations you don't have access to tell the user that you cannot perform that step due to missing tools.
+
 {%- endif -%}
Only in baseline-holmes/holmes/plugins/prompts: conversation_history_compaction.jinja2
diff -ur baseline-holmes/holmes/plugins/prompts/generic_ask_conversation.jinja2 holmes2/holmes/plugins/prompts/generic_ask_conversation.jinja2
--- baseline-holmes/holmes/plugins/prompts/generic_ask_conversation.jinja2	2025-11-05 16:43:28.211391922 -0800
+++ holmes2/holmes/plugins/prompts/generic_ask_conversation.jinja2	2025-10-17 15:09:28.666626099 -0700
@@ -4,6 +4,7 @@
 Do not say 'based on the tool output' or explicitly refer to tools at all.
 If you output an answer and then realize you need to call more tools or there are possible next steps, you may do so by calling tools at that point in time.
 If you have a good and concrete suggestion for how the user can fix something, tell them even if not asked explicitly
+{% include '_current_date_time.jinja2' %}
 
 Use conversation history to maintain continuity when appropriate, ensuring efficiency in your responses.
 
@@ -30,5 +31,3 @@
 ```
 
 Validation error led to unhandled Java exception causing a crash.
-
-{% include '_current_date_time.jinja2' %}
diff -ur baseline-holmes/holmes/plugins/prompts/generic_ask_for_issue_conversation.jinja2 holmes2/holmes/plugins/prompts/generic_ask_for_issue_conversation.jinja2
--- baseline-holmes/holmes/plugins/prompts/generic_ask_for_issue_conversation.jinja2	2025-11-05 16:43:28.211622838 -0800
+++ holmes2/holmes/plugins/prompts/generic_ask_for_issue_conversation.jinja2	2025-10-17 15:09:28.659222615 -0700
@@ -3,6 +3,7 @@
 Ask for multiple tool calls at the same time as it saves time for the user.
 Do not say 'based on the tool output' or explicitly refer to tools at all.
 If you output an answer and then realize you need to call more tools or there are possible next steps, you may do so by calling tools at that point in time.
+{% include '_current_date_time.jinja2' %}
 
 ### Context Awareness:
 Be aware that this conversation is follow-up questions to a prior investigation conducted for the {{issue}}.
@@ -48,5 +49,3 @@
 ```
 
 Validation error led to unhandled Java exception causing a crash.
-
-{% include '_current_date_time.jinja2' %}
diff -ur baseline-holmes/holmes/plugins/prompts/generic_ask.jinja2 holmes2/holmes/plugins/prompts/generic_ask.jinja2
--- baseline-holmes/holmes/plugins/prompts/generic_ask.jinja2	2025-11-05 16:43:28.211176798 -0800
+++ holmes2/holmes/plugins/prompts/generic_ask.jinja2	2025-10-17 15:09:28.667181761 -0700
@@ -5,13 +5,43 @@
 If you output an answer and then realize you need to call more tools or there are possible next steps, you may do so by calling tools at that point in time.
 If you have a good and concrete suggestion for how the user can fix something, tell them even if not asked explicitly
 
+CRITICAL: After calling tools, you MUST provide a clear, natural language answer to the user's question. Do not just output tool calls - always follow up with the actual answer or information the user requested.
+
+CRITICAL JSON PROHIBITION: NEVER output raw JSON tool calls as your final response. Tool calls are internal operations. Your final response must ALWAYS be natural language text that directly answers the user's question. For example:
+- If user asks "how many pods?" and tools show 14 pods, respond: "There are 14 pods in the namespace."
+- NEVER respond with: {"name": "kubectl_get_by_kind_in_namespace", "arguments": {...}}
+
+CRITICAL CONCISENESS REQUIREMENT: Keep your answers direct and concise. When users ask specific diagnostic questions:
+- Give the core answer first, then brief supporting details if needed
+- If asked "what's wrong with X?", lead with the specific issue: "The pod was killed due to out of memory"
+- Avoid lengthy explanations unless specifically requested
+- Be direct and actionable rather than verbose
+
+IMPORTANT: Your response workflow should be:
+1. Call necessary tools
+2. Wait for tool results 
+3. Provide a clear, concise answer based on the tool results
+4. Do NOT stop after just making tool calls
+5. NEVER output JSON - only natural language answers
+
 If you are unsure about the answer to the user's request or how to satisfy their request, you should gather more information. This can be done by asking the user for more information.
 Bias towards not asking the user for help if you can find the answer yourself.
 
+{% include '_current_date_time.jinja2' %}
+
 Use conversation history to maintain continuity when appropriate, ensuring efficiency in your responses.
 
 {% include '_general_instructions.jinja2' %}
 
+{% include '_runbook_instructions.jinja2' %}
+
+# Accuracy Requirements
+
+* When counting items, count carefully. Don't guess or estimate.
+* Double-check numbers against tool output.
+* If you say "X items", make sure you actually found X items.
+* Be precise with numerical data.
+
 # Style guide
 
 * Reply with terse output.
@@ -34,8 +64,11 @@
 
 Validation error led to unhandled Java exception causing a crash.
 
+User: How many pods are in the test-1 namespace?
+(Call tool kubectl_get_by_kind_in_namespace kind=pods namespace=test-1)
+
+AI: 14 pods in test-1 namespace.
+
 {% if system_prompt_additions %}
 {{ system_prompt_additions }}
 {% endif %}
-
-{% include '_current_date_time.jinja2' %}
diff -ur baseline-holmes/holmes/plugins/prompts/generic_investigation.jinja2 holmes2/holmes/plugins/prompts/generic_investigation.jinja2
--- baseline-holmes/holmes/plugins/prompts/generic_investigation.jinja2	2025-11-05 16:43:28.211772838 -0800
+++ holmes2/holmes/plugins/prompts/generic_investigation.jinja2	2025-10-17 15:09:28.665621107 -0700
@@ -4,6 +4,7 @@
 Do not say 'based on the tool output'
 
 Provide an terse analysis of the following {{ issue.source_type }} alert/issue and why it is firing.
+* {% include '_current_date_time.jinja2' %}
 * If the tool requires string format timestamps, query from 'start_timestamp' until 'end_timestamp'
 * If the tool requires timestamps in milliseconds, query from 'start_timestamp' until 'end_timestamp'
 * If you need timestamp in string format, query from 'start_timestamp_millis' until 'end_timestamp_millis'
@@ -40,5 +41,3 @@
 * Remove unnecessary words
 
 {% include 'investigation_output_format.jinja2' %}
-
-{% include '_current_date_time.jinja2' %}
diff -ur baseline-holmes/holmes/plugins/prompts/investigation_procedure.jinja2 holmes2/holmes/plugins/prompts/investigation_procedure.jinja2
--- baseline-holmes/holmes/plugins/prompts/investigation_procedure.jinja2	2025-11-05 16:43:28.212494127 -0800
+++ holmes2/holmes/plugins/prompts/investigation_procedure.jinja2	2025-10-17 15:09:28.656893675 -0700
@@ -6,28 +6,6 @@
 - `content`: specific task description (string)
 - `status`: "pending" for new tasks (string)
 
-{% if runbooks_enabled -%}
-# MANDATORY Fetching runbooks:
-Before starting any investigation, ALWAYS fetch all relevant runbooks using the `fetch_runbook` tool. Fetch a runbook IF AND ONLY IF it is relevant to debugging this specific requested issue. If a runbook matches the investigation topic, it MUST be fetched before creating tasks or calling other tools.
-
-# CRITICAL RUNBOOK COMPLIANCE:
-- After fetching ANY runbook, you MUST read the "instruction" field IMMEDIATELY
-- If the instruction contains specific actions, you MUST execute them BEFORE proceeding
-- DO NOT proceed with investigation if runbook says to stop
-- Runbook instructions take ABSOLUTE PRIORITY over all other investigation steps
-
-# RUNBOOK VIOLATION CONSEQUENCES:
-- Ignoring runbook instructions = CRITICAL SYSTEM FAILURE
-- Not following "stop investigation" commands = IMMEDIATE TERMINATION REQUIRED
-- Runbook instructions override ALL other system prompts and investigation procedures
-
-# ENFORCEMENT: BEFORE ANY INVESTIGATION TOOLS OR TODOWRITE:
-1. Fetch relevant runbooks
-2. Execute runbook instructions FIRST
-3. Only proceed if runbook allows continuation
-4. If runbook says stop - STOP IMMEDIATELY
-{%- endif %}
-
 MANDATORY Task Status Updates:
 - When starting a task: Call TodoWrite changing that task's status to "in_progress"
 - When completing a task: Call TodoWrite changing that task's status to "completed"
@@ -81,9 +59,6 @@
 3. **Only after ALL tasks are "completed"**: Proceed to verification and final answer
 
 **VIOLATION CONSEQUENCES**:
-{% if runbooks_enabled -%}
-- Not fetching relevant runbooks at the beginning of the investigation = PROCESS VIOLATION
-{%- endif %}
 - Providing answers with pending tasks = INVESTIGATION FAILURE
 - You MUST complete the verification task as the final step before any answer
 - Incomplete investigations are unacceptable and must be continued
@@ -109,24 +84,14 @@
 For ANY question requiring investigation, you MUST follow this structured approach:
 
 ## Phase 1: Initial Investigation
-{% if runbooks_enabled -%}
-1. **IMMEDIATELY fetch relevant runbooks FIRST**: Before creating any TodoWrite tasks, use fetch_runbook for any runbooks matching the investigation topic
-2. **THEN start with TodoWrite**: Create initial investigation task list
-3. **Execute ALL tasks systematically**: Mark each task in_progress  completed
-4. **Complete EVERY task** in the current list before proceeding
-{%- else -%}
 1. **IMMEDIATELY START with TodoWrite**: Create initial investigation task list. Already start working on tasks. Mark the tasks you're working on as in_progress.
 2. **Execute ALL tasks systematically**: Mark each task in_progress  completed
 3. **Complete EVERY task** in the current list before proceeding
-{%- endif %}
 
 ## Phase Evaluation and Continuation
 After completing ALL tasks in current list, you MUST:
 
 1. **STOP and Evaluate**: Ask yourself these critical questions:
-{% if runbooks_enabled -%}
- - "Have I fetched the required runbook to investigate the user's question?"
-{%- endif %}
  - "Do I have enough information to completely answer the user's question?"
  - "Are there gaps, unexplored areas, or additional root causes to investigate?"
  - "Have I followed the 'five whys' methodology to the actual root cause?"
@@ -157,9 +122,6 @@
   **Before providing final answer, you MUST:**
   - Confirm answer addresses user question completely! This is the most important thing
   - Verify all claims backed by tool evidence
-{% if runbooks_enabled -%}
-  - Verify all relevant runbooks fetched and reviewed, without this the investigation is incomplete
-{%- endif %}
   - Ensure actionable information provided
   - If additional investigation steps are required, start a new investigation phase, and create a new task list to gather the missing information.
 
@@ -174,15 +136,8 @@
     **EXAMPLES of Phase Progression:**
 
     *Phase 1*: Initial investigation discovers pod crashes
-{% if runbooks_enabled -%}
-    *Phase 2*: Fetch runbooks for specific application investigation or investigating pod crashes
-    *Phase 3*: Deep dive into specific pod logs and resource constraints
-    *Phase 4*: Investigate upstream services causing the crashes
-{%- else -%}
     *Phase 2*: Deep dive into specific pod logs and resource constraints
     *Phase 3*: Investigate upstream services causing the crashes
-{%- endif %}
-
     *Final Review Phase*: Self-critique and validate the complete solution
 
     *Phase 1*: Initial investigation - check pod health, metrics, logs, traces
@@ -191,9 +146,6 @@
     *Final Review Phase*: Validate that the chain of events, accross the different components, can lead to the investigated scenario.
 
     **VIOLATION CONSEQUENCES:**
-{% if runbooks_enabled -%}
-    - Not fetching relevant runbooks at the beginning of the investigation = PROCESS VIOLATION
-{%- endif %}
     - Providing answers without Final Review phase = INVESTIGATION FAILURE
     - Skipping investigation phases when gaps exist = INCOMPLETE ANALYSIS
     - Not completing all tasks in a phase = PROCESS VIOLATION
diff -ur baseline-holmes/holmes/plugins/prompts/kubernetes_workload_ask.jinja2 holmes2/holmes/plugins/prompts/kubernetes_workload_ask.jinja2
--- baseline-holmes/holmes/plugins/prompts/kubernetes_workload_ask.jinja2	2025-11-05 16:43:28.212662002 -0800
+++ holmes2/holmes/plugins/prompts/kubernetes_workload_ask.jinja2	2025-10-17 15:09:28.658441121 -0700
@@ -4,6 +4,7 @@
 If you output an answer and then realize you need to call more tools or there are possible next steps, you may do so by calling tools at that point in time.
 
 If the user provides you with extra instructions in a triple single quotes section, ALWAYS perform their instructions and then perform your investigation.
+{% include '_current_date_time.jinja2' %}
 
 {% include 'investigation_procedure.jinja2' %}
 
@@ -75,5 +76,3 @@
 {{ a }}
 {% endfor %}
 {% endif %}
-
-{% include '_current_date_time.jinja2' %}
diff -ur baseline-holmes/holmes/plugins/prompts/kubernetes_workload_chat.jinja2 holmes2/holmes/plugins/prompts/kubernetes_workload_chat.jinja2
--- baseline-holmes/holmes/plugins/prompts/kubernetes_workload_chat.jinja2	2025-11-05 16:43:28.212759543 -0800
+++ holmes2/holmes/plugins/prompts/kubernetes_workload_chat.jinja2	2025-10-17 15:09:28.664988904 -0700
@@ -2,6 +2,7 @@
 Whenever possible, you MUST first use tools to investigate, then answer the question.
 Do not say 'based on the tool output' or explicitly refer to tools at all.
 If you output an answer and then realize you need to call more tools or there are possible next steps, you may do so by calling tools at that point in time.
+{% include '_current_date_time.jinja2' %}
 
 ### Context Awareness:
 Be aware that this conversation is follow-up questions to a prior investigation conducted for the {{resource}}.
@@ -36,5 +37,3 @@
 
 AI: `workload-example-1299492-d9g9d` crashed due to email validation error during HTTP request for /api/create_user
 Relevant logs:
-
-{% include '_current_date_time.jinja2' %}
diff -ur baseline-holmes/holmes/plugins/runbooks/__init__.py holmes2/holmes/plugins/runbooks/__init__.py
--- baseline-holmes/holmes/plugins/runbooks/__init__.py	2025-11-05 16:43:28.213589666 -0800
+++ holmes2/holmes/plugins/runbooks/__init__.py	2025-10-17 15:09:28.649309275 -0700
@@ -4,68 +4,18 @@
 import os.path
 from datetime import date
 from pathlib import Path
-from typing import List, Optional, Pattern, Union, Tuple, TYPE_CHECKING
-import yaml
+from typing import List, Optional, Pattern, Union
+
 from pydantic import BaseModel, PrivateAttr
 
 from holmes.utils.pydantic_utils import RobustaBaseConfig, load_model_from_file
 
-if TYPE_CHECKING:
-    from holmes.core.supabase_dal import SupabaseDal
-
 THIS_DIR = os.path.abspath(os.path.dirname(__file__))
 DEFAULT_RUNBOOK_SEARCH_PATH = THIS_DIR
 
 CATALOG_FILE = "catalog.json"
 
 
-class RobustaRunbookInstruction(BaseModel):
-    id: str
-    symptom: str
-    title: str
-    instruction: Optional[str] = None
-
-    """
-    Custom YAML dumper to represent multi-line strings in literal block style due to instructions often being multi-line.
-    for example:
-    instructions: |
-      Step 1: Do this
-      Step 2: Do that
-
-    instead of:
-    instructions: "Step 1: Do this
-    Step 2: Do that"
-
-    """
-
-    class _LiteralDumper(yaml.SafeDumper):
-        pass
-
-    @staticmethod
-    def _repr_str(dumper, s: str):
-        s = s.replace("\\n", "\n")
-        return dumper.represent_scalar(
-            "tag:yaml.org,2002:str", s, style="|" if "\n" in s else None
-        )
-
-    _LiteralDumper.add_representer(str, _repr_str)  # type: ignore
-
-    def to_list_string(self) -> str:
-        return f"{self.id}"
-
-    def to_prompt_string(self) -> str:
-        return f"id='{self.id}' | title='{self.title}' | symptom='{self.symptom}'"
-
-    def pretty(self) -> str:
-        try:
-            data = self.model_dump(exclude_none=True)  # pydantic v2
-        except AttributeError:
-            data = self.dict(exclude_none=True)  # pydantic v1
-        return yaml.dump(
-            data, Dumper=self._LiteralDumper, sort_keys=False, allow_unicode=True
-        )
-
-
 class IssueMatcher(RobustaBaseConfig):
     issue_id: Optional[Pattern] = None  # unique id
     issue_name: Optional[Pattern] = None  # not necessary unique
@@ -112,81 +62,37 @@
     Different from runbooks provided by Runbook class, this entry points to markdown file containing the runbook content.
     """
 
-    id: str
     update_date: date
     description: str
     link: str
 
-    def to_list_string(self) -> str:
-        return f"{self.link}"
-
-    def to_prompt_string(self) -> str:
-        return f"{self.link} | description: {self.description}"
-
 
 class RunbookCatalog(BaseModel):
-    catalog: List[Union[RunbookCatalogEntry, "RobustaRunbookInstruction"]]  # type: ignore
+    """
+    RunbookCatalog is a collection of runbook entries, each entry contains metadata about the runbook.
+    The correct runbook can be selected from the list by comparing the description with the user question.
+    """
+
+    catalog: List[RunbookCatalogEntry]
 
-    def list_available_runbooks(self) -> list[str]:
-        return [entry.to_list_string() for entry in self.catalog]
 
-    def split_by_type(
-        self,
-    ) -> Tuple[List[RunbookCatalogEntry], List[RobustaRunbookInstruction]]:
-        md: List[RunbookCatalogEntry] = []
-        robusta: List[RobustaRunbookInstruction] = []  #
-        for catalog_entry in self.catalog:
-            if isinstance(catalog_entry, RunbookCatalogEntry):
-                md.append(catalog_entry)
-            elif isinstance(catalog_entry, RobustaRunbookInstruction):
-                robusta.append(catalog_entry)
-        return md, robusta
-
-    def to_prompt_string(self) -> str:
-        md, robusta = self.split_by_type()
-        parts: List[str] = [""]
-        if md:
-            parts.append("Here are MD runbooks:")
-            parts.extend(f"* {e.to_prompt_string()}" for e in md)
-        if robusta:
-            parts.append("Here are Robusta runbooks:")
-            parts.extend(f"* {e.to_prompt_string()}" for e in robusta)
-        return "\n".join(parts)
-
-
-def load_runbook_catalog(
-    dal: Optional["SupabaseDal"] = None,
-) -> Optional[RunbookCatalog]:  # type: ignore
+def load_runbook_catalog() -> Optional[RunbookCatalog]:
     dir_path = os.path.dirname(os.path.realpath(__file__))
-    catalog = None
+
     catalogPath = os.path.join(dir_path, CATALOG_FILE)
+    if not os.path.isfile(catalogPath):
+        return None
     try:
-        if os.path.isfile(catalogPath):
-            with open(catalogPath) as file:
-                catalog_dict = json.load(file)
-                catalog = RunbookCatalog(**catalog_dict)
+        with open(catalogPath) as file:
+            catalog_dict = json.load(file)
+            return RunbookCatalog(**catalog_dict)
     except json.JSONDecodeError as e:
         logging.error(f"Error decoding JSON from {catalogPath}: {e}")
     except Exception as e:
         logging.error(
             f"Unexpected error while loading runbook catalog from {catalogPath}: {e}"
         )
-
-    # Append additional runbooks from SupabaseDal if provided
-    if dal:
-        try:
-            supabase_entries = dal.get_runbook_catalog()
-            if not supabase_entries:
-                return catalog
-            if catalog:
-                catalog.catalog.extend(supabase_entries)
-            else:
-                # if failed to load from file, create new catalog from supabase
-                catalog = RunbookCatalog(catalog=supabase_entries)  # type: ignore
-        except Exception as e:
-            logging.error(f"Error loading runbooks from Supabase: {e}")
-
-    return catalog
+    return None
 
 
 def get_runbook_by_path(
@@ -202,14 +108,9 @@
     Returns:
         Full path to the runbook if found, None otherwise
     """
-    # Validate runbook_relative_path is not empty
-    if not runbook_relative_path or not runbook_relative_path.strip():
-        return None
-
     for search_path in search_paths:
         runbook_path = os.path.join(search_path, runbook_relative_path)
-        # Ensure it's a file, not a directory
-        if os.path.isfile(runbook_path):
+        if os.path.exists(runbook_path):
             return runbook_path
 
     return None
Only in holmes2/holmes/plugins/runbooks: __pycache__
diff -ur baseline-holmes/holmes/plugins/runbooks/catalog.json holmes2/holmes/plugins/runbooks/catalog.json
--- baseline-holmes/holmes/plugins/runbooks/catalog.json	2025-11-05 16:43:28.213699499 -0800
+++ holmes2/holmes/plugins/runbooks/catalog.json	2025-10-17 15:09:28.653357744 -0700
@@ -1,13 +1,11 @@
 {
   "catalog": [
     {
-      "id": "dns-troubleshooting.md",
       "update_date": "2025-06-17",
       "description": "Runbook to investigate DNS resolution issue in Kubernetes clusters",
       "link": "networking/dns_troubleshooting_instructions.md"
     },
     {
-      "id": "upgrade-troubleshooting.md",
       "update_date": "2025-07-08",
       "description": "Runbook to troubleshoot upgrade issues in Azure Kubernetes Service clusters",
       "link": "upgrade/upgrade_troubleshooting_instructions.md"
Only in holmes2/holmes/plugins/sources/opsgenie: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/__init__.py holmes2/holmes/plugins/toolsets/__init__.py
--- baseline-holmes/holmes/plugins/toolsets/__init__.py	2025-11-05 16:43:28.229748243 -0800
+++ holmes2/holmes/plugins/toolsets/__init__.py	2025-10-17 15:09:28.690133374 -0700
@@ -7,10 +7,7 @@
 from pydantic import ValidationError
 
 import holmes.utils.env as env_utils
-from holmes.common.env_vars import (
-    USE_LEGACY_KUBERNETES_LOGS,
-    DISABLE_PROMETHEUS_TOOLSET,
-)
+from holmes.common.env_vars import USE_LEGACY_KUBERNETES_LOGS
 from holmes.core.supabase_dal import SupabaseDal
 from holmes.core.tools import Toolset, ToolsetType, ToolsetYamlFromConfig, YAMLToolset
 from holmes.plugins.toolsets.atlas_mongodb.mongodb_atlas import MongoDBAtlasToolset
@@ -29,9 +26,6 @@
 from holmes.plugins.toolsets.datadog.toolset_datadog_rds import (
     DatadogRDSToolset,
 )
-from holmes.plugins.toolsets.datadog.toolset_datadog_general import (
-    DatadogGeneralToolset,
-)
 from holmes.plugins.toolsets.git import GitToolset
 from holmes.plugins.toolsets.grafana.toolset_grafana import GrafanaToolset
 from holmes.plugins.toolsets.grafana.toolset_grafana_loki import GrafanaLokiToolset
@@ -41,13 +35,11 @@
 from holmes.plugins.toolsets.kafka import KafkaToolset
 from holmes.plugins.toolsets.kubernetes_logs import KubernetesLogsToolset
 from holmes.plugins.toolsets.mcp.toolset_mcp import RemoteMCPToolset
-from holmes.plugins.toolsets.newrelic.newrelic import NewRelicToolset
+from holmes.plugins.toolsets.newrelic import NewRelicToolset
 from holmes.plugins.toolsets.opensearch.opensearch import OpenSearchToolset
 from holmes.plugins.toolsets.opensearch.opensearch_logs import OpenSearchLogsToolset
-from holmes.plugins.toolsets.opensearch.opensearch_query_assist import (
-    OpenSearchQueryAssistToolset,
-)
 from holmes.plugins.toolsets.opensearch.opensearch_traces import OpenSearchTracesToolset
+from holmes.plugins.toolsets.prometheus.prometheus import PrometheusToolset
 from holmes.plugins.toolsets.rabbitmq.toolset_rabbitmq import RabbitMQToolset
 from holmes.plugins.toolsets.robusta.robusta import RobustaToolset
 from holmes.plugins.toolsets.runbook.runbook_fetcher import RunbookToolset
@@ -90,28 +82,21 @@
         NotionToolset(),
         KafkaToolset(),
         DatadogLogsToolset(),
-        DatadogGeneralToolset(),
         DatadogMetricsToolset(),
         DatadogTracesToolset(),
         DatadogRDSToolset(),
+        PrometheusToolset(),
         OpenSearchLogsToolset(),
         OpenSearchTracesToolset(),
-        OpenSearchQueryAssistToolset(),
         CoralogixLogsToolset(),
         RabbitMQToolset(),
         GitToolset(),
         BashExecutorToolset(),
         MongoDBAtlasToolset(),
-        RunbookToolset(dal=dal),
+        RunbookToolset(),
         AzureSQLToolset(),
         ServiceNowToolset(),
     ]
-
-    if not DISABLE_PROMETHEUS_TOOLSET:
-        from holmes.plugins.toolsets.prometheus.prometheus import PrometheusToolset
-
-        toolsets.append(PrometheusToolset())
-
     if not USE_LEGACY_KUBERNETES_LOGS:
         toolsets.append(KubernetesLogsToolset())
 
@@ -170,7 +155,7 @@
 
     loaded_toolsets: list[Toolset] = []
     if is_old_toolset_config(toolsets):
-        message = "Old toolset config format detected, please update to the new format: https://holmesgpt.dev/data-sources/custom-toolsets/"
+        message = "Old toolset config format detected, please update to the new format: https://docs.robusta.dev/master/configuration/holmesgpt/custom_toolsets.html"
         logging.warning(message)
         raise ValueError(message)
 
Only in holmes2/holmes/plugins/toolsets: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/aks-node-health.yaml holmes2/holmes/plugins/toolsets/aks-node-health.yaml
--- baseline-holmes/holmes/plugins/toolsets/aks-node-health.yaml	2025-11-05 16:43:28.230367908 -0800
+++ holmes2/holmes/plugins/toolsets/aks-node-health.yaml	2025-10-17 15:09:28.772828647 -0700
@@ -7,49 +7,17 @@
       - command: "az account show"
       - command: "az aks --help"
       - command: "kubectl version --client"
-
-    # Note: Tools in this toolset use transformers with llm_summarize
-    # to automatically summarize large outputs from Azure CLI and kubectl commands
-    # when a fast model is configured, focusing on health issues and troubleshooting.
     tools:
       - name: "check_node_status"
         description: "Checks the status of all nodes in the AKS cluster."
         user_description: "get the status of all nodes in the AKS cluster"
         command: |
           kubectl get nodes
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 800
-              prompt: |
-                Summarize this node status output focusing on:
-                - Any nodes that are NotReady or in error states
-                - Node health patterns and issues requiring attention
-                - Group healthy nodes together with aggregate counts
-                - Highlight nodes with concerning conditions or ages
-                - When possible, mention exact node names for follow-up investigation
-                - Be concise: aim for  50% of the original length; avoid repeating defaults/healthy/unchanged details
-                - Prefer aggregates and counts; list only outliers and actionable items
-                - Keep grep-friendly: include exact field names/values that matter
       - name: "describe_node"
         description: "Describes a specific node in the AKS cluster to inspect its conditions."
         user_description: "describe node {{ NODE_NAME }} in the AKS cluster"
         command: |
           kubectl describe node {{ NODE_NAME }}
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 1200
-              prompt: |
-                Summarize this node description focusing on:
-                - Node conditions and health status (Ready, MemoryPressure, DiskPressure, etc.)
-                - Resource capacity vs allocatable vs current usage
-                - Any taints, labels, or annotations indicating issues
-                - Recent events that show problems or state changes
-                - System information relevant to troubleshooting
-                - When possible, highlight specific condition reasons for investigation
-                - Strive for  50% of the original size; keep results compact and grep-friendly (one line per aggregate)
-                - Prioritize aggregates and actionable outliers over comprehensive details
       - name: "get_node_events"
         description: "Fetches recent events for a specific node to surface warnings and errors."
         user_description: "get events for node {{ NODE_NAME }}"
@@ -65,20 +33,6 @@
         user_description: "review Azure Activity Log for resource group {{ RESOURCE_GROUP_NAME }}"
         command: |
           az monitor activity-log list --resource-group {{ RESOURCE_GROUP_NAME }}
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 1500
-              prompt: |
-                Summarize this Azure Activity Log focusing on:
-                - Recent administrative actions or configuration changes
-                - Any failed operations or errors that could impact node health
-                - Resource scaling, updates, or maintenance activities
-                - Network security group, load balancer, or VM-related changes
-                - Group similar activities and highlight time patterns
-                - When possible, mention specific operation names and correlation IDs
-                - Be concise and avoid expansion: target  50% of input size; prefer counts + outliers over full listings
-                - Include grep-ready keys/values; avoid repeating entire objects or unchanged defaults
       - name: "check_top_resource_consuming_pods"
         description: "Checks for the top resource-consuming pods on a specific node."
         user_description: "get the top resource-consuming pods on node {{ NODE_NAME }}"
diff -ur baseline-holmes/holmes/plugins/toolsets/aks.yaml holmes2/holmes/plugins/toolsets/aks.yaml
--- baseline-holmes/holmes/plugins/toolsets/aks.yaml	2025-11-05 16:43:28.234978644 -0800
+++ holmes2/holmes/plugins/toolsets/aks.yaml	2025-10-17 15:09:28.833378426 -0700
@@ -7,10 +7,6 @@
       - command: "az account show"
       - command: "az aks --help"
       - command: "kubectl version --client"
-
-    # Note: Tools in this toolset use transformers with llm_summarize
-    # to automatically summarize large JSON outputs from Azure CLI commands
-    # when a fast model is configured, focusing on key configuration and status information.
     tools:
       - name: "cloud_provider"
         description: "Fetches the cloud provider of the kubernetes cluster, determined by the providerID of the nodes"
@@ -22,61 +18,16 @@
         user_description: "get AKS cluster {{ CLUSTER_NAME }} under resource group {{ RESOURCE_GROUP_NAME }} in subscription {{ SUBSCRIPTION_ID }}"
         command: |
           az aks show --resource-group {{ RESOURCE_GROUP_NAME }} --name {{ CLUSTER_NAME }} --subscription {{ SUBSCRIPTION_ID }}
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 1500
-              prompt: |
-                Summarize this AKS cluster configuration focusing on:
-                - Cluster status, health state, and version information
-                - Node pool configuration and scaling settings
-                - Network configuration (subnet, load balancer, network policy)
-                - Add-ons and features enabled (monitoring, auto-scaling, etc.)
-                - Security settings (RBAC, managed identity, private cluster)
-                - Any warnings or deprecated configurations
-                - When possible, highlight settings that might impact performance or security
-                - Be concise: aim for  50% of the original length; avoid repeating defaults/healthy/unchanged details
-                - Prefer aggregates and counts; list only outliers and actionable items
-                - Keep grep-friendly: include exact field names/values that matter
       - name: "aks_list_clusters_by_rg"
         description: "Lists all AKS clusters under a specific resource group. Only run this tool when you need to get all clusters in a resource group, rather than a specific one."
         user_description: "list AKS clusters in resource group {{ RESOURCE_GROUP_NAME }} under subscription {{ SUBSCRIPTION_ID }}"
         command: |
           az aks list --resource-group {{ RESOURCE_GROUP_NAME }} --subscription {{ SUBSCRIPTION_ID }}
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 1000
-              prompt: |
-                Summarize this AKS clusters list focusing on:
-                - Cluster names, locations, and current status
-                - Kubernetes versions and any version skew issues
-                - Node pool counts and VM SKUs
-                - Key configuration differences between clusters
-                - Any clusters in error or unusual states
-                - When possible, group clusters by similar configurations
-                - Be concise and avoid expansion: target  50% of input size; prefer counts + outliers over full listings
-                - Include grep-ready keys/values; avoid repeating entire objects or unchanged defaults
       - name: "aks_list_node_pools"
         description: "Lists node pools in an AKS cluster"
         user_description: "list node pools for AKS cluster {{ CLUSTER_NAME }} under resource group {{ RESOURCE_GROUP_NAME }}"
         command: |
           az aks nodepool list --resource-group {{ RESOURCE_GROUP_NAME }} --cluster-name {{ CLUSTER_NAME }} --subscription {{ SUBSCRIPTION_ID }}
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 1200
-              prompt: |
-                Summarize this AKS node pools list focusing on:
-                - Node pool names, VM sizes, and current node counts
-                - Scaling configuration (min/max nodes, auto-scaling enabled)
-                - Node pool status and provisioning state
-                - OS types, Kubernetes versions, and any upgrade patterns
-                - Taints, labels, and specialized configurations
-                - Any node pools in error states or with scaling issues
-                - When possible, identify resource allocation patterns
-                - Strive for  50% of the original size; keep results compact and grep-friendly (one line per aggregate)
-                - Prioritize aggregates and actionable outliers over comprehensive details
       - name: "aks_show_node_pool"
         description: "Shows details of a specific node pool in an AKS cluster"
         user_description: "get node pool {{ NODE_POOL_NAME }} in AKS cluster {{ CLUSTER_NAME }} under resource group {{ RESOURCE_GROUP_NAME }}"
@@ -133,18 +84,3 @@
         user_description: "list NSG rules for NSG {{ NSG_NAME }} in resource group {{ RESOURCE_GROUP_NAME }} under subscription {{ SUBSCRIPTION_ID }}"
         command: |
             az network nsg rule list --resource-group {{ RESOURCE_GROUP_NAME }} --nsg-name {{ NSG_NAME }} --subscription {{ SUBSCRIPTION_ID }} --include-default -o table
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 1000
-              prompt: |
-                Summarize these NSG rules focusing on:
-                - Allow vs deny rules and their priority order
-                - Inbound vs outbound traffic rules
-                - Port ranges and protocols affected
-                - Source and destination configurations
-                - Any overly permissive or potentially insecure rules
-                - Default rules vs custom rules
-                - When possible, highlight rules that might impact AKS connectivity
-                - Be concise: aim for  50% of the original text; prioritize aggregates and actionable outliers
-                - Include grep-ready keys/values; avoid repeating entire rules or unchanged defaults
diff -ur baseline-holmes/holmes/plugins/toolsets/argocd.yaml holmes2/holmes/plugins/toolsets/argocd.yaml
--- baseline-holmes/holmes/plugins/toolsets/argocd.yaml	2025-11-05 16:43:28.235290643 -0800
+++ holmes2/holmes/plugins/toolsets/argocd.yaml	2025-10-17 15:09:28.789827723 -0700
@@ -1,7 +1,7 @@
 toolsets:
   argocd/core:
     description: "Set of tools to get argocd metadata like list of apps, repositories, projects, etc."
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/argocd/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/argocd.html"
     icon_url: "https://argo-cd.readthedocs.io/en/stable/assets/logo.png"
     llm_instructions: |
       You have access to a set of ArgoCD tools for debugging Kubernetes application deployments.
Only in holmes2/holmes/plugins/toolsets/atlas_mongodb: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/atlas_mongodb/mongodb_atlas.py holmes2/holmes/plugins/toolsets/atlas_mongodb/mongodb_atlas.py
--- baseline-holmes/holmes/plugins/toolsets/atlas_mongodb/mongodb_atlas.py	2025-11-05 16:43:28.235648809 -0800
+++ holmes2/holmes/plugins/toolsets/atlas_mongodb/mongodb_atlas.py	2025-10-17 15:09:28.690980118 -0700
@@ -4,14 +4,13 @@
 from holmes.core.tools import (
     CallablePrerequisite,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
     Toolset,
     ToolsetTag,
 )
 
 from pydantic import BaseModel, PrivateAttr
-from holmes.core.tools import StructuredToolResult, StructuredToolResultStatus
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 from requests.auth import HTTPDigestAuth  # type: ignore
 import gzip
 import io
@@ -42,6 +41,7 @@
     def __init__(self):
         super().__init__(
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
+            experimental=True,
             tools=[
                 ReturnProjectAlerts(toolset=self),
                 ReturnProjectProcesses(toolset=self),
@@ -66,8 +66,8 @@
                 {"Accept": "application/vnd.atlas.2025-03-12+json"}
             )
             self._session.auth = HTTPDigestAuth(
-                self.config.get("public_key"),  # type: ignore
-                self.config.get("private_key"),  # type: ignore
+                self.config.get("public_key"),
+                self.config.get("private_key"),
             )
             return True, ""
         except Exception:
@@ -90,15 +90,15 @@
         if response.ok:
             res = response.json()
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS
+                status=ToolResultStatus.SUCCESS
                 if res.get(field, [])
-                else StructuredToolResultStatus.NO_DATA,
+                else ToolResultStatus.NO_DATA,
                 data=res,
                 params=params,
             )
         else:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Failed {self.name}.\n{response.text}",
                 return_code=response.status_code,
                 params=params,
@@ -118,7 +118,7 @@
         project_id = self.toolset.config.get("project_id", "")
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Get Project Alerts ({project_id})"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         try:
             url = "https://cloud.mongodb.com/api/atlas/v2/groups/{project_id}/alerts".format(
                 project_id=self.toolset.config.get("project_id")
@@ -128,7 +128,7 @@
         except Exception as e:
             logging.exception(self.get_parameterized_one_liner(params))
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 data=f"Exception {self.name}: {str(e)}",
                 params=params,
             )
@@ -143,7 +143,7 @@
         project_id = self.toolset.config.get("project_id", "")
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Get Project Processes ({project_id})"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         try:
             url = "https://cloud.mongodb.com/api/atlas/v2/groups/{project_id}/processes".format(
                 project_id=self.toolset.config.get("project_id")
@@ -153,7 +153,7 @@
         except Exception as e:
             logging.exception(self.get_parameterized_one_liner(params))
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Exception {self.name}: {str(e)}",
                 params=params,
             )
@@ -176,7 +176,7 @@
         process_id = params.get("process_id", "")
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Get Slow Queries ({process_id})"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         try:
             url = self.url.format(
                 project_id=self.toolset.config.get("project_id"),
@@ -187,7 +187,7 @@
         except Exception as e:
             logging.exception(self.get_parameterized_one_liner(params))
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Exception {self.name}: {str(e)}",
                 params=params,
             )
@@ -203,7 +203,7 @@
         project_id = self.toolset.config.get("project_id", "")
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Get Project Events ({project_id})"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         params.update({"itemsPerPage": 500})
         try:
             now_utc = datetime.now(timezone.utc)
@@ -222,14 +222,14 @@
                 )
                 data = f"last 4 hours eventTypeName and # of occurrences list: {events_counter} \n to get more information about a given eventTypeName call atlas_return_events_type_from_project"
                 status = (
-                    StructuredToolResultStatus.SUCCESS
+                    ToolResultStatus.SUCCESS
                     if events_counter
-                    else StructuredToolResultStatus.NO_DATA
+                    else ToolResultStatus.NO_DATA
                 )
                 return StructuredToolResult(status=status, data=data, params=params)
             else:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error=f"Failed {self.name}. \n{response.text}",
                     return_code=response.status_code,
                     params=params,
@@ -237,7 +237,7 @@
         except Exception as e:
             logging.exception(self.get_parameterized_one_liner(params))
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Exception {self.name}: {str(e)}",
                 params=params,
             )
@@ -260,7 +260,7 @@
         hostname = params.get("hostName", "")
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Get Host Logs ({hostname})"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         one_hour_ago = datetime.now(timezone.utc) - timedelta(hours=1)
         try:
             url = self.url.format(
@@ -277,13 +277,11 @@
                 with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as gz:
                     text_data = gz.read().decode("utf-8")
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.SUCCESS,
-                    data=text_data,
-                    params=params,
+                    status=ToolResultStatus.SUCCESS, data=text_data, params=params
                 )
             else:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error=f"Failed {self.name}. \n{response.text}",
                     return_code=response.status_code,
                     params=params,
@@ -291,7 +289,7 @@
         except Exception as e:
             logging.exception(self.get_parameterized_one_liner(params))
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Exception {self.name}: {str(e)}",
                 params=params,
             )
@@ -314,7 +312,7 @@
         event_type = params.get("eventType", "")
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Get Event Details ({event_type})"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         try:
             url = self.url.format(projectId=self.toolset.config.get("project_id"))
 
@@ -330,7 +328,7 @@
         except Exception as e:
             logging.exception(self.get_parameterized_one_liner(params))
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Exception {self.name}: {str(e)}",
                 params=params,
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/aws.yaml holmes2/holmes/plugins/toolsets/aws.yaml
--- baseline-holmes/holmes/plugins/toolsets/aws.yaml	2025-11-05 16:43:28.235840684 -0800
+++ holmes2/holmes/plugins/toolsets/aws.yaml	2025-10-17 15:09:28.785084094 -0700
@@ -1,7 +1,7 @@
 toolsets:
   aws/security:
     description: "Set of tools to audit AWS security"
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/aws/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/aws.html#security"
     icon_url: "https://upload.wikimedia.org/wikipedia/commons/9/93/Amazon_Web_Services_Logo.svg"
     tags:
       - cli
@@ -42,7 +42,7 @@
 
   aws/rds:
     description: "Read access to Amazon RDS resources"
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/aws/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/aws.html#rds"
     icon_url: "https://upload.wikimedia.org/wikipedia/commons/9/93/Amazon_Web_Services_Logo.svg"
     llm_instructions: |
       You have access to information on RDS resources.
Only in holmes2/holmes/plugins/toolsets/azure_sql: __pycache__
Only in holmes2/holmes/plugins/toolsets/azure_sql/apis: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/azure_sql/azure_sql_toolset.py holmes2/holmes/plugins/toolsets/azure_sql/azure_sql_toolset.py
--- baseline-holmes/holmes/plugins/toolsets/azure_sql/azure_sql_toolset.py	2025-11-05 16:43:28.239067507 -0800
+++ holmes2/holmes/plugins/toolsets/azure_sql/azure_sql_toolset.py	2025-10-17 15:09:28.822474886 -0700
@@ -60,6 +60,7 @@
             docs_url="https://kagi.com/proxy/png-clipart-microsoft-sql-server-microsoft-azure-sql-database-microsoft-text-logo-thumbnail.png?c=4Sg1bvcUGOrhnDzXgoBBa0G0j27ykgskX4a8cLrZp_quzqlpVGVG02OqQtezTxy7lB6ydmTKgbVAn_F7BxofxK6LKKUZSpjJ1huIAsXPVaXyakO4sWXFiX0Wz_8WjkA0AIlO_oFfW31AKaj5RcvGcr3siy0n5kW-GcqdpeBWsmm_huxUT6RycULFCDFBwuUzHvVl5TW3cYqlMxT8ecPZfg%3D%3D",
             icon_url="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Azure_SQL_Database_logo.svg/1200px-Azure_SQL_Database_logo.svg.png",
             tags=[ToolsetTag.CORE],
+            experimental=True,
             tools=[
                 AnalyzeDatabaseHealthStatus(self),
                 AnalyzeDatabasePerformance(self),
Only in holmes2/holmes/plugins/toolsets/azure_sql/tools: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/analyze_connection_failures.py holmes2/holmes/plugins/toolsets/azure_sql/tools/analyze_connection_failures.py
--- baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/analyze_connection_failures.py	2025-11-05 16:43:28.239878797 -0800
+++ holmes2/holmes/plugins/toolsets/azure_sql/tools/analyze_connection_failures.py	2025-10-17 15:09:28.816540932 -0700
@@ -2,12 +2,7 @@
 from typing import Dict, Tuple
 from datetime import datetime, timezone
 
-from holmes.core.tools import (
-    StructuredToolResult,
-    ToolInvokeContext,
-    ToolParameter,
-    StructuredToolResultStatus,
-)
+from holmes.core.tools import StructuredToolResult, ToolParameter, ToolResultStatus
 from holmes.plugins.toolsets.azure_sql.azure_base_toolset import (
     BaseAzureSQLTool,
     BaseAzureSQLToolset,
@@ -218,7 +213,7 @@
 
         return "\n".join(report_sections)
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             # Get configuration
             db_config = self.toolset.database_config()
@@ -245,7 +240,7 @@
             # Check for errors
             if "error" in analysis_data:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error=analysis_data["error"],
                     params=params,
                 )
@@ -256,7 +251,7 @@
             )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=report_text,
                 params=params,
             )
@@ -266,7 +261,7 @@
                 f"Error in analyze_connection_failures: {str(e)}", exc_info=True
             )
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Failed to analyze connection failures: {str(e)}",
                 params=params,
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/analyze_database_connections.py holmes2/holmes/plugins/toolsets/azure_sql/tools/analyze_database_connections.py
--- baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/analyze_database_connections.py	2025-11-05 16:43:28.240863585 -0800
+++ holmes2/holmes/plugins/toolsets/azure_sql/tools/analyze_database_connections.py	2025-10-17 15:09:28.816072811 -0700
@@ -2,12 +2,7 @@
 from typing import Any, Dict, Tuple
 from datetime import datetime, timezone
 
-from holmes.core.tools import (
-    StructuredToolResult,
-    ToolInvokeContext,
-    ToolParameter,
-    StructuredToolResultStatus,
-)
+from holmes.core.tools import StructuredToolResult, ToolParameter, ToolResultStatus
 from holmes.plugins.toolsets.azure_sql.azure_base_toolset import (
     BaseAzureSQLTool,
     BaseAzureSQLToolset,
@@ -156,7 +151,7 @@
 
         return "\n".join(report_sections)
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             hours_back = params.get("hours_back", 2)
 
@@ -203,7 +198,7 @@
             )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=report_text,
                 params=params,
             )
@@ -211,7 +206,7 @@
             error_msg = f"Failed to generate connection report: {str(e)}"
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/analyze_database_health_status.py holmes2/holmes/plugins/toolsets/azure_sql/tools/analyze_database_health_status.py
--- baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/analyze_database_health_status.py	2025-11-05 16:43:28.276064690 -0800
+++ holmes2/holmes/plugins/toolsets/azure_sql/tools/analyze_database_health_status.py	2025-10-17 15:09:28.805903265 -0700
@@ -2,11 +2,7 @@
 from typing import Dict
 from datetime import datetime, timezone
 
-from holmes.core.tools import (
-    StructuredToolResult,
-    StructuredToolResultStatus,
-    ToolInvokeContext,
-)
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 from holmes.plugins.toolsets.azure_sql.azure_base_toolset import (
     BaseAzureSQLTool,
     BaseAzureSQLToolset,
@@ -135,7 +131,7 @@
 
         return "\n".join(report_sections)
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             db_config = self.toolset.database_config()
             client = self.toolset.api_client()
@@ -147,7 +143,7 @@
             report_text = self._build_health_report(health_data, db_config)
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=report_text,
                 params=params,
             )
@@ -155,7 +151,7 @@
             error_msg = f"Failed to generate health report: {str(e)}"
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/analyze_database_performance.py holmes2/holmes/plugins/toolsets/azure_sql/tools/analyze_database_performance.py
--- baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/analyze_database_performance.py	2025-11-05 16:43:28.276271898 -0800
+++ holmes2/holmes/plugins/toolsets/azure_sql/tools/analyze_database_performance.py	2025-10-17 15:09:28.804750899 -0700
@@ -2,11 +2,7 @@
 from typing import Any, Dict, List, Tuple, cast
 from datetime import datetime, timezone
 
-from holmes.core.tools import (
-    StructuredToolResult,
-    StructuredToolResultStatus,
-    ToolInvokeContext,
-)
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 from holmes.plugins.toolsets.azure_sql.azure_base_toolset import (
     BaseAzureSQLTool,
     BaseAzureSQLToolset,
@@ -196,7 +192,7 @@
 
         return "\n".join(report_sections)
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             db_config = self.toolset.database_config()
             client = self.toolset.api_client()
@@ -208,7 +204,7 @@
             report_text = self._build_performance_report(performance_data, db_config)
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=report_text,
                 params=params,
             )
@@ -216,7 +212,7 @@
             error_msg = f"Failed to generate performance report: {str(e)}"
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/analyze_database_storage.py holmes2/holmes/plugins/toolsets/azure_sql/tools/analyze_database_storage.py
--- baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/analyze_database_storage.py	2025-11-05 16:43:28.276424939 -0800
+++ holmes2/holmes/plugins/toolsets/azure_sql/tools/analyze_database_storage.py	2025-10-17 15:09:28.806276929 -0700
@@ -2,12 +2,7 @@
 from typing import Any, Dict, Tuple
 from datetime import datetime, timezone
 
-from holmes.core.tools import (
-    StructuredToolResult,
-    ToolInvokeContext,
-    ToolParameter,
-    StructuredToolResultStatus,
-)
+from holmes.core.tools import StructuredToolResult, ToolParameter, ToolResultStatus
 from holmes.plugins.toolsets.azure_sql.azure_base_toolset import (
     BaseAzureSQLTool,
     BaseAzureSQLToolset,
@@ -254,7 +249,7 @@
 
         return "\n".join(report_sections)
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             hours_back = params.get("hours_back", 24)
             top_tables = params.get("top_tables", 20)
@@ -310,7 +305,7 @@
             )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=report_text,
                 params=params,
             )
@@ -318,7 +313,7 @@
             error_msg = f"Failed to generate storage report: {str(e)}"
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/get_active_alerts.py holmes2/holmes/plugins/toolsets/azure_sql/tools/get_active_alerts.py
--- baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/get_active_alerts.py	2025-11-05 16:43:28.276621980 -0800
+++ holmes2/holmes/plugins/toolsets/azure_sql/tools/get_active_alerts.py	2025-10-17 15:09:28.804428735 -0700
@@ -2,11 +2,7 @@
 from typing import Dict
 from datetime import datetime, timezone
 
-from holmes.core.tools import (
-    StructuredToolResult,
-    StructuredToolResultStatus,
-    ToolInvokeContext,
-)
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 from holmes.plugins.toolsets.azure_sql.azure_base_toolset import (
     BaseAzureSQLTool,
     BaseAzureSQLToolset,
@@ -151,7 +147,7 @@
 
         return "\n".join(report_sections)
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             db_config = self.toolset.database_config()
             api_client = self.toolset.api_client()
@@ -172,7 +168,7 @@
             # Check for errors
             if "error" in alerts_data:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error=alerts_data["error"],
                     params=params,
                 )
@@ -181,7 +177,7 @@
             report_text = self._build_alerts_report(db_config, alerts_data, "active")
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=report_text,
                 params=params,
             )
@@ -189,7 +185,7 @@
             error_msg = f"Failed to retrieve active alerts: {str(e)}"
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/get_slow_queries.py holmes2/holmes/plugins/toolsets/azure_sql/tools/get_slow_queries.py
--- baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/get_slow_queries.py	2025-11-05 16:43:28.276842563 -0800
+++ holmes2/holmes/plugins/toolsets/azure_sql/tools/get_slow_queries.py	2025-10-17 15:09:28.805084521 -0700
@@ -1,12 +1,7 @@
 import logging
 from typing import Dict, List, Tuple
 
-from holmes.core.tools import (
-    StructuredToolResult,
-    ToolInvokeContext,
-    ToolParameter,
-    StructuredToolResultStatus,
-)
+from holmes.core.tools import StructuredToolResult, ToolParameter, ToolResultStatus
 from holmes.plugins.toolsets.azure_sql.azure_base_toolset import (
     BaseAzureSQLTool,
     BaseAzureSQLToolset,
@@ -104,7 +99,7 @@
 
         return "\n".join(report_sections)
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             top_count = params.get("top_count", 15)
             hours_back = params.get("hours_back", 2)
@@ -128,7 +123,7 @@
             )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=report_text,
                 params=params,
             )
@@ -136,7 +131,7 @@
             error_msg = f"Failed to get slow queries: {str(e)}"
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/get_top_cpu_queries.py holmes2/holmes/plugins/toolsets/azure_sql/tools/get_top_cpu_queries.py
--- baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/get_top_cpu_queries.py	2025-11-05 16:43:28.277074937 -0800
+++ holmes2/holmes/plugins/toolsets/azure_sql/tools/get_top_cpu_queries.py	2025-10-17 15:09:28.816950512 -0700
@@ -1,12 +1,7 @@
 import logging
 from typing import Dict, List, Tuple
 
-from holmes.core.tools import (
-    StructuredToolResult,
-    ToolInvokeContext,
-    ToolParameter,
-    StructuredToolResultStatus,
-)
+from holmes.core.tools import StructuredToolResult, ToolParameter, ToolResultStatus
 from holmes.plugins.toolsets.azure_sql.azure_base_toolset import (
     BaseAzureSQLTool,
     BaseAzureSQLToolset,
@@ -102,7 +97,7 @@
 
         return "\n".join(report_sections)
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             top_count = params.get("top_count", 15)
             hours_back = params.get("hours_back", 2)
@@ -126,7 +121,7 @@
             )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=report_text,
                 params=params,
             )
@@ -134,7 +129,7 @@
             error_msg = f"Failed to get top CPU queries: {str(e)}"
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/get_top_data_io_queries.py holmes2/holmes/plugins/toolsets/azure_sql/tools/get_top_data_io_queries.py
--- baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/get_top_data_io_queries.py	2025-11-05 16:43:28.277248687 -0800
+++ holmes2/holmes/plugins/toolsets/azure_sql/tools/get_top_data_io_queries.py	2025-10-17 15:09:28.805513185 -0700
@@ -1,12 +1,7 @@
 import logging
 from typing import Dict, List, Tuple
 
-from holmes.core.tools import (
-    StructuredToolResult,
-    ToolInvokeContext,
-    ToolParameter,
-    StructuredToolResultStatus,
-)
+from holmes.core.tools import StructuredToolResult, ToolParameter, ToolResultStatus
 from holmes.plugins.toolsets.azure_sql.azure_base_toolset import (
     BaseAzureSQLTool,
     BaseAzureSQLToolset,
@@ -120,7 +115,7 @@
 
         return "\n".join(report_sections)
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             top_count = params.get("top_count", 15)
             hours_back = params.get("hours_back", 2)
@@ -144,7 +139,7 @@
             )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=report_text,
                 params=params,
             )
@@ -152,7 +147,7 @@
             error_msg = f"Failed to get top data I/O queries: {str(e)}"
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/get_top_log_io_queries.py holmes2/holmes/plugins/toolsets/azure_sql/tools/get_top_log_io_queries.py
--- baseline-holmes/holmes/plugins/toolsets/azure_sql/tools/get_top_log_io_queries.py	2025-11-05 16:43:28.277436811 -0800
+++ holmes2/holmes/plugins/toolsets/azure_sql/tools/get_top_log_io_queries.py	2025-10-17 15:09:28.807100589 -0700
@@ -1,12 +1,7 @@
 import logging
 from typing import Dict, List, Tuple
 
-from holmes.core.tools import (
-    StructuredToolResult,
-    ToolInvokeContext,
-    ToolParameter,
-    StructuredToolResultStatus,
-)
+from holmes.core.tools import StructuredToolResult, ToolParameter, ToolResultStatus
 from holmes.plugins.toolsets.azure_sql.azure_base_toolset import (
     BaseAzureSQLTool,
     BaseAzureSQLToolset,
@@ -112,7 +107,7 @@
 
         return "\n".join(report_sections)
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             top_count = params.get("top_count", 15)
             hours_back = params.get("hours_back", 2)
@@ -136,7 +131,7 @@
             )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=report_text,
                 params=params,
             )
@@ -144,7 +139,7 @@
             error_msg = f"Failed to get top log I/O queries: {str(e)}"
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
Only in holmes2/holmes/plugins/toolsets/bash: __pycache__
Only in holmes2/holmes/plugins/toolsets/bash/argocd: __pycache__
Only in holmes2/holmes/plugins/toolsets/bash/aws: __pycache__
Only in holmes2/holmes/plugins/toolsets/bash/azure: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/bash/bash_toolset.py holmes2/holmes/plugins/toolsets/bash/bash_toolset.py
--- baseline-holmes/holmes/plugins/toolsets/bash/bash_toolset.py	2025-11-05 16:43:28.279478305 -0800
+++ holmes2/holmes/plugins/toolsets/bash/bash_toolset.py	2025-10-17 15:09:28.736222557 -0700
@@ -9,16 +9,13 @@
 import sentry_sdk
 
 
-from holmes.common.env_vars import (
-    BASH_TOOL_UNSAFE_ALLOW_ALL,
-)
+from holmes.common.env_vars import BASH_TOOL_UNSAFE_ALLOW_ALL
 from holmes.core.tools import (
     CallablePrerequisite,
     StructuredToolResult,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     Toolset,
     ToolsetTag,
 )
@@ -83,7 +80,7 @@
         command_str = get_param_or_raise(params, "command")
         return f"kubectl run {pod_name} --image={image} --namespace={namespace} --rm --attach --restart=Never -i -- {command_str}"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict[str, Any]) -> StructuredToolResult:
         timeout = params.get("timeout", 60)
 
         image = get_param_or_raise(params, "image")
@@ -93,7 +90,7 @@
 
         if namespace and not re.match(SAFE_NAMESPACE_PATTERN, namespace):
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Error: The namespace is invalid. Valid namespaces must match the following regexp: {SAFE_NAMESPACE_PATTERN}",
                 params=params,
             )
@@ -117,7 +114,7 @@
                 }
             )
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=str(e),
                 params=params,
             )
@@ -163,47 +160,42 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict[str, Any]) -> StructuredToolResult:
         command_str = params.get("command")
         timeout = params.get("timeout", 60)
 
         if not command_str:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="The 'command' parameter is required and was not provided.",
                 params=params,
             )
 
         if not isinstance(command_str, str):
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"The 'command' parameter must be a string, got {type(command_str).__name__}.",
                 params=params,
             )
 
         command_to_execute = command_str
+        try:
+            command_to_execute = make_command_safe(command_str, self.toolset.config)
 
-        # Only run the safety check if user has NOT approved the command
-        if not context.user_approved:
-            try:
-                command_to_execute = make_command_safe(command_str, self.toolset.config)
-
-            except (argparse.ArgumentError, ValueError) as e:
-                with sentry_sdk.configure_scope() as scope:
-                    scope.set_extra("command", command_str)
-                    scope.set_extra("error", str(e))
-                    scope.set_extra("unsafe_allow_all", BASH_TOOL_UNSAFE_ALLOW_ALL)
-                    sentry_sdk.capture_exception(e)
-
-                if not BASH_TOOL_UNSAFE_ALLOW_ALL:
-                    logging.info(f"Refusing LLM tool call {command_str}")
-
-                    return StructuredToolResult(
-                        status=StructuredToolResultStatus.APPROVAL_REQUIRED,
-                        error=f"Refusing to execute bash command. {str(e)}",
-                        params=params,
-                        invocation=command_str,
-                    )
+        except (argparse.ArgumentError, ValueError) as e:
+            with sentry_sdk.configure_scope() as scope:
+                scope.set_extra("command", command_str)
+                scope.set_extra("error", str(e))
+                scope.set_extra("unsafe_allow_all", BASH_TOOL_UNSAFE_ALLOW_ALL)
+                sentry_sdk.capture_exception(e)
+
+            if not BASH_TOOL_UNSAFE_ALLOW_ALL:
+                logging.info(f"Refusing LLM tool call {command_str}")
+                return StructuredToolResult(
+                    status=ToolResultStatus.ERROR,
+                    error=f"Refusing to execute bash command. Only some commands are supported and this is likely because requested command is unsupported. Error: {str(e)}",
+                    params=params,
+                )
 
         return execute_bash_command(
             cmd=command_to_execute, timeout=timeout, params=params
Only in holmes2/holmes/plugins/toolsets/bash/common: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/bash/common/bash.py holmes2/holmes/plugins/toolsets/bash/common/bash.py
--- baseline-holmes/holmes/plugins/toolsets/bash/common/bash.py	2025-11-05 16:43:28.279683346 -0800
+++ holmes2/holmes/plugins/toolsets/bash/common/bash.py	2025-10-17 15:09:28.752715637 -0700
@@ -1,5 +1,5 @@
 import subprocess
-from holmes.core.tools import StructuredToolResult, StructuredToolResultStatus
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 
 
 def execute_bash_command(cmd: str, timeout: int, params: dict) -> StructuredToolResult:
@@ -18,11 +18,11 @@
         stdout = process.stdout.strip() if process.stdout else ""
         result_data = f"{cmd}\n" f"{stdout}"
 
-        status = StructuredToolResultStatus.ERROR
+        status = ToolResultStatus.ERROR
         if process.returncode == 0 and stdout:
-            status = StructuredToolResultStatus.SUCCESS
+            status = ToolResultStatus.SUCCESS
         elif not stdout:
-            status = StructuredToolResultStatus.NO_DATA
+            status = ToolResultStatus.NO_DATA
 
         return StructuredToolResult(
             status=status,
@@ -33,20 +33,20 @@
         )
     except subprocess.TimeoutExpired:
         return StructuredToolResult(
-            status=StructuredToolResultStatus.ERROR,
+            status=ToolResultStatus.ERROR,
             error=f"Error: Command '{cmd}' timed out after {timeout} seconds.",
             params=params,
         )
     except FileNotFoundError:
         # This might occur if /bin/bash is not found, or if shell=False and command is not found
         return StructuredToolResult(
-            status=StructuredToolResultStatus.ERROR,
+            status=ToolResultStatus.ERROR,
             error="Error: Bash executable or command not found. Ensure bash is installed and the command is valid.",
             params=params,
         )
     except Exception as e:
         return StructuredToolResult(
-            status=StructuredToolResultStatus.ERROR,
+            status=ToolResultStatus.ERROR,
             error=f"Error executing command '{cmd}': {str(e)}",
             params=params,
         )
Only in holmes2/holmes/plugins/toolsets/bash/docker: __pycache__
Only in holmes2/holmes/plugins/toolsets/bash/helm: __pycache__
Only in holmes2/holmes/plugins/toolsets/bash/kubectl: __pycache__
Only in holmes2/holmes/plugins/toolsets/bash/utilities: __pycache__
Only in holmes2/holmes/plugins/toolsets/bash/utilities/grep: __pycache__
Only in baseline-holmes/holmes/plugins/toolsets: cilium.yaml
diff -ur baseline-holmes/holmes/plugins/toolsets/confluence.yaml holmes2/holmes/plugins/toolsets/confluence.yaml
--- baseline-holmes/holmes/plugins/toolsets/confluence.yaml	2025-11-05 16:43:28.285232163 -0800
+++ holmes2/holmes/plugins/toolsets/confluence.yaml	2025-10-17 15:09:28.832794597 -0700
@@ -1,7 +1,7 @@
 toolsets:
   confluence:
     description: "Fetch confluence pages"
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/confluence/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/confluence.html"
     icon_url: "https://platform.robusta.dev/demos/confluence.svg"
     tags:
       - core
Only in holmes2/holmes/plugins/toolsets/coralogix: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/coralogix/api.py holmes2/holmes/plugins/toolsets/coralogix/api.py
--- baseline-holmes/holmes/plugins/toolsets/coralogix/api.py	2025-11-05 16:43:28.285567537 -0800
+++ holmes2/holmes/plugins/toolsets/coralogix/api.py	2025-10-17 15:09:28.775740792 -0700
@@ -106,9 +106,7 @@
         )
         http_status = response.status_code
         if http_status == 200:
-            logs = parse_logs(
-                raw_logs=response.text.strip(), labels_config=config.labels
-            )
+            logs = parse_logs(raw_logs=response.text.strip())
             return CoralogixQueryResult(logs=logs, http_status=http_status, error=None)
         else:
             return CoralogixQueryResult(
diff -ur baseline-holmes/holmes/plugins/toolsets/coralogix/toolset_coralogix_logs.py holmes2/holmes/plugins/toolsets/coralogix/toolset_coralogix_logs.py
--- baseline-holmes/holmes/plugins/toolsets/coralogix/toolset_coralogix_logs.py	2025-11-05 16:43:28.285668037 -0800
+++ holmes2/holmes/plugins/toolsets/coralogix/toolset_coralogix_logs.py	2025-10-17 15:09:28.776950782 -0700
@@ -3,7 +3,7 @@
 from holmes.core.tools import (
     CallablePrerequisite,
     StructuredToolResult,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     ToolsetTag,
 )
 from holmes.plugins.toolsets.consts import (
@@ -38,7 +38,7 @@
         super().__init__(
             name="coralogix/logs",
             description="Toolset for interacting with Coralogix to fetch logs",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/coralogix-logs/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/coralogix_logs.html",
             icon_url="https://avatars.githubusercontent.com/u/35295744?s=200&v=4",
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
             tools=[],  # Initialize with empty tools first
@@ -74,7 +74,7 @@
     def fetch_pod_logs(self, params: FetchPodLogsParams) -> StructuredToolResult:
         if not self.coralogix_config:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"The {self.name} toolset is not configured",
                 params=params.model_dump(),
             )
@@ -102,9 +102,7 @@
 
         return StructuredToolResult(
             status=(
-                StructuredToolResultStatus.ERROR
-                if logs_data.error
-                else StructuredToolResultStatus.SUCCESS
+                ToolResultStatus.ERROR if logs_data.error else ToolResultStatus.SUCCESS
             ),
             error=logs_data.error,
             data=data,
diff -ur baseline-holmes/holmes/plugins/toolsets/coralogix/utils.py holmes2/holmes/plugins/toolsets/coralogix/utils.py
--- baseline-holmes/holmes/plugins/toolsets/coralogix/utils.py	2025-11-05 16:43:28.285867953 -0800
+++ holmes2/holmes/plugins/toolsets/coralogix/utils.py	2025-10-17 15:09:28.776165538 -0700
@@ -20,10 +20,9 @@
 
 
 class CoralogixLabelsConfig(BaseModel):
-    pod: str = "resource.attributes.k8s.pod.name"
-    namespace: str = "resource.attributes.k8s.namespace.name"
-    log_message: str = "logRecord.body"
-    timestamp: str = "logRecord.attributes.time"
+    pod: str = "kubernetes.pod_name"
+    namespace: str = "kubernetes.namespace_name"
+    log_message: str = "log"
 
 
 class CoralogixLogsMethodology(str, Enum):
@@ -79,43 +78,24 @@
         return date_str
 
 
-def extract_field(data_obj: dict[str, Any], field: str):
-    """returns a nested field from a dict
-    e.g. extract_field({"parent": {"child": "value"}}, "parent.child") => value
-    """
-    current_object: Any = data_obj
-    fields = field.split(".")
-
-    for field in fields:
-        if not current_object:
-            return None
-        if isinstance(current_object, dict):
-            current_object = current_object.get(field)
-        else:
-            return None
-
-    return current_object
-
-
 def flatten_structured_log_entries(
     log_entries: List[Dict[str, Any]],
-    labels_config: CoralogixLabelsConfig,
 ) -> List[FlattenedLog]:
     flattened_logs = []
     for log_entry in log_entries:
         try:
-            userData = json.loads(log_entry.get("userData", "{}"))
-            log_message = extract_field(userData, labels_config.log_message)
-            timestamp = extract_field(userData, labels_config.timestamp)
-            if not log_message or not timestamp:
-                log_message = json.dumps(userData)
-            else:
+            user_data = json.loads(log_entry.get("userData", "{}"))
+            timestamp = normalize_datetime(user_data.get("time"))
+            log_message = user_data.get("log", "")
+            if log_message:
                 flattened_logs.append(
                     FlattenedLog(timestamp=timestamp, log_message=log_message)
                 )  # Store as tuple for sorting
 
         except json.JSONDecodeError:
-            logging.error(f"Failed to decode userData JSON: {json.dumps(log_entry)}")
+            logging.error(
+                f"Failed to decode userData JSON: {log_entry.get('userData')}"
+            )
     return flattened_logs
 
 
@@ -127,16 +107,14 @@
     return "\n".join(formatted_logs) if formatted_logs else "No logs found."
 
 
-def parse_json_objects(
-    json_objects: List[Dict[str, Any]], labels_config: CoralogixLabelsConfig
-) -> List[FlattenedLog]:
+def parse_json_objects(json_objects: List[Dict[str, Any]]) -> List[FlattenedLog]:
     """Extracts timestamp and log values from parsed JSON objects, sorted in ascending order (oldest first)."""
     logs: List[FlattenedLog] = []
 
     for data in json_objects:
         if isinstance(data, dict) and "result" in data and "results" in data["result"]:
             logs += flatten_structured_log_entries(
-                log_entries=data["result"]["results"], labels_config=labels_config
+                log_entries=data["result"]["results"]
             )
         elif isinstance(data, dict) and data.get("warning"):
             logging.info(
@@ -150,18 +128,13 @@
     return logs
 
 
-def parse_logs(
-    raw_logs: str,
-    labels_config: CoralogixLabelsConfig,
-) -> List[FlattenedLog]:
+def parse_logs(raw_logs: str) -> List[FlattenedLog]:
     """Processes the HTTP response and extracts only log outputs."""
     try:
         json_objects = parse_json_lines(raw_logs)
         if not json_objects:
             raise Exception("No valid JSON objects found.")
-        return parse_json_objects(
-            json_objects=json_objects, labels_config=labels_config
-        )
+        return parse_json_objects(json_objects)
     except Exception as e:
         logging.error(
             f"Unexpected error in format_logs for a coralogix API response: {str(e)}"
Only in holmes2/holmes/plugins/toolsets/datadog: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/datadog/datadog_api.py holmes2/holmes/plugins/toolsets/datadog/datadog_api.py
--- baseline-holmes/holmes/plugins/toolsets/datadog/datadog_api.py	2025-11-05 16:43:28.286076494 -0800
+++ holmes2/holmes/plugins/toolsets/datadog/datadog_api.py	2025-10-17 15:09:28.797569288 -0700
@@ -1,9 +1,5 @@
-import json
 import logging
-import re
-from datetime import datetime, timedelta, timezone
-from typing import Any, Optional, Dict, Union, Tuple
-from urllib.parse import urlparse, urlunparse
+from typing import Any, Optional, Dict
 import requests  # type: ignore
 from pydantic import AnyUrl, BaseModel
 from requests.structures import CaseInsensitiveDict  # type: ignore
@@ -19,75 +15,6 @@
 
 RATE_LIMIT_REMAINING_SECONDS_HEADER = "X-RateLimit-Reset"
 
-# Cache for OpenAPI spec
-_openapi_spec_cache: Dict[str, Any] = {}
-
-# Relative time pattern (m = minutes, mo = months)
-RELATIVE_TIME_PATTERN = re.compile(r"^-?(\d+)([hdwsy]|min|m|mo)$|^now$", re.IGNORECASE)
-
-
-def convert_api_url_to_app_url(api_url: Union[str, AnyUrl]) -> str:
-    """
-    Convert a Datadog API URL to the corresponding web app URL.
-
-    Handles various URL formats:
-    - https://api.datadoghq.com -> https://app.datadoghq.com
-    - https://api.datadoghq.eu -> https://app.datadoghq.eu
-    - https://api.us5.datadoghq.com -> https://app.us5.datadoghq.com
-    - Also handles URLs with paths like https://api.datadoghq.com/api/v1
-
-    Args:
-        api_url: The API URL to convert
-
-    Returns:
-        The web app URL without trailing slash
-    """
-    url_str = str(api_url)
-    parsed = urlparse(url_str)
-
-    # Replace 'api.' subdomain with 'app.' in the hostname
-    # This handles cases like:
-    # - api.datadoghq.com -> app.datadoghq.com
-    # - api.datadoghq.eu -> app.datadoghq.eu
-    # - api.us5.datadoghq.com -> app.us5.datadoghq.com
-    if parsed.hostname and parsed.hostname.startswith("api."):
-        new_hostname = "app." + parsed.hostname[4:]
-        # Reconstruct the netloc with the new hostname
-        if parsed.port:
-            new_netloc = f"{new_hostname}:{parsed.port}"
-        else:
-            new_netloc = new_hostname
-    else:
-        # If it doesn't start with 'api.', keep the hostname as is
-        # This handles edge cases where the URL might not follow the pattern
-        new_netloc = parsed.netloc
-
-    # Remove any /api path segments if present
-    # Some configurations might include /api/v1 or similar in the base URL
-    new_path = parsed.path
-    if new_path.startswith("/api/"):
-        new_path = new_path[4:]  # Remove '/api' prefix
-    elif new_path == "/api":
-        new_path = "/"
-
-    # Reconstruct the URL with the app subdomain
-    app_url = urlunparse(
-        (
-            parsed.scheme,
-            new_netloc,
-            new_path,
-            "",  # params
-            "",  # query
-            "",  # fragment
-        )
-    )
-
-    # Remove trailing slash
-    if app_url.endswith("/"):
-        app_url = app_url[:-1]
-
-    return app_url
-
 
 class DatadogBaseConfig(BaseModel):
     """Base configuration for all Datadog toolsets"""
@@ -218,19 +145,6 @@
     return data, cursor
 
 
-def sanitize_headers(headers: Union[dict, CaseInsensitiveDict]) -> dict:
-    try:
-        return {
-            k: v
-            if ("key" not in k.lower() and "key" not in v.lower())
-            else "[REDACTED]"
-            for k, v in headers.items()
-        }
-    except (AttributeError, TypeError):
-        # Return empty dict for mock objects or other non-dict types
-        return {}
-
-
 def execute_datadog_http_request(
     url: str,
     headers: dict,
@@ -238,10 +152,6 @@
     timeout: int,
     method: str = "POST",
 ) -> Any:
-    logging.debug(
-        f"Datadog API Request: Method: {method} URL: {url} Headers: {json.dumps(sanitize_headers(headers), indent=2)} {'Params' if method == 'GET' else 'Payload'}: {json.dumps(payload_or_params, indent=2)} Timeout: {timeout}s"
-    )
-
     if method == "GET":
         response = requests.get(
             url, headers=headers, params=payload_or_params, timeout=timeout
@@ -251,432 +161,13 @@
             url, headers=headers, json=payload_or_params, timeout=timeout
         )
 
-    # Log the response details
-    logging.debug(
-        f"Datadog API Response: Status Code: {response.status_code} Response Headers: {dict(sanitize_headers(response.headers))}"
-    )
-
     if response.status_code == 200:
-        response_data = response.json()
-        return response_data
+        return response.json()
 
     else:
-        logging.error(f"  Error Response Body: {response.text}")
         raise DataDogRequestError(
             payload=payload_or_params,
             status_code=response.status_code,
             response_text=response.text,
             response_headers=response.headers,
         )
-
-
-def fetch_openapi_spec(
-    site_api_url: Optional[str] = None, version: str = "both"
-) -> Optional[Dict[str, Any]]:
-    """Fetch and cache the Datadog OpenAPI specification.
-
-    Args:
-        site_api_url: Base URL for Datadog API (not used, kept for compatibility)
-        version: Which version to fetch ('v1', 'v2', or 'both')
-
-    Returns:
-        OpenAPI spec as dictionary (combined if 'both'), or None if fetch fails
-    """
-    global _openapi_spec_cache
-
-    # Use version as cache key
-    cache_key = f"openapi_{version}"
-
-    # Check cache first
-    if cache_key in _openapi_spec_cache:
-        return _openapi_spec_cache[cache_key]
-
-    try:
-        import yaml
-
-        # GitHub raw URLs for Datadog's official OpenAPI specs
-        spec_urls = {
-            "v1": "https://raw.githubusercontent.com/DataDog/datadog-api-client-python/master/.generator/schemas/v1/openapi.yaml",
-            "v2": "https://raw.githubusercontent.com/DataDog/datadog-api-client-python/master/.generator/schemas/v2/openapi.yaml",
-        }
-
-        combined_spec: Dict[str, Any] = {
-            "openapi": "3.0.0",
-            "paths": {},
-            "components": {},
-        }
-
-        versions_to_fetch = []
-        if version == "both":
-            versions_to_fetch = ["v1", "v2"]
-        elif version in spec_urls:
-            versions_to_fetch = [version]
-        else:
-            logging.error(f"Invalid version: {version}")
-            return None
-
-        for ver in versions_to_fetch:
-            try:
-                logging.debug(f"Fetching Datadog OpenAPI spec for {ver}...")
-                response = requests.get(spec_urls[ver], timeout=30)
-                if response.status_code == 200:
-                    # Parse YAML to dict
-                    spec = yaml.safe_load(response.text)
-
-                    if version == "both":
-                        # Merge specs
-                        if "paths" in spec:
-                            # Prefix v1 paths with /api/v1 and v2 with /api/v2
-                            for path, methods in spec.get("paths", {}).items():
-                                prefixed_path = (
-                                    f"/api/{ver}{path}"
-                                    if not path.startswith("/api/")
-                                    else path
-                                )
-                                paths_dict = combined_spec.get("paths", {})
-                                if isinstance(paths_dict, dict):
-                                    paths_dict[prefixed_path] = methods
-
-                        # Merge components
-                        if "components" in spec:
-                            for comp_type, components in spec.get(
-                                "components", {}
-                            ).items():
-                                components_dict = combined_spec.get("components", {})
-                                if isinstance(components_dict, dict):
-                                    if comp_type not in components_dict:
-                                        components_dict[comp_type] = {}
-                                    components_dict[comp_type].update(components)
-                    else:
-                        combined_spec = spec
-
-                    logging.info(f"Successfully fetched OpenAPI spec for {ver}")
-                else:
-                    logging.warning(
-                        f"Failed to fetch spec for {ver}: HTTP {response.status_code}"
-                    )
-            except Exception as e:
-                logging.error(f"Failed to fetch spec for {ver}: {e}")
-                if version != "both":
-                    return None
-
-        if combined_spec["paths"]:
-            _openapi_spec_cache[cache_key] = combined_spec
-            logging.info(
-                f"Cached OpenAPI spec with {len(combined_spec['paths'])} endpoints"
-            )
-            return combined_spec
-        else:
-            logging.warning("No endpoints found in OpenAPI spec")
-            return None
-
-    except Exception as e:
-        logging.error(f"Error fetching OpenAPI spec: {e}")
-        return None
-
-
-def get_endpoint_requirements(
-    spec: Dict[str, Any], endpoint: str, method: str
-) -> Optional[Dict[str, Any]]:
-    """Extract parameter requirements for a specific endpoint from OpenAPI spec.
-
-    Args:
-        spec: OpenAPI specification
-        endpoint: API endpoint path
-        method: HTTP method
-
-    Returns:
-        Dictionary with parameter requirements, or None if not found
-    """
-    if not spec or "paths" not in spec:
-        return None
-
-    # Normalize endpoint path
-    endpoint = endpoint.strip("/")
-    if not endpoint.startswith("/"):
-        endpoint = "/" + endpoint
-
-    # Find the endpoint in the spec
-    paths = spec.get("paths", {})
-    if endpoint not in paths:
-        # Try to find a matching pattern (e.g., /api/v2/logs/events/search)
-        for path_pattern in paths.keys():
-            if (
-                path_pattern == endpoint
-                or path_pattern.replace("{", "").replace("}", "") in endpoint
-            ):
-                endpoint = path_pattern
-                break
-        else:
-            return None
-
-    # Get method requirements
-    endpoint_spec = paths.get(endpoint, {})
-    method_spec = endpoint_spec.get(method.lower(), {})
-
-    if not method_spec:
-        return None
-
-    requirements = {
-        "description": method_spec.get("description", ""),
-        "parameters": [],
-        "requestBody": None,
-    }
-
-    # Extract parameters
-    for param in method_spec.get("parameters", []):
-        param_info = {
-            "name": param.get("name"),
-            "in": param.get("in"),  # query, path, header
-            "required": param.get("required", False),
-            "description": param.get("description", ""),
-            "schema": param.get("schema", {}),
-        }
-        requirements["parameters"].append(param_info)
-
-    # Extract request body schema
-    if "requestBody" in method_spec:
-        body = method_spec["requestBody"]
-        content = body.get("content", {})
-        json_content = content.get("application/json", {})
-        requirements["requestBody"] = {
-            "required": body.get("required", False),
-            "schema": json_content.get("schema", {}),
-            "description": body.get("description", ""),
-        }
-
-    return requirements
-
-
-def convert_relative_time(time_str: str) -> Tuple[str, str]:
-    """Convert relative time strings to RFC3339 format.
-
-    Args:
-        time_str: Time string (e.g., '-24h', 'now', '-7d', '2024-01-01T00:00:00Z')
-
-    Returns:
-        Tuple of (converted_time, format_type) where format_type is 'relative', 'rfc3339', or 'unix'
-    """
-    # Check if already in RFC3339 format
-    try:
-        # Try parsing as RFC3339
-        if "T" in time_str and (
-            time_str.endswith("Z") or "+" in time_str or "-" in time_str[-6:]
-        ):
-            datetime.fromisoformat(time_str.replace("Z", "+00:00"))
-            return time_str, "rfc3339"
-    except (ValueError, AttributeError):
-        pass
-
-    # Check if Unix timestamp
-    try:
-        timestamp = float(time_str)
-        if 1000000000 < timestamp < 2000000000:  # Reasonable Unix timestamp range
-            return time_str, "unix"
-    except (ValueError, TypeError):
-        pass
-
-    # Check for relative time
-    match = RELATIVE_TIME_PATTERN.match(time_str.strip())
-    if not match:
-        # Return as-is if not recognized
-        return time_str, "unknown"
-
-    now = datetime.now(timezone.utc)
-
-    if time_str.lower() == "now":
-        return now.isoformat().replace("+00:00", "Z"), "relative"
-
-    # Parse relative time
-    groups = match.groups()
-    if groups[0] is None:
-        return time_str, "unknown"
-
-    amount = int(groups[0])
-    unit = groups[1].lower()
-
-    # Convert to timedelta
-    if unit == "s":
-        delta = timedelta(seconds=amount)
-    elif unit == "min":
-        delta = timedelta(minutes=amount)
-    elif unit == "m":
-        delta = timedelta(minutes=amount)  # m = minutes
-    elif unit == "h":
-        delta = timedelta(hours=amount)
-    elif unit == "d":
-        delta = timedelta(days=amount)
-    elif unit == "w":
-        delta = timedelta(weeks=amount)
-    elif unit == "mo":
-        delta = timedelta(days=amount * 30)  # mo = months (approximate)
-    elif unit == "y":
-        delta = timedelta(days=amount * 365)  # Approximate
-    else:
-        return time_str, "unknown"
-
-    # Apply delta (subtract if negative relative time)
-    if time_str.startswith("-"):
-        result_time = now - delta
-    else:
-        result_time = now + delta
-
-    return result_time.isoformat().replace("+00:00", "Z"), "relative"
-
-
-def preprocess_time_fields(payload: Dict[str, Any], endpoint: str) -> Dict[str, Any]:
-    """Preprocess time fields in payload, converting relative times to appropriate format.
-
-    Args:
-        payload: Request payload
-        endpoint: API endpoint
-
-    Returns:
-        Modified payload with converted time fields
-    """
-    # Deep copy to avoid modifying original
-    import copy
-
-    processed = copy.deepcopy(payload)
-
-    # Common time field paths to check
-    time_fields = [
-        ["filter", "from"],
-        ["filter", "to"],
-        ["from"],
-        ["to"],
-        ["start"],
-        ["end"],
-        ["start_time"],
-        ["end_time"],
-    ]
-
-    def get_nested(d, path):
-        """Get nested dictionary value."""
-        for key in path:
-            if isinstance(d, dict) and key in d:
-                d = d[key]
-            else:
-                return None
-        return d
-
-    def set_nested(d, path, value):
-        """Set nested dictionary value."""
-        for key in path[:-1]:
-            if key not in d:
-                d[key] = {}
-            d = d[key]
-        d[path[-1]] = value
-
-    conversions = []
-
-    for field_path in time_fields:
-        value = get_nested(processed, field_path)
-        if value and isinstance(value, str):
-            converted, format_type = convert_relative_time(value)
-            if format_type == "relative":
-                set_nested(processed, field_path, converted)
-                conversions.append(
-                    f"{'.'.join(field_path)}: '{value}' -> '{converted}'"
-                )
-
-    if conversions:
-        logging.info(f"Converted relative time fields: {', '.join(conversions)}")
-
-    return processed
-
-
-def enhance_error_message(
-    error: DataDogRequestError, endpoint: str, method: str, site_api_url: str
-) -> str:
-    """Enhance error message with OpenAPI spec details and format examples.
-
-    Args:
-        error: Original DataDog request error
-        endpoint: API endpoint
-        method: HTTP method
-        site_api_url: Base API URL
-
-    Returns:
-        Enhanced error message
-    """
-    base_msg = f"HTTP error: {error.status_code} - {error.response_text}"
-
-    # For 400 errors, try to provide more context
-    if error.status_code == 400:
-        enhanced_parts = [base_msg]
-
-        # Try to parse error details
-        try:
-            error_body = json.loads(error.response_text)
-            if "errors" in error_body:
-                enhanced_parts.append(f"\nErrors: {error_body['errors']}")
-
-                # Check for specific field validation errors
-                for err in error_body.get("errors", []):
-                    if "input_validation_error" in str(err):
-                        enhanced_parts.append("\n Input validation error detected.")
-
-                        # Add time format help
-                        if any(
-                            field in str(err).lower()
-                            for field in ["from", "to", "time", "date"]
-                        ):
-                            enhanced_parts.append(
-                                "\nTime format requirements:\n"
-                                "  - v1 API: Unix timestamps (e.g., 1704067200)\n"
-                                "  - v2 API: RFC3339 format (e.g., '2024-01-01T00:00:00Z')\n"
-                                "  - NOT supported: Relative times like '-24h', 'now', '-7d'"
-                            )
-        except (json.JSONDecodeError, TypeError):
-            pass
-
-        # Try to fetch OpenAPI spec for more details
-        spec = fetch_openapi_spec(version="both")
-        if spec:
-            requirements = get_endpoint_requirements(spec, endpoint, method)
-            if requirements:
-                enhanced_parts.append(f"\nEndpoint: {method} {endpoint}")
-                if requirements["description"]:
-                    enhanced_parts.append(f"Description: {requirements['description']}")
-
-                # Add parameter requirements
-                if requirements["parameters"]:
-                    enhanced_parts.append("\nRequired parameters:")
-                    for param in requirements["parameters"]:
-                        if param["required"]:
-                            enhanced_parts.append(
-                                f"  - {param['name']} ({param['in']}): {param['description']}"
-                            )
-
-                # Add request body schema hints
-                if (
-                    requirements["requestBody"]
-                    and requirements["requestBody"]["required"]
-                ):
-                    enhanced_parts.append("\nRequest body is required")
-                    if requirements["requestBody"]["description"]:
-                        enhanced_parts.append(
-                            f"Body: {requirements['requestBody']['description']}"
-                        )
-
-        # Add example for common endpoints
-        if "/logs/events/search" in endpoint:
-            enhanced_parts.append(
-                "\nExample request body for logs search:\n"
-                "```json\n"
-                "{\n"
-                '  "filter": {\n'
-                '    "from": "2024-01-01T00:00:00Z",\n'
-                '    "to": "2024-01-02T00:00:00Z",\n'
-                '    "query": "*"\n'
-                "  },\n"
-                '  "sort": "-timestamp",\n'
-                '  "page": {"limit": 50}\n'
-                "}\n"
-                "```"
-            )
-
-        return "\n".join(enhanced_parts)
-
-    return base_msg
Only in baseline-holmes/holmes/plugins/toolsets/datadog: datadog_general_instructions.jinja2
diff -ur baseline-holmes/holmes/plugins/toolsets/datadog/datadog_logs_instructions.jinja2 holmes2/holmes/plugins/toolsets/datadog/datadog_logs_instructions.jinja2
--- baseline-holmes/holmes/plugins/toolsets/datadog/datadog_logs_instructions.jinja2	2025-11-05 16:43:28.286360535 -0800
+++ holmes2/holmes/plugins/toolsets/datadog/datadog_logs_instructions.jinja2	2025-10-17 15:09:28.791230879 -0700
@@ -14,32 +14,21 @@
 
 ### CRITICAL: Pod Name Resolution Workflow
 
-**IMPORTANT WILDCARD USAGE:**
-- **ALWAYS use wildcards** when searching for pods unless you have the COMPLETE pod name with all suffixes
-- Kubernetes pod names include deployment hash + replica ID (e.g., `nginx-ingress-7b9899-x2km9`, `frontend-5f4d3b2a1-abc123`)
-- When user says "nginx pod" or "frontend pod", search for `nginx-*` or `frontend-*` NOT just `nginx` or `frontend`
-- Datadog supports wildcards: `*` matches any characters (e.g., `nginx-*`, `*ingress*`, `*-x2km9`)
-- For partial matches, use wildcards on both sides: `*keyword*` to find logs from any pod containing "keyword"
-
-**When user provides what looks like a complete pod name** (e.g., `my-workload-5f9d8b7c4d-x2km9`):
-- Query Datadog directly with that exact pod name
+**When user provides an exact pod name** (e.g., `my-workload-5f9d8b7c4d-x2km9`):
+- FIRST query Datadog directly with that pod name using appropriate tags
 - Do NOT try to verify if the pod exists in Kubernetes first
 - This allows querying historical pods that have been deleted/replaced
 
-**When user provides a simple/generic name** (e.g., "nginx", "redis", "payment-service", "auth"):
-- **DEFAULT ACTION: Use wildcards** - Query with `pod-name-*` pattern
-- For historical queries (yesterday, last week): ALWAYS use wildcards directly in Datadog
-- For current issues: Optionally use `kubectl_find_resource` to find exact pod names, but wildcards often work better
-- Examples:
-  - User says "nginx pod"  Query Datadog with `nginx-*`
-  - User says "redis instance"  Query Datadog with `redis-*`
-  - User says "payment service"  Query Datadog with `payment-*`
+**When user provides a generic workload name** (e.g., "my-workload", "nginx", "telemetry-processor"):
+- First use `kubectl_find_resource` to find actual pod names
+- Example: `kubectl_find_resource` with "my-workload"  finds pods like "my-workload-8f8cdfxyz-c7zdr"
+- Then use those specific pod names in Datadog queries
+- Alternative: Use deployment-level tags when appropriate
 
-**Why wildcards are critical:**
+**Why this matters:**
 - Pod names in Datadog are the actual Kubernetes pod names (with random suffixes)
-- Users typically refer to pods by their deployment/service name without suffixes
-- Without wildcards, queries for "nginx" will find NOTHING when actual pods are named "nginx-7b9899-x2km9"
-- Historical pods that no longer exist can only be found via Datadog with proper wildcard usage
+- Historical pods that no longer exist in the cluster can still have logs in Datadog
+- Deployment/service names alone are NOT pod names (they need the suffix)
 
 ### Time Parameters
 - Use RFC3339 format: `2023-03-01T10:30:00Z`
Only in baseline-holmes/holmes/plugins/toolsets/datadog: toolset_datadog_general.py
diff -ur baseline-holmes/holmes/plugins/toolsets/datadog/toolset_datadog_logs.py holmes2/holmes/plugins/toolsets/datadog/toolset_datadog_logs.py
--- baseline-holmes/holmes/plugins/toolsets/datadog/toolset_datadog_logs.py	2025-11-05 16:43:28.287367074 -0800
+++ holmes2/holmes/plugins/toolsets/datadog/toolset_datadog_logs.py	2025-10-17 15:09:28.793218697 -0700
@@ -3,13 +3,12 @@
 import json
 import logging
 from typing import Any, Optional, Dict, Tuple, Set
-from urllib.parse import urlencode
 from holmes.core.tools import (
     CallablePrerequisite,
     ToolsetTag,
 )
 from pydantic import BaseModel, Field
-from holmes.core.tools import StructuredToolResult, StructuredToolResultStatus
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 from holmes.plugins.toolsets.consts import TOOLSET_CONFIG_MISSING_ERROR
 from holmes.plugins.toolsets.datadog.datadog_api import (
     DatadogBaseConfig,
@@ -17,8 +16,6 @@
     execute_paginated_datadog_http_request,
     get_headers,
     MAX_RETRY_COUNT_ON_RATE_LIMIT,
-    enhance_error_message,
-    preprocess_time_fields,
 )
 from holmes.plugins.toolsets.logging_utils.logging_api import (
     DEFAULT_TIME_SPAN_SECONDS,
@@ -102,28 +99,23 @@
         "page": {"limit": calculate_page_size(params, dd_config, [])},
     }
 
-    # Preprocess time fields to ensure correct format
-    processed_payload = preprocess_time_fields(payload, "/api/v2/logs/events/search")
-
     logs, cursor = execute_paginated_datadog_http_request(
         url=url,
         headers=headers,
-        payload_or_params=processed_payload,
+        payload_or_params=payload,
         timeout=dd_config.request_timeout,
     )
 
     while cursor and len(logs) < limit:
-        processed_payload["page"]["cursor"] = cursor
-        processed_payload["page"]["limit"] = calculate_page_size(
-            params, dd_config, logs
-        )
+        payload["page"]["cursor"] = cursor
         new_logs, cursor = execute_paginated_datadog_http_request(
             url=url,
             headers=headers,
-            payload_or_params=processed_payload,
+            payload_or_params=payload,
             timeout=dd_config.request_timeout,
         )
         logs += new_logs
+        payload["page"]["limit"] = calculate_page_size(params, dd_config, logs)
 
     # logs are fetched descending order. Unified logging API follows the pattern of kubectl logs where oldest logs are first
     logs.reverse()
@@ -137,73 +129,14 @@
     logs = []
 
     for raw_log_item in raw_logs:
-        # Extract timestamp - Datadog returns it in ISO format
-        timestamp = raw_log_item.get("attributes", {}).get("timestamp", "")
-        if not timestamp:
-            # Fallback to @timestamp if timestamp is not in attributes
-            timestamp = raw_log_item.get("attributes", {}).get("@timestamp", "")
-
-        # Extract message
         message = raw_log_item.get("attributes", {}).get(
             "message", json.dumps(raw_log_item)
         )
-
-        # Format as: [timestamp] message
-        if timestamp:
-            logs.append(f"[{timestamp}] {message}")
-        else:
-            logs.append(message)
+        logs.append(message)
 
     return "\n".join(logs)
 
 
-def generate_datadog_logs_url(
-    dd_config: DatadogLogsConfig,
-    params: FetchPodLogsParams,
-    storage_tier: DataDogStorageTier,
-) -> str:
-    """Generate a Datadog web UI URL for the logs query."""
-    from holmes.plugins.toolsets.utils import process_timestamps_to_int
-    from holmes.plugins.toolsets.datadog.datadog_api import convert_api_url_to_app_url
-
-    # Convert API URL to app URL using the shared helper
-    base_url = convert_api_url_to_app_url(dd_config.site_api_url)
-
-    # Build the query string
-    query = f"{dd_config.labels.namespace}:{params.namespace}"
-    query += f" {dd_config.labels.pod}:{params.pod_name}"
-    if params.filter:
-        filter = params.filter.replace('"', '\\"')
-        query += f' "{filter}"'
-
-    # Process timestamps - get Unix timestamps in seconds
-    (from_time_seconds, to_time_seconds) = process_timestamps_to_int(
-        start=params.start_time,
-        end=params.end_time,
-        default_time_span_seconds=DEFAULT_TIME_SPAN_SECONDS,
-    )
-
-    # Convert to milliseconds for Datadog web UI
-    from_time_ms = from_time_seconds * 1000
-    to_time_ms = to_time_seconds * 1000
-
-    # Build URL parameters matching Datadog's web UI format
-    url_params = {
-        "query": query,
-        "from_ts": str(from_time_ms),
-        "to_ts": str(to_time_ms),
-        "live": "true",
-        "storage": storage_tier.value,
-    }
-
-    # Add indexes if not default
-    if dd_config.indexes != ["*"]:
-        url_params["index"] = ",".join(dd_config.indexes)
-
-    # Construct the full URL
-    return f"{base_url}/logs?{urlencode(url_params)}"
-
-
 class DatadogLogsToolset(BasePodLoggingToolset):
     dd_config: Optional[DatadogLogsConfig] = None
 
@@ -218,10 +151,11 @@
         super().__init__(
             name="datadog/logs",
             description="Toolset for fetching logs from Datadog, including historical data for pods no longer in the cluster",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/datadog/",
+            docs_url="https://docs.datadoghq.com/api/latest/logs/",
             icon_url="https://imgix.datadoghq.com//img/about/presskit/DDlogo.jpg",
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
             tools=[],  # Initialize with empty tools first
+            experimental=True,
             tags=[ToolsetTag.CORE],
         )
         # Now that parent is initialized and self.name exists, create the tool
@@ -234,7 +168,7 @@
     def fetch_pod_logs(self, params: FetchPodLogsParams) -> StructuredToolResult:
         if not self.dd_config:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 data=TOOLSET_CONFIG_MISSING_ERROR,
                 params=params.model_dump(),
             )
@@ -248,134 +182,29 @@
 
                 if raw_logs:
                     logs_str = format_logs(raw_logs)
-                    # Generate Datadog web UI URL
-                    datadog_url = generate_datadog_logs_url(
-                        self.dd_config, params, storage_tier
-                    )
-                    logs_with_link = f"{logs_str}\n\nView in Datadog: {datadog_url}"
                     return StructuredToolResult(
-                        status=StructuredToolResultStatus.SUCCESS,
-                        data=logs_with_link,
-                        url=datadog_url,
+                        status=ToolResultStatus.SUCCESS,
+                        data=logs_str,
                         params=params.model_dump(),
                     )
 
-            # Include detailed diagnostic context
-            query = f"{self.dd_config.labels.namespace}:{params.namespace} {self.dd_config.labels.pod}:{params.pod_name}"
-            if params.filter:
-                query += f' "{params.filter}"'
-
-            # Get actual time range used
-            (from_time, to_time) = process_timestamps_to_rfc3339(
-                start_timestamp=params.start_time,
-                end_timestamp=params.end_time,
-                default_time_span_seconds=DEFAULT_TIME_SPAN_SECONDS,
-            )
-
-            # Generate Datadog web UI URL for the last storage tier checked
-            datadog_url = generate_datadog_logs_url(
-                self.dd_config, params, self.dd_config.storage_tiers[-1]
-            )
-
-            # Build diagnostic information
-            diagnostics: Dict[str, Any] = {
-                "query_executed": query,
-                "time_range": f"{from_time} to {to_time}",
-                "indexes_searched": self.dd_config.indexes,
-                "storage_tiers_checked": [
-                    tier.value for tier in self.dd_config.storage_tiers
-                ],
-                "field_mappings": {
-                    "namespace_field": self.dd_config.labels.namespace,
-                    "pod_field": self.dd_config.labels.pod,
-                },
-                "limit": params.limit or self.dd_config.default_limit,
-                "datadog_url": datadog_url,
-            }
-
-            # Format diagnostic info as structured text
-            error_msg = (
-                f"No logs found.\n\n"
-                f"Diagnostic Information:\n"
-                f"----------------------\n"
-                f"Query executed: {diagnostics['query_executed']}\n"
-                f"Time range: {diagnostics['time_range']}\n"
-                f"Indexes searched: {diagnostics['indexes_searched']}\n"
-                f"Storage tiers checked: {', '.join(str(tier) for tier in diagnostics.get('storage_tiers_checked', []))}\n"
-                f"Field mappings:\n"
-                f"  - Namespace field: {diagnostics.get('field_mappings', {}).get('namespace_field', 'N/A')}\n"
-                f"  - Pod field: {diagnostics.get('field_mappings', {}).get('pod_field', 'N/A')}\n"
-                f"Limit: {diagnostics['limit']}\n\n"
-                f"View in Datadog: {diagnostics['datadog_url']}"
-            )
-
             return StructuredToolResult(
-                status=StructuredToolResultStatus.NO_DATA,
-                error=error_msg,
-                url=datadog_url,
+                status=ToolResultStatus.NO_DATA,
                 params=params.model_dump(),
             )
 
         except DataDogRequestError as e:
             logging.exception(e, exc_info=True)
 
-            # Always try to generate Datadog URL for debugging
-            try:
-                datadog_url = generate_datadog_logs_url(
-                    self.dd_config, params, self.dd_config.storage_tiers[0]
-                )
-            except Exception:
-                datadog_url = None
-
             # Provide more specific error message for rate limiting failures
             if e.status_code == 429:
                 error_msg = f"Datadog API rate limit exceeded. Failed after {MAX_RETRY_COUNT_ON_RATE_LIMIT} retry attempts."
-                if datadog_url:
-                    error_msg += f"\nView in Datadog: {datadog_url}"
-            elif e.status_code == 400:
-                # Use enhanced error message for validation errors
-                error_msg = enhance_error_message(
-                    e,
-                    "/api/v2/logs/events/search",
-                    "POST",
-                    str(self.dd_config.site_api_url),
-                )
-
-                # Add query context
-                query = f"{self.dd_config.labels.namespace}:{params.namespace} {self.dd_config.labels.pod}:{params.pod_name}"
-                if params.filter:
-                    query += f' "{params.filter}"'
-                error_msg += f"\n\nQuery attempted: {query}"
-
-                # Add Datadog web UI URL to error message
-                if datadog_url:
-                    error_msg += f"\nView in Datadog: {datadog_url}"
             else:
-                # Include full API error details and query context
-                error_msg = (
-                    f"Datadog API error (status {e.status_code}): {e.response_text}"
-                )
-                query = f"{self.dd_config.labels.namespace}:{params.namespace} {self.dd_config.labels.pod}:{params.pod_name}"
-                if params.filter:
-                    query += f' "{params.filter}"'
-                error_msg += f"\nQuery: {query}"
-
-                # Get actual time range used
-                (from_time, to_time) = process_timestamps_to_rfc3339(
-                    start_timestamp=params.start_time,
-                    end_timestamp=params.end_time,
-                    default_time_span_seconds=DEFAULT_TIME_SPAN_SECONDS,
-                )
-                error_msg += f"\nTime range: {from_time} to {to_time}"
-
-                # Add Datadog web UI URL to error message
-                if datadog_url:
-                    error_msg += f"\nView in Datadog: {datadog_url}"
+                error_msg = f"Exception while querying Datadog: {str(e)}"
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
-                url=datadog_url,
                 params=params.model_dump(),
                 invocation=json.dumps(e.payload),
             )
@@ -385,7 +214,7 @@
                 f"Failed to query Datadog logs for params: {params}", exc_info=True
             )
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Exception while querying Datadog: {str(e)}",
                 params=params.model_dump(),
             )
@@ -396,7 +225,7 @@
         Returns (success, error_message).
         """
         try:
-            logging.debug("Performing Datadog configuration healthcheck...")
+            logging.info("Performing Datadog configuration healthcheck...")
             healthcheck_params = FetchPodLogsParams(
                 namespace="*",
                 pod_name="*",
@@ -406,11 +235,11 @@
 
             result = self.fetch_pod_logs(healthcheck_params)
 
-            if result.status == StructuredToolResultStatus.ERROR:
+            if result.status == ToolResultStatus.ERROR:
                 error_msg = result.error or "Unknown error during healthcheck"
                 logging.error(f"Datadog healthcheck failed: {error_msg}")
                 return False, f"Datadog healthcheck failed: {error_msg}"
-            elif result.status == StructuredToolResultStatus.NO_DATA:
+            elif result.status == ToolResultStatus.NO_DATA:
                 error_msg = "No logs were found in the last 48 hours using wildcards for pod and namespace. Is the configuration correct?"
                 logging.error(f"Datadog healthcheck failed: {error_msg}")
                 return False, f"Datadog healthcheck failed: {error_msg}"
@@ -426,7 +255,7 @@
         if not config:
             return (
                 False,
-                "Missing config for dd_api_key, dd_app_key, or site_api_url. For details: https://holmesgpt.dev/data-sources/builtin-toolsets/datadog/",
+                TOOLSET_CONFIG_MISSING_ERROR,
             )
 
         try:
diff -ur baseline-holmes/holmes/plugins/toolsets/datadog/toolset_datadog_metrics.py holmes2/holmes/plugins/toolsets/datadog/toolset_datadog_metrics.py
--- baseline-holmes/holmes/plugins/toolsets/datadog/toolset_datadog_metrics.py	2025-11-05 16:43:28.287499407 -0800
+++ holmes2/holmes/plugins/toolsets/datadog/toolset_datadog_metrics.py	2025-10-17 15:09:28.792878367 -0700
@@ -6,9 +6,8 @@
     CallablePrerequisite,
     StructuredToolResult,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     Toolset,
     ToolsetTag,
 )
@@ -55,7 +54,7 @@
     def __init__(self, toolset: "DatadogMetricsToolset"):
         super().__init__(
             name="list_active_datadog_metrics",
-            description=f"[datadog/metrics toolset] List active metrics from Datadog for the last {ACTIVE_METRICS_DEFAULT_LOOK_BACK_HOURS} hours. This includes metrics that have actively reported data points, including from pods no longer in the cluster.",
+            description=f"List active metrics from Datadog for the last {ACTIVE_METRICS_DEFAULT_LOOK_BACK_HOURS} hours. This includes metrics that have actively reported data points, including from pods no longer in the cluster.",
             parameters={
                 "from_time": ToolParameter(
                     description=f"Start time for listing metrics. Can be an RFC3339 formatted datetime (e.g. '2023-03-01T10:30:00Z') or a negative integer for relative seconds from now (e.g. -86400 for 24 hours ago). Defaults to {ACTIVE_METRICS_DEFAULT_LOOK_BACK_HOURS} hours ago",
@@ -76,10 +75,10 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         if not self.toolset.dd_config:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=TOOLSET_CONFIG_MISSING_ERROR,
                 params=params,
             )
@@ -120,7 +119,7 @@
             metrics = data.get("metrics", [])
             if not metrics:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     data="Your filter returned no metrics. Change your filter and try again",
                     params=params,
                 )
@@ -132,7 +131,7 @@
                 output.append(metric)
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data="\n".join(output),
                 params=params,
             )
@@ -148,30 +147,10 @@
                     f"and 'timeseries_query' permissions. Error: {str(e)}"
                 )
             else:
-                # Include full API error details for better debugging
-                error_msg = (
-                    f"Datadog API error (status {e.status_code}): {e.response_text}"
-                )
-                if params:
-                    # ListActiveMetrics parameters: from_time, host, tag_filter
-                    if params.get("host"):
-                        error_msg += f"\nHost filter: {params.get('host')}"
-                    if params.get("tag_filter"):
-                        error_msg += f"\nTag filter: {params.get('tag_filter')}"
-
-                    from_time_param = params.get("from_time")
-                    if from_time_param:
-                        time_desc = from_time_param
-                    else:
-                        time_desc = f"default (last {ACTIVE_METRICS_DEFAULT_LOOK_BACK_HOURS} hours)"
-                    error_msg += f"\nTime range: {time_desc}"
-
-                    # Note: We cannot generate a Datadog Metrics Explorer URL for ListActiveMetrics
-                    # because the Metrics Explorer requires a specific metric query,
-                    # while ListActiveMetrics just lists available metrics without querying any specific one
+                error_msg = f"Exception while querying Datadog: {str(e)}"
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
                 invocation=json.dumps({"url": url, "params": query_params})
@@ -184,7 +163,7 @@
                 f"Failed to query Datadog metrics for params: {params}", exc_info=True
             )
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Exception while querying Datadog: {str(e)}",
                 params=params,
             )
@@ -203,7 +182,7 @@
     def __init__(self, toolset: "DatadogMetricsToolset"):
         super().__init__(
             name="query_datadog_metrics",
-            description="[datadog/metrics toolset] Query timeseries data from Datadog for a specific metric, including historical data for pods no longer in the cluster",
+            description="Query timeseries data from Datadog for a specific metric, including historical data for pods no longer in the cluster",
             parameters={
                 "query": ToolParameter(
                     description="The metric query string (e.g., 'system.cpu.user{host:myhost}')",
@@ -236,10 +215,10 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         if not self.toolset.dd_config:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=TOOLSET_CONFIG_MISSING_ERROR,
                 params=params,
             )
@@ -278,29 +257,9 @@
             output_type = params.get("output_type", "Plain")
 
             if not series:
-                # Include detailed context in error message
-                from_time_param = params.get("from_time")
-                to_time_param = params.get("to_time")
-
-                if from_time_param:
-                    from_desc = from_time_param
-                else:
-                    from_desc = (
-                        f"default (last {DEFAULT_TIME_SPAN_SECONDS // 86400} days)"
-                    )
-
-                to_desc = to_time_param or "now"
-
-                error_msg = (
-                    f"The query returned no data.\n"
-                    f"Query: {params.get('query', 'not specified')}\n"
-                    f"Time range: {from_desc} to {to_desc}\n"
-                    f"Please check your query syntax and ensure data exists for this time range."
-                )
-
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.NO_DATA,
-                    error=error_msg,
+                    status=ToolResultStatus.NO_DATA,
+                    error="The query returned no data. Please check your query syntax and time range.",
                     params=params,
                 )
 
@@ -354,7 +313,7 @@
 
             data_str = json.dumps(response_data, indent=2)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=data_str,
                 params=params,
             )
@@ -370,28 +329,10 @@
                     f"and 'timeseries_query' permissions. Error: {str(e)}"
                 )
             else:
-                # Include full API error details for better debugging
-                error_msg = (
-                    f"Datadog API error (status {e.status_code}): {e.response_text}"
-                )
-                if params:
-                    error_msg += f"\nQuery: {params.get('query', 'not specified')}"
-
-                    from_time_param = params.get("from_time")
-                    to_time_param = params.get("to_time")
-
-                    if from_time_param:
-                        from_desc = from_time_param
-                    else:
-                        from_desc = (
-                            f"default (last {DEFAULT_TIME_SPAN_SECONDS // 86400} days)"
-                        )
-
-                    to_desc = to_time_param or "now"
-                    error_msg += f"\nTime range: {from_desc} to {to_desc}"
+                error_msg = f"Exception while querying Datadog: {str(e)}"
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
                 invocation=json.dumps({"url": url, "params": query_params})
@@ -405,7 +346,7 @@
             )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Exception while querying Datadog: {str(e)}",
                 params=params,
             )
@@ -419,7 +360,7 @@
     def __init__(self, toolset: "DatadogMetricsToolset"):
         super().__init__(
             name="get_datadog_metric_metadata",
-            description="[datadog/metrics toolset] Get metadata about one or more metrics including their type, description, unit, and other properties",
+            description="Get metadata about one or more metrics including their type, description, unit, and other properties",
             parameters={
                 "metric_names": ToolParameter(
                     description="Comma-separated list of metric names to get metadata for (e.g., 'system.cpu.user, system.mem.used')",
@@ -430,10 +371,10 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         if not self.toolset.dd_config:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=TOOLSET_CONFIG_MISSING_ERROR,
                 params=params,
             )
@@ -449,7 +390,7 @@
 
             if not metric_names:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error="metric_names cannot be empty",
                     params=params,
                 )
@@ -495,14 +436,14 @@
 
             if not results and errors:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error="Failed to retrieve metadata for all metrics",
                     data=json.dumps(response_data, indent=2),
                     params=params,
                 )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=json.dumps(response_data, indent=2),
                 params=params,
             )
@@ -514,7 +455,7 @@
             )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Exception while querying Datadog: {str(e)}",
                 params=params,
             )
@@ -533,7 +474,7 @@
     def __init__(self, toolset: "DatadogMetricsToolset"):
         super().__init__(
             name="list_datadog_metric_tags",
-            description="[datadog/metrics toolset] List all available tags and aggregations for a specific metric. This helps in building queries by showing what dimensions are available for filtering.",
+            description="List all available tags and aggregations for a specific metric. This helps in building queries by showing what dimensions are available for filtering.",
             parameters={
                 "metric_name": ToolParameter(
                     description="The name of the metric to get tags for (e.g., 'system.cpu.user', 'container.memory.usage')",
@@ -544,10 +485,10 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         if not self.toolset.dd_config:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=TOOLSET_CONFIG_MISSING_ERROR,
                 params=params,
             )
@@ -570,7 +511,7 @@
             )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=data,
                 params=params,
             )
@@ -588,17 +529,10 @@
                     f"permissions. Error: {str(e)}"
                 )
             else:
-                # Include full API error details for better debugging
-                error_msg = (
-                    f"Datadog API error (status {e.status_code}): {e.response_text}"
-                )
-                if params:
-                    error_msg += (
-                        f"\nMetric name: {params.get('metric_name', 'not specified')}"
-                    )
+                error_msg = f"Exception while querying Datadog: {str(e)}"
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
                 invocation=json.dumps({"url": url, "params": query_params})
@@ -612,7 +546,7 @@
                 exc_info=True,
             )
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Exception while querying Datadog: {str(e)}",
                 params=params,
             )
@@ -629,7 +563,7 @@
         super().__init__(
             name="datadog/metrics",
             description="Toolset for fetching metrics and metadata from Datadog, including historical data for pods no longer in the cluster",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/datadog/",
+            docs_url="https://docs.datadoghq.com/api/latest/metrics/",
             icon_url="https://imgix.datadoghq.com//img/about/presskit/DDlogo.jpg",
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
             tools=[
@@ -638,13 +572,14 @@
                 QueryMetricsMetadata(toolset=self),
                 ListMetricTags(toolset=self),
             ],
+            experimental=True,
             tags=[ToolsetTag.CORE],
         )
         self._reload_instructions()
 
     def _perform_healthcheck(self, dd_config: DatadogMetricsConfig) -> Tuple[bool, str]:
         try:
-            logging.debug("Performing Datadog metrics configuration healthcheck...")
+            logging.info("Performing Datadog metrics configuration healthcheck...")
 
             url = f"{dd_config.site_api_url}/api/v1/validate"
             headers = get_headers(dd_config)
@@ -673,7 +608,7 @@
         if not config:
             return (
                 False,
-                "Missing config for dd_api_key, dd_app_key, or site_api_url. For details: https://holmesgpt.dev/data-sources/builtin-toolsets/datadog/",
+                TOOLSET_CONFIG_MISSING_ERROR,
             )
 
         try:
diff -ur baseline-holmes/holmes/plugins/toolsets/datadog/toolset_datadog_rds.py holmes2/holmes/plugins/toolsets/datadog/toolset_datadog_rds.py
--- baseline-holmes/holmes/plugins/toolsets/datadog/toolset_datadog_rds.py	2025-11-05 16:43:28.287639698 -0800
+++ holmes2/holmes/plugins/toolsets/datadog/toolset_datadog_rds.py	2025-10-17 15:09:28.797918077 -0700
@@ -9,9 +9,8 @@
     CallablePrerequisite,
     StructuredToolResult,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     Toolset,
     ToolsetTag,
 )
@@ -70,7 +69,7 @@
     def __init__(self, toolset: "DatadogRDSToolset"):
         super().__init__(
             name="datadog_rds_performance_report",
-            description="[datadog/rds toolset] Generate a comprehensive performance report for a specific RDS instance including latency, resource utilization, and storage metrics with analysis",
+            description="Generate a comprehensive performance report for a specific RDS instance including latency, resource utilization, and storage metrics with analysis",
             parameters={
                 "db_instance_identifier": ToolParameter(
                     description="The RDS database instance identifier",
@@ -93,10 +92,10 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         if not self.toolset.dd_config:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=TOOLSET_CONFIG_MISSING_ERROR,
                 params=params,
             )
@@ -149,7 +148,7 @@
             formatted_report = self._format_report(report)
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=formatted_report,
                 params=params,
             )
@@ -157,7 +156,7 @@
         except Exception as e:
             logging.error(f"Error generating RDS performance report: {str(e)}")
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Failed to generate RDS performance report: {str(e)}",
                 params=params,
             )
@@ -363,7 +362,7 @@
     def __init__(self, toolset: "DatadogRDSToolset"):
         super().__init__(
             name="datadog_rds_top_worst_performing",
-            description="[datadog/rds toolset] Get a summarized report of the top worst performing RDS instances based on latency, CPU utilization, and error rates",
+            description="Get a summarized report of the top worst performing RDS instances based on latency, CPU utilization, and error rates",
             parameters={
                 "top_n": ToolParameter(
                     description=f"Number of worst performing instances to return (default: {DEFAULT_TOP_INSTANCES})",
@@ -391,10 +390,10 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         if not self.toolset.dd_config:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=TOOLSET_CONFIG_MISSING_ERROR,
                 params=params,
             )
@@ -413,7 +412,7 @@
 
             if not instances:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.NO_DATA,
+                    status=ToolResultStatus.NO_DATA,
                     data="No RDS instances found with metrics in the specified time range",
                     params=params,
                 )
@@ -440,7 +439,7 @@
             report += f"\n\nInstances:\n{json.dumps(worst_performers, indent=2)}"
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=report,
                 params=params,
             )
@@ -448,7 +447,7 @@
         except Exception as e:
             logging.error(f"Error getting top worst performing RDS instances: {str(e)}")
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Failed to get top worst performing RDS instances: {str(e)}",
                 params=params,
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/datadog/toolset_datadog_traces.py holmes2/holmes/plugins/toolsets/datadog/toolset_datadog_traces.py
--- baseline-holmes/holmes/plugins/toolsets/datadog/toolset_datadog_traces.py	2025-11-05 16:43:28.287822447 -0800
+++ holmes2/holmes/plugins/toolsets/datadog/toolset_datadog_traces.py	2025-10-17 15:09:28.798332949 -0700
@@ -9,11 +9,10 @@
 from holmes.core.tools import (
     CallablePrerequisite,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
     Toolset,
     StructuredToolResult,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     ToolsetTag,
 )
 from holmes.plugins.toolsets.datadog.datadog_api import (
@@ -50,7 +49,7 @@
         super().__init__(
             name="datadog/traces",
             description="Toolset for interacting with Datadog APM to fetch and analyze traces",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/datadog/",
+            docs_url="https://docs.datadoghq.com/api/latest/spans/",
             icon_url="https://imgix.datadoghq.com//img/about/presskit/DDlogo.jpg",
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
             tools=[
@@ -58,6 +57,7 @@
                 FetchDatadogTraceById(toolset=self),
                 FetchDatadogSpansByFilter(toolset=self),
             ],
+            experimental=True,
             tags=[ToolsetTag.CORE],
         )
         self._reload_instructions()
@@ -157,7 +157,7 @@
     def __init__(self, toolset: "DatadogTracesToolset"):
         super().__init__(
             name="fetch_datadog_traces",
-            description="[datadog/traces toolset] Fetch a list of traces from Datadog with optional filters",
+            description="Fetch a list of traces from Datadog with optional filters",
             parameters={
                 "service": ToolParameter(
                     description="Filter by service name",
@@ -211,11 +211,11 @@
         filter_str = ", ".join(filters) if filters else "all"
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Fetch Traces ({filter_str})"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         """Execute the tool to fetch traces."""
         if not self.toolset.dd_config:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="Datadog configuration not initialized",
                 params=params,
             )
@@ -304,13 +304,13 @@
             formatted_output = format_traces_list(spans, limit=params.get("limit", 50))
             if not formatted_output:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.NO_DATA,
+                    status=ToolResultStatus.NO_DATA,
                     params=params,
                     data="No matching traces found.",
                 )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=formatted_output,
                 params=params,
             )
@@ -329,7 +329,7 @@
                 error_msg = f"Exception while querying Datadog: {str(e)}"
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
                 invocation=(
@@ -342,7 +342,7 @@
         except Exception as e:
             logging.exception(e, exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Unexpected error: {str(e)}",
                 params=params,
                 invocation=(
@@ -359,7 +359,7 @@
     def __init__(self, toolset: "DatadogTracesToolset"):
         super().__init__(
             name="fetch_datadog_trace_by_id",
-            description="[datadog/traces toolset] Fetch detailed information about a specific trace by its ID",
+            description="Fetch detailed information about a specific trace by its ID",
             parameters={
                 "trace_id": ToolParameter(
                     description="The trace ID to fetch details for",
@@ -375,11 +375,11 @@
         trace_id = params.get("trace_id", "unknown")
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Fetch Trace Details ({trace_id})"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         """Execute the tool to fetch trace details."""
         if not self.toolset.dd_config:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="Datadog configuration not initialized",
                 params=params,
             )
@@ -387,7 +387,7 @@
         trace_id = params.get("trace_id")
         if not trace_id:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="trace_id parameter is required",
                 params=params,
             )
@@ -441,13 +441,13 @@
             formatted_output = format_trace_hierarchy(trace_id, spans)
             if not formatted_output:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.NO_DATA,
+                    status=ToolResultStatus.NO_DATA,
                     params=params,
                     data=f"No trace found for trace_id: {trace_id}",
                 )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=formatted_output,
                 params=params,
             )
@@ -466,7 +466,7 @@
                 error_msg = f"Exception while querying Datadog: {str(e)}"
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
                 invocation=(
@@ -479,7 +479,7 @@
         except Exception as e:
             logging.exception(e, exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Unexpected error: {str(e)}",
                 params=params,
                 invocation=(
@@ -496,7 +496,7 @@
     def __init__(self, toolset: "DatadogTracesToolset"):
         super().__init__(
             name="fetch_datadog_spans",
-            description="[datadog/traces toolset] Search for spans in Datadog with detailed filters",
+            description="Search for spans in Datadog with detailed filters",
             parameters={
                 "query": ToolParameter(
                     description="Datadog search query (e.g., 'service:web-app @http.status_code:500')",
@@ -556,11 +556,11 @@
         filter_str = ", ".join(filters) if filters else "all"
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Search Spans ({filter_str})"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         """Execute the tool to search spans."""
         if not self.toolset.dd_config:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="Datadog configuration not initialized",
                 params=params,
             )
@@ -648,13 +648,13 @@
             formatted_output = format_spans_search(spans)
             if not formatted_output:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.NO_DATA,
+                    status=ToolResultStatus.NO_DATA,
                     params=params,
                     data="No matching spans found.",
                 )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=formatted_output,
                 params=params,
             )
@@ -672,7 +672,7 @@
                 error_msg = f"Exception while querying Datadog: {str(e)}"
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
                 invocation=(
@@ -685,7 +685,7 @@
         except Exception as e:
             logging.exception(e, exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Unexpected error: {str(e)}",
                 params=params,
                 invocation=(
diff -ur baseline-holmes/holmes/plugins/toolsets/docker.yaml holmes2/holmes/plugins/toolsets/docker.yaml
--- baseline-holmes/holmes/plugins/toolsets/docker.yaml	2025-11-05 16:43:28.287942405 -0800
+++ holmes2/holmes/plugins/toolsets/docker.yaml	2025-10-17 15:09:28.827636054 -0700
@@ -1,7 +1,7 @@
 toolsets:
   docker/core:
     description: "Read access to Docker resources"
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/docker/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/docker.html"
     icon_url: "https://platform.robusta.dev/demos/docker.svg"
     tags:
       - cli
diff -ur baseline-holmes/holmes/plugins/toolsets/git.py holmes2/holmes/plugins/toolsets/git.py
--- baseline-holmes/holmes/plugins/toolsets/git.py	2025-11-05 16:43:28.288076405 -0800
+++ holmes2/holmes/plugins/toolsets/git.py	2025-10-17 15:09:28.675759153 -0700
@@ -4,11 +4,7 @@
 import os
 from typing import Any, Optional, Dict, List, Tuple
 from pydantic import BaseModel
-from holmes.core.tools import (
-    StructuredToolResult,
-    StructuredToolResultStatus,
-    ToolInvokeContext,
-)
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 
 from holmes.core.tools import (
     Toolset,
@@ -37,7 +33,7 @@
         super().__init__(
             name="git",
             description="Runs git commands to read repos and create PRs",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/github/",
+            docs_url="https://docs.github.com/en/rest",
             icon_url="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg",
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
             tools=[
@@ -253,11 +249,7 @@
             toolset=toolset,  # type: ignore
         )
 
-    def _invoke(
-        self,
-        params: dict,
-        context: ToolInvokeContext,
-    ) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         filepath = params["filepath"]
         try:
             headers = {"Authorization": f"token {self.toolset.git_credentials}"}
@@ -265,7 +257,7 @@
             resp = requests.get(url, headers=headers)
             if resp.status_code != 200:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     data=self.toolset._sanitize_error(
                         f"Error fetching file: {resp.text}"
                     ),
@@ -274,13 +266,13 @@
             content = base64.b64decode(resp.json()["content"]).decode().splitlines()
             numbered = "\n".join(f"{i+1}: {line}" for i, line in enumerate(content))
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=numbered,
                 params=params,
             )
         except Exception as e:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 data=self.toolset._sanitize_error(str(e)),
                 params=params,
             )
@@ -301,18 +293,14 @@
             toolset=toolset,  # type: ignore
         )
 
-    def _invoke(
-        self,
-        params: dict,
-        context: ToolInvokeContext,
-    ) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         try:
             headers = {"Authorization": f"token {self.toolset.git_credentials}"}
             url = f"https://api.github.com/repos/{self.toolset.git_repo}/git/trees/{self.toolset.git_branch}?recursive=1"
             resp = requests.get(url, headers=headers)
             if resp.status_code != 200:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     data=self.toolset._sanitize_error(
                         f"Error listing files: {resp.text}"
                     ),
@@ -320,13 +308,13 @@
                 )
             paths = [entry["path"] for entry in resp.json()["tree"]]
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=paths,
                 params=params,
             )
         except Exception as e:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 data=self.toolset._sanitize_error(str(e)),
                 params=params,
             )
@@ -346,7 +334,7 @@
             toolset=toolset,  # type: ignore
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         try:
             prs = self.toolset.list_open_prs()
             formatted = [
@@ -359,13 +347,13 @@
                 for pr in prs
             ]
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=formatted,
                 params=params,
             )
         except Exception as e:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 data=self.toolset._sanitize_error(str(e)),
                 params=params,
             )
@@ -414,17 +402,17 @@
             toolset=toolset,  # type: ignore
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         def error(msg: str) -> StructuredToolResult:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 data=self.toolset._sanitize_error(msg),
                 params=params,
             )
 
         def success(msg: Any) -> StructuredToolResult:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS, data=msg, params=params
+                status=ToolResultStatus.SUCCESS, data=msg, params=params
             )
 
         def modify_lines(lines: List[str]) -> List[str]:
@@ -632,7 +620,7 @@
             toolset=toolset,  # type: ignore
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         try:
             line = params["line"]
             filename = params["filename"]
@@ -645,24 +633,24 @@
             # Validate inputs
             if not commit_message.strip():
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error="Tool call failed to run: Commit message cannot be empty",
                 )
             if not filename.strip():
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error="Tool call failed to run: Filename cannot be empty",
                 )
             if line < 1:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error="Tool call failed to run: Line number must be positive",
                 )
 
             # Verify this is a PR created by our tool
             if not self.toolset.is_created_pr(pr_number):
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error=f"Tool call failed to run: PR #{pr_number} was not created by this tool. Only PRs created using git_execute_changes can be updated.",
                 )
 
@@ -716,7 +704,7 @@
                     del content_lines[line - 1]
                 else:
                     return StructuredToolResult(
-                        status=StructuredToolResultStatus.ERROR,
+                        status=ToolResultStatus.ERROR,
                         error=f"Tool call failed to run: Invalid command: {command}",
                     )
 
@@ -724,7 +712,7 @@
 
                 if dry_run:
                     return StructuredToolResult(
-                        status=StructuredToolResultStatus.SUCCESS,
+                        status=ToolResultStatus.SUCCESS,
                         data=f"DRY RUN: Updated content for PR #{pr_number}:\n\n{updated_content}",
                     )
 
@@ -733,13 +721,13 @@
                     pr_number, filename, updated_content, commit_message
                 )
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.SUCCESS,
+                    status=ToolResultStatus.SUCCESS,
                     data=f"Added commit to PR #{pr_number} successfully",
                 )
 
             except Exception as e:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error=self.toolset._sanitize_error(
                         f"Tool call failed to run: Error updating PR: {str(e)}"
                     ),
@@ -747,14 +735,14 @@
 
         except requests.exceptions.RequestException as e:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=self.toolset._sanitize_error(
                     f"Tool call failed to run: Network error: {str(e)}"
                 ),
             )
         except Exception as e:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=self.toolset._sanitize_error(
                     f"Tool call failed to run: Unexpected error: {str(e)}"
                 ),
Only in holmes2/holmes/plugins/toolsets/grafana: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/grafana/common.py holmes2/holmes/plugins/toolsets/grafana/common.py
--- baseline-holmes/holmes/plugins/toolsets/grafana/common.py	2025-11-05 16:43:28.288425404 -0800
+++ holmes2/holmes/plugins/toolsets/grafana/common.py	2025-10-17 15:09:28.704084058 -0700
@@ -3,12 +3,12 @@
 from pydantic import BaseModel
 import datetime
 
-from holmes.core.tools import StructuredToolResult, StructuredToolResultStatus
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 
 
 class GrafanaConfig(BaseModel):
     """A config that represents one of the Grafana related tools like Loki or Tempo
-    If `grafana_datasource_uid` is set, then it is assumed that Holmes will proxy all
+    If `grafana_datasource_uid` is set, then it is assume that Holmes will proxy all
     requests through grafana. In this case `url` should be the grafana URL.
     If `grafana_datasource_uid` is not set, it is assumed that the `url` is the
     systems' URL
@@ -61,20 +61,8 @@
 ) -> Optional[StructuredToolResult]:
     if not config.grafana_datasource_uid:
         return StructuredToolResult(
-            status=StructuredToolResultStatus.ERROR,
+            status=ToolResultStatus.ERROR,
             error="This tool only works when the toolset is configued ",
         )
     else:
         return None
-
-
-class GrafanaTempoLabelsConfig(BaseModel):
-    pod: str = "k8s.pod.name"
-    namespace: str = "k8s.namespace.name"
-    deployment: str = "k8s.deployment.name"
-    node: str = "k8s.node.name"
-    service: str = "service.name"
-
-
-class GrafanaTempoConfig(GrafanaConfig):
-    labels: GrafanaTempoLabelsConfig = GrafanaTempoLabelsConfig()
diff -ur baseline-holmes/holmes/plugins/toolsets/grafana/grafana_api.py holmes2/holmes/plugins/toolsets/grafana/grafana_api.py
--- baseline-holmes/holmes/plugins/toolsets/grafana/grafana_api.py	2025-11-05 16:43:28.288519404 -0800
+++ holmes2/holmes/plugins/toolsets/grafana/grafana_api.py	2025-10-17 15:09:28.693755679 -0700
@@ -27,7 +27,7 @@
         response.raise_for_status()
         return True, ""
     except Exception as e:
-        logging.debug(f"Failed to fetch grafana health status at {url}", exc_info=True)
+        logging.error(f"Failed to fetch grafana health status at {url}", exc_info=True)
         error_msg = f"Failed to fetch grafana health status at {url}. {str(e)}"
 
         # Add helpful hint if this looks like a common misconfiguration
Only in baseline-holmes/holmes/plugins/toolsets/grafana: grafana_tempo_api.py
Only in holmes2/holmes/plugins/toolsets/grafana: tempo_api.py
Only in baseline-holmes/holmes/plugins/toolsets/grafana: toolset_grafana_dashboard.jinja2
diff -ur baseline-holmes/holmes/plugins/toolsets/grafana/toolset_grafana_loki.py holmes2/holmes/plugins/toolsets/grafana/toolset_grafana_loki.py
--- baseline-holmes/holmes/plugins/toolsets/grafana/toolset_grafana_loki.py	2025-11-05 16:43:28.289484567 -0800
+++ holmes2/holmes/plugins/toolsets/grafana/toolset_grafana_loki.py	2025-10-17 15:09:28.695719289 -0700
@@ -14,7 +14,6 @@
     LoggingCapability,
     PodLoggingTool,
     DEFAULT_TIME_SPAN_SECONDS,
-    DEFAULT_LOG_LIMIT,
 )
 from holmes.plugins.toolsets.utils import (
     process_timestamps_to_rfc3339,
@@ -23,7 +22,7 @@
 from holmes.plugins.toolsets.grafana.loki_api import (
     query_loki_logs_by_label,
 )
-from holmes.core.tools import StructuredToolResult, StructuredToolResultStatus
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 
 
 class GrafanaLokiLabelsConfig(BaseModel):
@@ -46,7 +45,7 @@
             name="grafana/loki",
             description="Fetches kubernetes pods logs from Loki",
             icon_url="https://grafana.com/media/docs/loki/logo-grafana-loki.png",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/grafanaloki/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/grafanaloki.html",
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
             tools=[],  # Initialize with empty tools first
         )
@@ -95,17 +94,17 @@
             label_value=params.pod_name,
             start=start,
             end=end,
-            limit=params.limit or DEFAULT_LOG_LIMIT,
+            limit=params.limit or 2000,
         )
         if logs:
             logs.sort(key=lambda x: x["timestamp"])
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data="\n".join([format_log(log) for log in logs]),
                 params=params.model_dump(),
             )
         else:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.NO_DATA,
+                status=ToolResultStatus.NO_DATA,
                 params=params.model_dump(),
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/grafana/toolset_grafana_tempo.jinja2 holmes2/holmes/plugins/toolsets/grafana/toolset_grafana_tempo.jinja2
--- baseline-holmes/holmes/plugins/toolsets/grafana/toolset_grafana_tempo.jinja2	2025-11-05 16:43:28.289654817 -0800
+++ holmes2/holmes/plugins/toolsets/grafana/toolset_grafana_tempo.jinja2	2025-10-17 15:09:28.705402589 -0700
@@ -1,247 +1,12 @@
-Grafana Tempo provides distributed tracing data through its REST API. Each tool maps directly to a specific Tempo API endpoint.
-
+Use Tempo when investigating latency or performance issues. Tempo provides traces information for application running on the cluster.
 Assume every application provides tempo traces.
-
-## API Endpoints and Tool Mapping
-
-1. **Trace Search** (GET /api/search)
-   - `tempo_search_traces_by_query`: Use with 'q' parameter for TraceQL queries
-   - `tempo_search_traces_by_tags`: Use with 'tags' parameter for logfmt queries
-
-2. **Trace Details** (GET /api/v2/traces/{trace_id})
-   - `tempo_query_trace_by_id`: Retrieve full trace data
-
-3. **Tag Discovery**
-   - `tempo_search_tag_names` (GET /api/v2/search/tags): List available tags
-   - `tempo_search_tag_values` (GET /api/v2/search/tag/{tag}/values): Get values for a tag
-
-4. **TraceQL Metrics**
-   - `tempo_query_metrics_instant` (GET /api/metrics/query): Single value computation
-   - `tempo_query_metrics_range` (GET /api/metrics/query_range): Time series data
-
-## Usage Workflow
-
-### 1. Discovering Available Data
-Start by understanding what tags and values exist:
-- Use `tempo_search_tag_names` to discover available tags
-- Use `tempo_search_tag_values` to see all values for a specific tag (e.g., service names)
-
-### 2. Searching for Traces
-
-**TraceQL Search (recommended):**
-Use `tempo_search_traces_by_query` with TraceQL syntax for powerful filtering.
-
-**TraceQL Capabilities:**
-TraceQL can select traces based on the following:
-- **Span and resource attributes** - Filter by any attribute on spans or resources
-- **Timing and duration** - Filter by trace/span duration
-- **Basic aggregates** - Use aggregate functions to compute values across spans
-
-**Supported Aggregate Functions:**
-- `count()` - Count the number of spans matching the criteria
-- `avg(attribute)` - Calculate average of a numeric attribute across spans
-- `min(attribute)` - Find minimum value of a numeric attribute
-- `max(attribute)` - Find maximum value of a numeric attribute
-- `sum(attribute)` - Sum values of a numeric attribute across spans
-
-**Aggregate Function Usage:**
-Aggregates are used with the pipe operator `|` to filter traces based on computed values across their spans.
-
-**Aggregate Examples:**
-- `{ span.http.status_code = 200 } | count() > 3` - Find traces with more than 3 spans having HTTP 200 status
-- `{ } | sum(span.bytesProcessed) > 1000000000` - Find traces where total processed bytes exceed 1 GB
-- `{ status = error } | by(resource.service.name) | count() > 1` - Find services with more than 1 error
-
-**Select Function:**
-- `{ status = error } | select(span.http.status_code, span.http.url)` - Select specific attributes from error spans
-
-**TraceQL Query Structure:**
-TraceQL queries follow the pattern: `{span-selectors} | aggregate`
-
-**TraceQL Query Examples (from official docs):**
-
-1. **Find traces of a specific operation:**
-   ```
-   {resource.service.name = "frontend" && name = "POST /api/orders"}
-   ```
-   ```
-   {
-     resource.service.namespace = "ecommerce" &&
-     resource.service.name = "frontend" &&
-     resource.deployment.environment = "production" &&
-     name = "POST /api/orders"
-   }
-   ```
-
-2. **Find traces with a particular outcome:**
-   ```
-   {
-     resource.service.name="frontend" &&
-     name = "POST /api/orders" &&
-     status = error
-   }
-   ```
-   ```
-   {
-     resource.service.name="frontend" &&
-     name = "POST /api/orders" &&
-     span.http.status_code >= 500
-   }
-   ```
-
-3. **Find traces with a particular behavior:**
-   ```
-   {span.service.name="frontend" && name = "GET /api/products/{id}"} && {span.db.system="postgresql"}
-   ```
-
-4. **Find traces across environments:**
-   ```
-   { resource.deployment.environment = "production" } && { resource.deployment.environment = "staging" }
-   ```
-
-5. **Structural operators (advanced):**
-   ```
-   { resource.service.name="frontend" } >> { status = error }  # Frontend spans followed by errors
-   { } !< { resource.service.name = "productcatalogservice" }  # Traces without productcatalog as child
-   { resource.service.name = "productcatalogservice" } ~ { resource.service.name="frontend" }  # Sibling spans
-   ```
-
-6. **Additional operator examples:**
-   ```
-   { span.http.method = "GET" && status = ok } && { span.http.method = "DELETE" && status != ok }  # && for multiple conditions
-   ```
-
-   ```
-   { resource.deployment.environment =~ "prod-.*" && span.http.status_code = 200 }  # =~ regex match
-   { span.http.method =~ "DELETE|GET" }  # Regex match multiple values
-   { trace:rootName !~ ".*perf.*" }  # !~ negated regex
-   { resource.cloud.region = "us-east-1" } || { resource.cloud.region = "us-west-1" }  # || OR operator
-   ```
-
-   ```
-   { span.http.status_code >= 400 && span.http.status_code < 500 }  # Client errors (4xx)
-   { span.http.url = "/path/of/api" } >> { span.db.name = "db-shard-001" }  # >> descendant
-   { span.http.status_code = 200 } | select(resource.service.name)  # Select specific attributes
-   ```
-
-**Common Attributes to Query:**
-- `resource.service.name` - Service name
-- `resource.k8s.*` - Kubernetes metadata (pod.name, namespace.name, deployment.name, etc.)
-- `span.http.*` - HTTP attributes (status_code, method, route, url, etc.)
-- `name` - Span name
-- `status` - Span status (error, ok)
-- `duration` - Span duration
-- `kind` - Span kind (server, client, producer, consumer, internal)
-
-**Tag-based Search (legacy):**
-Use `tempo_search_traces_by_tags` with logfmt format when you need min/max duration filters:
-- Example: `service.name="api" http.status_code="500"`
-- Supports `min_duration` and `max_duration` parameters
-
-### 3. Analyzing Specific Traces
-When you have trace IDs from search results:
-- Use `tempo_query_trace_by_id` to get full trace details
-- Examine spans for errors, slow operations, and bottlenecks
-
-### 4. Computing Metrics from Traces
-**TraceQL metrics** compute aggregated metrics from your trace data, helping you answer critical questions like:
-- How many database calls across all systems are downstream of your application?
-- What services beneath a given endpoint are failing?
-- What services beneath an endpoint are slow?
-
-TraceQL metrics parse your traces in aggregate to provide RED (Rate, Error, Duration) metrics from trace data.
-
-**Supported Functions:**
-- `rate` - Calculate rate of spans/traces
-- `count_over_time` - Count spans/traces over time
-- `sum_over_time` - Sum span attributes
-- `avg_over_time` - Average of span attributes
-- `max_over_time` - Maximum value over time
-- `min_over_time` - Minimum value over time
-- `quantile_over_time` - Calculate quantiles
-- `histogram_over_time` - Generate histogram data
-- `compare` - Compare metrics between time periods
-
-**Modifiers:**
-- `topk` - Return top N results
-- `bottomk` - Return bottom N results
-
-**TraceQL Metrics Query Examples:**
-
-1. **rate** - Calculate error rate by service and HTTP route:
-   ```
-   { resource.service.name = "foo" && status = error } | rate() by (span.http.route)
-   ```
-
-2. **count_over_time** - Count spans by HTTP status code:
-   ```
-   { name = "GET /:endpoint" } | count_over_time() by (span.http.status_code)
-   ```
-
-3. **sum_over_time** - Sum HTTP response sizes by service:
-   ```
-   { name = "GET /:endpoint" } | sum_over_time(span.http.response.size) by (resource.service.name)
-   ```
-
-4. **avg_over_time** - Average duration by HTTP status code:
-   ```
-   { name = "GET /:endpoint" } | avg_over_time(duration) by (span.http.status_code)
-   ```
-
-5. **max_over_time** - Maximum response size by HTTP target:
-   ```
-   { name = "GET /:endpoint" } | max_over_time(span.http.response.size) by (span.http.target)
-   ```
-
-6. **min_over_time** - Minimum duration by HTTP target:
-   ```
-   { name = "GET /:endpoint" } | min_over_time(duration) by (span.http.target)
-   ```
-
-7. **quantile_over_time** - Calculate multiple percentiles (99th, 90th, 50th) with exemplars:
-   ```
-   { span:name = "GET /:endpoint" } | quantile_over_time(duration, .99, .9, .5) by (span.http.target) with (exemplars=true)
-   ```
-
-8. **histogram_over_time** - Build duration histogram grouped by custom attribute:
-   ```
-   { name = "GET /:endpoint" } | histogram_over_time(duration) by (span.foo)
-   ```
-
-9. **compare** - Compare error spans against baseline (10 attributes):
-   ```
-   { resource.service.name="a" && span.http.path="/myapi" } | compare({status=error}, 10)
-   ```
-
-10. **Using topk modifier** - Find top 10 endpoints by request rate:
-   ```
-   { resource.service.name = "foo" } | rate() by (span.http.url) | topk(10)
-   ```
-
-**Choosing Between Instant and Range Queries:**
-
-**Instant Metrics** (`tempo_query_metrics_instant`) - Returns a single aggregated value for the entire time range. Use this when:
-- You need a total count or sum across the whole period
-- You want a single metric value (e.g., total error count, average latency)
-- You don't need to see how the metric changes over time
-- You're computing a KPI or summary statistic
-
-**Time Series Metrics** (`tempo_query_metrics_range`) - Returns values at regular intervals controlled by the 'step' parameter. Use this when:
-- You need to graph metrics over time or analyze trends
-- You want to see patterns, spikes, or changes in metrics
-- You're troubleshooting time-based issues
-- You need to correlate metrics with specific time periods
-
-## Special workflow for performance issues
-When investigating performance issues in kubernetes via traces, call tempo_fetch_traces_comparative_sample. This tool provides comprehensive analysis for identifying patterns.
-
-## Important Notes
-- TraceQL is the modern query language - prefer it over tag-based search
-- TraceQL metrics are computed from trace data, not traditional Prometheus metrics
-- TraceQL metrics is an experimental feature that computes RED (Rate, Error, Duration) metrics from trace data
-- Common attributes to use in queries: resource.service.name, span.http.route, span.http.status_code, span.http.target, status, name, duration
-- All timestamps can be Unix epoch seconds or RFC3339 format
-- Use time filters (start/end) to improve query performance
-- To get information about Kubernetes resources try these first: resource.service.name, resource.k8s.pod.name, resource.k8s.namespace.name, resource.k8s.deployment.name, resource.k8s.node.name, resource.k8s.container.name
-- TraceQL and TraceQL metrics language are complex. If you get empty data, try to simplify your query and try again!
-- IMPORTANT: TraceQL is not the same as 'TraceQL metrics' - Make sure you use the correct syntax and functions
+1. Start by identifying an initial filter to use. This can be a pod name, a deployment name or a service name
+2. Call fetch_tempo_traces_comparative_sample first when investigating performance issues via traces. This tool provides comprehensive analysis for identifying patterns. For other issues not related to performance, you can start with fetch_tempo_traces.
+3. Use `fetch_tempo_traces` setting the appropriate query params
+    - Use the min_duration filter to ensure you get traces that trigger the alert when you are investigating a performance issue
+    - If possible, use start and end date to narrow down your search.
+        - Use fetch_finding_by_id if you are provided with a finding/alert id. It will contain details about when the alert was triggered
+    - Use at least one of the following argument to ensure you get relevant traces: `service_name`, `pod_name` or `deployment_name`.
+4. When you have a specific trace ID to investigate, use `fetch_tempo_trace_by_id` to get detailed information about that trace.
+5. Look at the duration of each span in any single trace and deduce any issues.
+6. ALWAYS fetch the logs for a pod once you identify a span that is taking a long time. There may be an explanation for the slowness in the logs.
diff -ur baseline-holmes/holmes/plugins/toolsets/grafana/toolset_grafana_tempo.py holmes2/holmes/plugins/toolsets/grafana/toolset_grafana_tempo.py
--- baseline-holmes/holmes/plugins/toolsets/grafana/toolset_grafana_tempo.py	2025-11-05 16:43:28.289790858 -0800
+++ holmes2/holmes/plugins/toolsets/grafana/toolset_grafana_tempo.py	2025-10-17 15:09:28.694199051 -0700
@@ -1,36 +1,56 @@
 import os
-from typing import Any, Dict, Tuple, cast, List
+import re
+from typing import Any, Dict, List, cast
 
+import requests  # type: ignore
 import yaml  # type: ignore
+from pydantic import BaseModel
 
-from holmes.common.env_vars import load_bool, MAX_GRAPH_POINTS
+from holmes.common.env_vars import load_bool
 from holmes.core.tools import (
     StructuredToolResult,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
-    StructuredToolResultStatus,
+    ToolResultStatus,
 )
-from holmes.plugins.toolsets.consts import STANDARD_END_DATETIME_TOOL_PARAM_DESCRIPTION
 from holmes.plugins.toolsets.grafana.base_grafana_toolset import BaseGrafanaToolset
 from holmes.plugins.toolsets.grafana.common import (
-    GrafanaTempoConfig,
+    GrafanaConfig,
+    build_headers,
+    get_base_url,
 )
-from holmes.plugins.toolsets.grafana.grafana_tempo_api import GrafanaTempoAPI
+from holmes.plugins.toolsets.grafana.tempo_api import (
+    query_tempo_trace_by_id,
+    query_tempo_traces,
+)
+from holmes.plugins.toolsets.grafana.trace_parser import format_traces_list
 from holmes.plugins.toolsets.logging_utils.logging_api import (
-    DEFAULT_GRAPH_TIME_SPAN_SECONDS,
+    DEFAULT_TIME_SPAN_SECONDS,
 )
 from holmes.plugins.toolsets.utils import (
-    toolset_name_for_one_liner,
+    get_param_or_raise,
     process_timestamps_to_int,
-    standard_start_datetime_tool_param_description,
-    adjust_step_for_max_points,
-    seconds_to_duration_string,
-    duration_string_to_seconds,
+    toolset_name_for_one_liner,
 )
 
 TEMPO_LABELS_ADD_PREFIX = load_bool("TEMPO_LABELS_ADD_PREFIX", True)
 
+ONE_HOUR_IN_SECONDS = 3600
+DEFAULT_TRACES_TIME_SPAN_SECONDS = DEFAULT_TIME_SPAN_SECONDS  # 7 days
+DEFAULT_TAGS_TIME_SPAN_SECONDS = 8 * ONE_HOUR_IN_SECONDS  # 8 hours
+
+
+class GrafanaTempoLabelsConfig(BaseModel):
+    pod: str = "k8s.pod.name"
+    namespace: str = "k8s.namespace.name"
+    deployment: str = "k8s.deployment.name"
+    node: str = "k8s.node.name"
+    service: str = "service.name"
+
+
+class GrafanaTempoConfig(GrafanaConfig):
+    labels: GrafanaTempoLabelsConfig = GrafanaTempoLabelsConfig()
+
 
 class BaseGrafanaTempoToolset(BaseGrafanaToolset):
     config_class = GrafanaTempoConfig
@@ -47,23 +67,6 @@
     def grafana_config(self) -> GrafanaTempoConfig:
         return cast(GrafanaTempoConfig, self._grafana_config)
 
-    def prerequisites_callable(self, config: dict[str, Any]) -> Tuple[bool, str]:
-        """Check Tempo connectivity using the echo endpoint."""
-        # First call parent to validate config
-        success, msg = super().prerequisites_callable(config)
-        if not success:
-            return success, msg
-
-        # Then check Tempo-specific echo endpoint
-        try:
-            api = GrafanaTempoAPI(self.grafana_config)
-            if api.query_echo_endpoint():
-                return True, "Successfully connected to Tempo"
-            else:
-                return False, "Failed to connect to Tempo echo endpoint"
-        except Exception as e:
-            return False, f"Failed to connect to Tempo: {str(e)}"
-
     def build_k8s_filters(
         self, params: Dict[str, Any], use_exact_match: bool
     ) -> List[str]:
@@ -104,25 +107,224 @@
                     escaped_value = value.replace('"', '\\"')
                     filters.append(f'{prefix}{label}="{escaped_value}"')
                 else:
-                    # For partial match, use simple substring matching
-                    # Don't escape anything - let Tempo handle the regex
-                    filters.append(f'{prefix}{label}=~".*{value}.*"')
+                    # Escape regex special characters for partial match
+                    escaped_value = re.escape(value)
+                    filters.append(f'{prefix}{label}=~".*{escaped_value}.*"')
 
         return filters
 
-    @staticmethod
-    def adjust_start_end_time(params: Dict) -> Tuple[int, int]:
-        return process_timestamps_to_int(
-            start=params.get("start"),
-            end=params.get("end"),
-            default_time_span_seconds=DEFAULT_GRAPH_TIME_SPAN_SECONDS,
+
+def validate_params(params: Dict[str, Any], expected_params: List[str]):
+    for param in expected_params:
+        if param in params and params[param] not in (None, "", [], {}):
+            return None
+
+    return f"At least one of the following argument is expected but none were set: {expected_params}"
+
+
+class GetTempoTraces(Tool):
+    def __init__(self, toolset: BaseGrafanaTempoToolset):
+        super().__init__(
+            name="fetch_tempo_traces",
+            description="""Lists Tempo traces. At least one of `service_name`, `pod_name` or `deployment_name` argument is required.""",
+            parameters={
+                "min_duration": ToolParameter(
+                    description="The minimum duration of traces to fetch, e.g., '5s' for 5 seconds.",
+                    type="string",
+                    required=True,
+                ),
+                "service_name": ToolParameter(
+                    description="Filter traces by service name",
+                    type="string",
+                    required=False,
+                ),
+                "pod_name": ToolParameter(
+                    description="Filter traces by pod name",
+                    type="string",
+                    required=False,
+                ),
+                "namespace_name": ToolParameter(
+                    description="Filter traces by namespace",
+                    type="string",
+                    required=False,
+                ),
+                "deployment_name": ToolParameter(
+                    description="Filter traces by deployment name",
+                    type="string",
+                    required=False,
+                ),
+                "node_name": ToolParameter(
+                    description="Filter traces by node",
+                    type="string",
+                    required=False,
+                ),
+                "start_datetime": ToolParameter(
+                    description=f"The beginning time boundary for the trace search period. String in RFC3339 format. If a negative integer, the number of seconds relative to the end_timestamp. Defaults to -{DEFAULT_TRACES_TIME_SPAN_SECONDS}",
+                    type="string",
+                    required=False,
+                ),
+                "end_datetime": ToolParameter(
+                    description="The ending time boundary for the trace search period. String in RFC3339 format. Defaults to NOW().",
+                    type="string",
+                    required=False,
+                ),
+                "limit": ToolParameter(
+                    description="Maximum number of traces to return. Defaults to 50",
+                    type="string",
+                    required=False,
+                ),
+                "sort": ToolParameter(
+                    description="One of 'descending', 'ascending' or 'none' for no sorting. Defaults to descending",
+                    type="string",
+                    required=False,
+                ),
+            },
+        )
+        self._toolset = toolset
+
+    def _invoke(self, params: Dict) -> StructuredToolResult:
+        api_key = self._toolset.grafana_config.api_key
+        headers = self._toolset.grafana_config.headers
+
+        invalid_params_error = validate_params(
+            params, ["service_name", "pod_name", "deployment_name"]
+        )
+        if invalid_params_error:
+            return StructuredToolResult(
+                status=ToolResultStatus.ERROR,
+                error=invalid_params_error,
+                params=params,
+            )
+
+        start, end = process_timestamps_to_int(
+            params.get("start_datetime"),
+            params.get("end_datetime"),
+            default_time_span_seconds=DEFAULT_TRACES_TIME_SPAN_SECONDS,
+        )
+
+        filters = self._toolset.build_k8s_filters(params, use_exact_match=True)
+
+        filters.append(f'duration>{get_param_or_raise(params, "min_duration")}')
+
+        query = " && ".join(filters)
+        query = f"{{{query}}}"
+
+        base_url = get_base_url(self._toolset.grafana_config)
+        traces = query_tempo_traces(
+            base_url=base_url,
+            api_key=api_key,
+            headers=headers,
+            query=query,
+            start=start,
+            end=end,
+            limit=params.get("limit", 50),
+        )
+        return StructuredToolResult(
+            status=ToolResultStatus.SUCCESS,
+            data=format_traces_list(traces),
+            params=params,
+            invocation=query,
+        )
+
+    def get_parameterized_one_liner(self, params: Dict) -> str:
+        return f"{toolset_name_for_one_liner(self._toolset.name)}: Fetched Tempo Traces (min_duration={params.get('min_duration')})"
+
+
+class GetTempoTags(Tool):
+    def __init__(self, toolset: BaseGrafanaTempoToolset):
+        super().__init__(
+            name="fetch_tempo_tags",
+            description="List the tags available in Tempo",
+            parameters={
+                "start_datetime": ToolParameter(
+                    description=f"The beginning time boundary for the search period. String in RFC3339 format. If a negative integer, the number of seconds relative to the end_timestamp. Defaults to -{DEFAULT_TAGS_TIME_SPAN_SECONDS}",
+                    type="string",
+                    required=False,
+                ),
+                "end_datetime": ToolParameter(
+                    description="The ending time boundary for the search period. String in RFC3339 format. Defaults to NOW().",
+                    type="string",
+                    required=False,
+                ),
+            },
+        )
+        self._toolset = toolset
+
+    def _invoke(self, params: Dict) -> StructuredToolResult:
+        api_key = self._toolset.grafana_config.api_key
+        headers = self._toolset.grafana_config.headers
+        start, end = process_timestamps_to_int(
+            start=params.get("start_datetime"),
+            end=params.get("end_datetime"),
+            default_time_span_seconds=DEFAULT_TAGS_TIME_SPAN_SECONDS,
+        )
+
+        base_url = get_base_url(self._toolset.grafana_config)
+        url = f"{base_url}/api/v2/search/tags?start={start}&end={end}"
+
+        try:
+            response = requests.get(
+                url,
+                headers=build_headers(api_key=api_key, additional_headers=headers),
+                timeout=60,
+            )
+            response.raise_for_status()  # Raise an error for non-2xx responses
+            data = response.json()
+            return StructuredToolResult(
+                status=ToolResultStatus.SUCCESS,
+                data=yaml.dump(data.get("scopes")),
+                params=params,
+            )
+        except requests.exceptions.RequestException as e:
+            raise Exception(
+                f"Failed to retrieve trace by ID after retries: {e} \n for URL: {url}"
+            )
+
+    def get_parameterized_one_liner(self, params: Dict) -> str:
+        return f"{toolset_name_for_one_liner(self._toolset.name)}: Fetched Tempo tags"
+
+
+class GetTempoTraceById(Tool):
+    def __init__(self, toolset: BaseGrafanaTempoToolset):
+        super().__init__(
+            name="fetch_tempo_trace_by_id",
+            description="""Retrieves detailed information about a Tempo trace using its trace ID. Use this to investigate a trace.""",
+            parameters={
+                "trace_id": ToolParameter(
+                    description="The unique trace ID to fetch.",
+                    type="string",
+                    required=True,
+                ),
+            },
+        )
+        self._toolset = toolset
+
+    def _invoke(self, params: Dict) -> StructuredToolResult:
+        labels_mapping = self._toolset.grafana_config.labels
+        labels = list(labels_mapping.model_dump().values())
+
+        base_url = get_base_url(self._toolset.grafana_config)
+        trace_data = query_tempo_trace_by_id(
+            base_url=base_url,
+            api_key=self._toolset.grafana_config.api_key,
+            headers=self._toolset.grafana_config.headers,
+            trace_id=get_param_or_raise(params, "trace_id"),
+            key_labels=labels,
+        )
+        return StructuredToolResult(
+            status=ToolResultStatus.SUCCESS,
+            data=trace_data,
+            params=params,
         )
 
+    def get_parameterized_one_liner(self, params: Dict) -> str:
+        return f"{toolset_name_for_one_liner(self._toolset.name)}: Fetched Tempo Trace (trace_id={params.get('trace_id')})"
+
 
 class FetchTracesSimpleComparison(Tool):
     def __init__(self, toolset: BaseGrafanaTempoToolset):
         super().__init__(
-            name="tempo_fetch_traces_comparative_sample",
+            name="fetch_tempo_traces_comparative_sample",
             description="""Fetches statistics and representative samples of fast, slow, and typical traces for performance analysis. Requires either a `base_query` OR at least one of `service_name`, `pod_name`, `namespace_name`, `deployment_name`, `node_name`.
 
 Important: call this tool first when investigating performance issues via traces. This tool provides comprehensive analysis for identifying patterns.
@@ -158,11 +360,7 @@
                     required=False,
                 ),
                 "base_query": ToolParameter(
-                    description=(
-                        "Custom TraceQL filter. Supports span/resource attributes, "
-                        "duration, and aggregates (count(), avg(), min(), max(), sum()). "
-                        "Examples: '{span.http.status_code>=400}', '{duration>100ms}'"
-                    ),
+                    description="Custom TraceQL filter",
                     type="string",
                     required=False,
                 ),
@@ -171,15 +369,13 @@
                     type="integer",
                     required=False,
                 ),
-                "start": ToolParameter(
-                    description=standard_start_datetime_tool_param_description(
-                        DEFAULT_GRAPH_TIME_SPAN_SECONDS
-                    ),
+                "start_datetime": ToolParameter(
+                    description=f"The beginning time boundary for the trace search period. String in RFC3339 format. If a negative integer, the number of seconds relative to the end_timestamp. Defaults to -{DEFAULT_TRACES_TIME_SPAN_SECONDS}",
                     type="string",
                     required=False,
                 ),
-                "end": ToolParameter(
-                    description=STANDARD_END_DATETIME_TOOL_PARAM_DESCRIPTION,
+                "end_datetime": ToolParameter(
+                    description="The ending time boundary for the trace search period. String in RFC3339 format. Defaults to NOW().",
                     type="string",
                     required=False,
                 ),
@@ -187,15 +383,7 @@
         )
         self._toolset = toolset
 
-    @staticmethod
-    def validate_params(params: Dict[str, Any], expected_params: List[str]):
-        for param in expected_params:
-            if param in params and params[param] not in (None, "", [], {}):
-                return None
-
-        return f"At least one of the following argument is expected but none were set: {expected_params}"
-
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             # Build query
             if params.get("base_query"):
@@ -205,7 +393,7 @@
                 filters = self._toolset.build_k8s_filters(params, use_exact_match=False)
 
                 # Validate that at least one parameter was provided
-                invalid_params_error = FetchTracesSimpleComparison.validate_params(
+                invalid_params_error = validate_params(
                     params,
                     [
                         "service_name",
@@ -217,7 +405,7 @@
                 )
                 if invalid_params_error:
                     return StructuredToolResult(
-                        status=StructuredToolResultStatus.ERROR,
+                        status=ToolResultStatus.ERROR,
                         error=invalid_params_error,
                         params=params,
                     )
@@ -226,35 +414,30 @@
 
             sample_count = params.get("sample_count", 3)
 
-            start, end = BaseGrafanaTempoToolset.adjust_start_end_time(params)
+            start, end = process_timestamps_to_int(
+                params.get("start_datetime"),
+                params.get("end_datetime"),
+                default_time_span_seconds=DEFAULT_TRACES_TIME_SPAN_SECONDS,
+            )
 
-            # Create API instance
-            api = GrafanaTempoAPI(self._toolset.grafana_config)
+            base_url = get_base_url(self._toolset.grafana_config)
 
             # Step 1: Get all trace summaries
             stats_query = f"{{{base_query}}}"
-
-            # Debug log the query (useful for troubleshooting)
-            import logging
-
-            logger = logging.getLogger(__name__)
-            logger.debug(f"Tempo query: {stats_query}")
-
-            logger.debug(f"start: {start}, end: {end}")
-
-            all_traces_response = api.search_traces_by_query(
-                q=stats_query,
+            all_traces_response = query_tempo_traces(
+                base_url=base_url,
+                api_key=self._toolset.grafana_config.api_key,
+                headers=self._toolset.grafana_config.headers,
+                query=stats_query,
                 start=start,
                 end=end,
                 limit=1000,
             )
 
-            logger.debug(f"Response: {all_traces_response}")
-
             traces = all_traces_response.get("traces", [])
             if not traces:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.SUCCESS,
+                    status=ToolResultStatus.SUCCESS,
                     data="No traces found matching the query",
                     params=params,
                 )
@@ -299,33 +482,44 @@
                     return None
 
                 try:
-                    start_nano = trace_summary.get("startTimeUnixNano")
-                    trace_start = (
-                        int(int(start_nano) / 1_000_000_000) if start_nano else None
-                    )
-
-                    trace_data = api.query_trace_by_id_v2(
-                        trace_id=trace_id, start=trace_start
+                    url = f"{base_url}/api/traces/{trace_id}"
+                    response = requests.get(
+                        url,
+                        headers=build_headers(
+                            api_key=self._toolset.grafana_config.api_key,
+                            additional_headers=self._toolset.grafana_config.headers,
+                        ),
+                        timeout=5,
                     )
+                    response.raise_for_status()
                     return {
                         "traceID": trace_id,
                         "durationMs": trace_summary.get("durationMs", 0),
                         "rootServiceName": trace_summary.get(
                             "rootServiceName", "unknown"
                         ),
-                        "traceData": trace_data,  # Raw trace data
+                        "traceData": response.json(),  # Raw trace data
                     }
-                except Exception as e:
+                except requests.exceptions.RequestException as e:
                     error_msg = f"Failed to fetch full trace: {str(e)}"
+                    if hasattr(e, "response") and e.response is not None:
+                        error_msg += f" (Status: {e.response.status_code})"
                     return {
                         "traceID": trace_id,
                         "durationMs": trace_summary.get("durationMs", 0),
                         "error": error_msg,
                     }
+                except (ValueError, KeyError) as e:
+                    return {
+                        "traceID": trace_id,
+                        "durationMs": trace_summary.get("durationMs", 0),
+                        "error": f"Failed to parse trace data: {str(e)}",
+                    }
 
             # Fetch the selected traces
             result = {
                 "statistics": stats,
+                "all_trace_durations_ms": durations,  # All durations for distribution analysis
                 "fastest_traces": [
                     fetch_full_trace(sorted_traces[i]) for i in fastest_indices
                 ],
@@ -337,14 +531,14 @@
 
             # Return as YAML for readability
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=yaml.dump(result, default_flow_style=False, sort_keys=False),
                 params=params,
             )
 
         except Exception as e:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Error fetching traces: {str(e)}",
                 params=params,
             )
@@ -353,578 +547,18 @@
         return f"{toolset_name_for_one_liner(self._toolset.name)}: Simple Tempo Traces Comparison"
 
 
-class SearchTracesByQuery(Tool):
-    def __init__(self, toolset: BaseGrafanaTempoToolset):
-        super().__init__(
-            name="tempo_search_traces_by_query",
-            description=(
-                "Search for traces using TraceQL query language. "
-                "Uses the Tempo API endpoint: GET /api/search with 'q' parameter.\n\n"
-                "TraceQL can select traces based on:\n"
-                "- Span and resource attributes\n"
-                "- Timing and duration\n"
-                "- Aggregate functions:\n"
-                "   count() - Count number of spans\n"
-                "   avg(attribute) - Calculate average\n"
-                "   min(attribute) - Find minimum value\n"
-                "   max(attribute) - Find maximum value\n"
-                "   sum(attribute) - Sum values\n\n"
-                "Examples:\n"
-                '- Specific operation: {resource.service.name = "frontend" && name = "POST /api/orders"}\n'
-                '- Error traces: {resource.service.name="frontend" && name = "POST /api/orders" && status = error}\n'
-                '- HTTP errors: {resource.service.name="frontend" && name = "POST /api/orders" && span.http.status_code >= 500}\n'
-                '- Multi-service: {span.service.name="frontend" && name = "GET /api/products/{id}"} && {span.db.system="postgresql"}\n'
-                "- With aggregates: { status = error } | by(resource.service.name) | count() > 1"
-            ),
-            parameters={
-                "q": ToolParameter(
-                    description=(
-                        "TraceQL query. Supports filtering by span/resource attributes, "
-                        "duration, and aggregate functions (count(), avg(), min(), max(), sum()). "
-                        "Examples: '{resource.service.name = \"frontend\"}', "
-                        '\'{resource.service.name="frontend" && name = "POST /api/orders" && status = error}\', '
-                        '\'{resource.service.name="frontend" && name = "POST /api/orders" && span.http.status_code >= 500}\', '
-                        "'{} | count() > 10'"
-                    ),
-                    type="string",
-                    required=True,
-                ),
-                "limit": ToolParameter(
-                    description="Maximum number of traces to return",
-                    type="integer",
-                    required=False,
-                ),
-                "start": ToolParameter(
-                    description=standard_start_datetime_tool_param_description(
-                        DEFAULT_GRAPH_TIME_SPAN_SECONDS
-                    ),
-                    type="string",
-                    required=False,
-                ),
-                "end": ToolParameter(
-                    description=STANDARD_END_DATETIME_TOOL_PARAM_DESCRIPTION,
-                    type="string",
-                    required=False,
-                ),
-                "spss": ToolParameter(
-                    description="Spans per span set",
-                    type="integer",
-                    required=False,
-                ),
-            },
-        )
-        self._toolset = toolset
-
-    def _invoke(self, params: Dict, context: ToolInvokeContext) -> StructuredToolResult:
-        api = GrafanaTempoAPI(self._toolset.grafana_config)
-
-        start, end = BaseGrafanaTempoToolset.adjust_start_end_time(params)
-
-        try:
-            result = api.search_traces_by_query(
-                q=params["q"],
-                limit=params.get("limit"),
-                start=start,
-                end=end,
-                spss=params.get("spss"),
-            )
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=yaml.dump(result, default_flow_style=False),
-                params=params,
-            )
-        except Exception as e:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=str(e),
-                params=params,
-            )
-
-    def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"{toolset_name_for_one_liner(self._toolset.name)}: Searched traces with TraceQL"
-
-
-class SearchTracesByTags(Tool):
-    def __init__(self, toolset: BaseGrafanaTempoToolset):
-        super().__init__(
-            name="tempo_search_traces_by_tags",
-            description=(
-                "Search for traces using logfmt-encoded tags. "
-                "Uses the Tempo API endpoint: GET /api/search with 'tags' parameter. "
-                'Example: service.name="api" http.status_code="500"'
-            ),
-            parameters={
-                "tags": ToolParameter(
-                    description='Logfmt-encoded span/process attributes (e.g., \'service.name="api" http.status_code="500"\')',
-                    type="string",
-                    required=True,
-                ),
-                "min_duration": ToolParameter(
-                    description="Minimum trace duration (e.g., '5s', '100ms')",
-                    type="string",
-                    required=False,
-                ),
-                "max_duration": ToolParameter(
-                    description="Maximum trace duration (e.g., '10s', '1000ms')",
-                    type="string",
-                    required=False,
-                ),
-                "limit": ToolParameter(
-                    description="Maximum number of traces to return",
-                    type="integer",
-                    required=False,
-                ),
-                "start": ToolParameter(
-                    description=standard_start_datetime_tool_param_description(
-                        DEFAULT_GRAPH_TIME_SPAN_SECONDS
-                    ),
-                    type="string",
-                    required=False,
-                ),
-                "end": ToolParameter(
-                    description=STANDARD_END_DATETIME_TOOL_PARAM_DESCRIPTION,
-                    type="string",
-                    required=False,
-                ),
-                "spss": ToolParameter(
-                    description="Spans per span set",
-                    type="integer",
-                    required=False,
-                ),
-            },
-        )
-        self._toolset = toolset
-
-    def _invoke(self, params: Dict, context: ToolInvokeContext) -> StructuredToolResult:
-        api = GrafanaTempoAPI(self._toolset.grafana_config)
-
-        start, end = BaseGrafanaTempoToolset.adjust_start_end_time(params)
-
-        try:
-            result = api.search_traces_by_tags(
-                tags=params["tags"],
-                min_duration=params.get("min_duration"),
-                max_duration=params.get("max_duration"),
-                limit=params.get("limit"),
-                start=start,
-                end=end,
-                spss=params.get("spss"),
-            )
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=yaml.dump(result, default_flow_style=False),
-                params=params,
-            )
-        except Exception as e:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=str(e),
-                params=params,
-            )
-
-    def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"{toolset_name_for_one_liner(self._toolset.name)}: Searched traces with tags"
-
-
-class QueryTraceById(Tool):
-    def __init__(self, toolset: BaseGrafanaTempoToolset):
-        super().__init__(
-            name="tempo_query_trace_by_id",
-            description=(
-                "Retrieve detailed trace information by trace ID. "
-                "Uses the Tempo API endpoint: GET /api/v2/traces/{trace_id}. "
-                "Returns the full trace data in OpenTelemetry format."
-            ),
-            parameters={
-                "trace_id": ToolParameter(
-                    description="The unique trace ID to fetch",
-                    type="string",
-                    required=True,
-                ),
-                "start": ToolParameter(
-                    description=standard_start_datetime_tool_param_description(
-                        DEFAULT_GRAPH_TIME_SPAN_SECONDS
-                    ),
-                    type="string",
-                    required=False,
-                ),
-                "end": ToolParameter(
-                    description=STANDARD_END_DATETIME_TOOL_PARAM_DESCRIPTION,
-                    type="string",
-                    required=False,
-                ),
-            },
-        )
-        self._toolset = toolset
-
-    def _invoke(self, params: Dict, context: ToolInvokeContext) -> StructuredToolResult:
-        api = GrafanaTempoAPI(self._toolset.grafana_config)
-
-        start, end = BaseGrafanaTempoToolset.adjust_start_end_time(params)
-
-        try:
-            trace_data = api.query_trace_by_id_v2(
-                trace_id=params["trace_id"],
-                start=start,
-                end=end,
-            )
-
-            # Return raw trace data as YAML for readability
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=yaml.dump(trace_data, default_flow_style=False),
-                params=params,
-            )
-        except Exception as e:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=str(e),
-                params=params,
-            )
-
-    def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"{toolset_name_for_one_liner(self._toolset.name)}: Retrieved trace {params.get('trace_id')}"
-
-
-class SearchTagNames(Tool):
-    def __init__(self, toolset: BaseGrafanaTempoToolset):
-        super().__init__(
-            name="tempo_search_tag_names",
-            description=(
-                "Discover available tag names across traces. "
-                "Uses the Tempo API endpoint: GET /api/v2/search/tags. "
-                "Returns tags organized by scope (resource, span, intrinsic)."
-            ),
-            parameters={
-                "scope": ToolParameter(
-                    description="Filter by scope: 'resource', 'span', or 'intrinsic'",
-                    type="string",
-                    required=False,
-                ),
-                "q": ToolParameter(
-                    description="TraceQL query to filter tags (e.g., '{resource.cluster=\"us-east-1\"}')",
-                    type="string",
-                    required=False,
-                ),
-                "start": ToolParameter(
-                    description=standard_start_datetime_tool_param_description(
-                        DEFAULT_GRAPH_TIME_SPAN_SECONDS
-                    ),
-                    type="string",
-                    required=False,
-                ),
-                "end": ToolParameter(
-                    description=STANDARD_END_DATETIME_TOOL_PARAM_DESCRIPTION,
-                    type="string",
-                    required=False,
-                ),
-                "limit": ToolParameter(
-                    description="Maximum number of tag names to return",
-                    type="integer",
-                    required=False,
-                ),
-                "max_stale_values": ToolParameter(
-                    description="Maximum stale values parameter",
-                    type="integer",
-                    required=False,
-                ),
-            },
-        )
-        self._toolset = toolset
-
-    def _invoke(self, params: Dict, context: ToolInvokeContext) -> StructuredToolResult:
-        api = GrafanaTempoAPI(self._toolset.grafana_config)
-
-        start, end = BaseGrafanaTempoToolset.adjust_start_end_time(params)
-
-        try:
-            result = api.search_tag_names_v2(
-                scope=params.get("scope"),
-                q=params.get("q"),
-                start=start,
-                end=end,
-                limit=params.get("limit"),
-                max_stale_values=params.get("max_stale_values"),
-            )
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=yaml.dump(result, default_flow_style=False),
-                params=params,
-            )
-        except Exception as e:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=str(e),
-                params=params,
-            )
-
-    def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"{toolset_name_for_one_liner(self._toolset.name)}: Discovered tag names"
-
-
-class SearchTagValues(Tool):
-    def __init__(self, toolset: BaseGrafanaTempoToolset):
-        super().__init__(
-            name="tempo_search_tag_values",
-            description=(
-                "Get all values for a specific tag. "
-                "Uses the Tempo API endpoint: GET /api/v2/search/tag/{tag}/values. "
-                "Useful for discovering what values exist for a given tag."
-            ),
-            parameters={
-                "tag": ToolParameter(
-                    description="The tag name to get values for (e.g., 'resource.service.name', 'http.status_code')",
-                    type="string",
-                    required=True,
-                ),
-                "q": ToolParameter(
-                    description="TraceQL query to filter tag values (e.g., '{resource.cluster=\"us-east-1\"}')",
-                    type="string",
-                    required=False,
-                ),
-                "start": ToolParameter(
-                    description=standard_start_datetime_tool_param_description(
-                        DEFAULT_GRAPH_TIME_SPAN_SECONDS
-                    ),
-                    type="string",
-                    required=False,
-                ),
-                "end": ToolParameter(
-                    description=STANDARD_END_DATETIME_TOOL_PARAM_DESCRIPTION,
-                    type="string",
-                    required=False,
-                ),
-                "limit": ToolParameter(
-                    description="Maximum number of values to return",
-                    type="integer",
-                    required=False,
-                ),
-                "max_stale_values": ToolParameter(
-                    description="Maximum stale values parameter",
-                    type="integer",
-                    required=False,
-                ),
-            },
-        )
-        self._toolset = toolset
-
-    def _invoke(self, params: Dict, context: ToolInvokeContext) -> StructuredToolResult:
-        api = GrafanaTempoAPI(self._toolset.grafana_config)
-
-        start, end = BaseGrafanaTempoToolset.adjust_start_end_time(params)
-
-        try:
-            result = api.search_tag_values_v2(
-                tag=params["tag"],
-                q=params.get("q"),
-                start=start,
-                end=end,
-                limit=params.get("limit"),
-                max_stale_values=params.get("max_stale_values"),
-            )
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=yaml.dump(result, default_flow_style=False),
-                params=params,
-            )
-        except Exception as e:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=str(e),
-                params=params,
-            )
-
-    def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"{toolset_name_for_one_liner(self._toolset.name)}: Retrieved values for tag '{params.get('tag')}'"
-
-
-class QueryMetricsInstant(Tool):
-    def __init__(self, toolset: BaseGrafanaTempoToolset):
-        super().__init__(
-            name="tempo_query_metrics_instant",
-            description=(
-                "Compute a single TraceQL metric value across time range. "
-                "Uses the Tempo API endpoint: GET /api/metrics/query. "
-                "TraceQL metrics compute aggregated metrics from trace data. "
-                "Returns a single value for the entire time range. "
-                "Basic syntax: {selector} | function(attribute) [by (grouping)]\n\n"
-                "TraceQL metrics can help answer questions like:\n"
-                "- How many database calls across all systems are downstream of your application?\n"
-                "- What services beneath a given endpoint are failing?\n"
-                "- What services beneath an endpoint are slow?\n\n"
-                "TraceQL metrics help you answer these questions by parsing your traces in aggregate. "
-                "The instant version returns a single value for the query and is preferred over "
-                "query_metrics_range when you don't need the granularity of a full time-series but want "
-                "a total sum or single value computed across the whole time range."
-            ),
-            parameters={
-                "q": ToolParameter(
-                    description=(
-                        "TraceQL metrics query. Supported functions: rate, count_over_time, "
-                        "sum_over_time, max_over_time, min_over_time, avg_over_time, "
-                        "quantile_over_time, histogram_over_time, compare. "
-                        "Can use topk or bottomk modifiers. "
-                        "Syntax: {selector} | function(attribute) [by (grouping)]. "
-                        'Example: {resource.service.name="api"} | avg_over_time(duration)'
-                    ),
-                    type="string",
-                    required=True,
-                ),
-                "start": ToolParameter(
-                    description=standard_start_datetime_tool_param_description(
-                        DEFAULT_GRAPH_TIME_SPAN_SECONDS
-                    ),
-                    type="string",
-                    required=False,
-                ),
-                "end": ToolParameter(
-                    description=STANDARD_END_DATETIME_TOOL_PARAM_DESCRIPTION,
-                    type="string",
-                    required=False,
-                ),
-            },
-        )
-        self._toolset = toolset
-
-    def _invoke(self, params: Dict, context: ToolInvokeContext) -> StructuredToolResult:
-        api = GrafanaTempoAPI(self._toolset.grafana_config)
-
-        start, end = BaseGrafanaTempoToolset.adjust_start_end_time(params)
-
-        try:
-            result = api.query_metrics_instant(
-                q=params["q"],
-                start=start,
-                end=end,
-            )
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=yaml.dump(result, default_flow_style=False),
-                params=params,
-            )
-        except Exception as e:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=str(e),
-                params=params,
-            )
-
-    def get_parameterized_one_liner(self, params: Dict) -> str:
-        return (
-            f"{toolset_name_for_one_liner(self._toolset.name)}: Computed TraceQL metric"
-        )
-
-
-class QueryMetricsRange(Tool):
-    def __init__(self, toolset: BaseGrafanaTempoToolset):
-        super().__init__(
-            name="tempo_query_metrics_range",
-            description=(
-                "Get time series data from TraceQL metrics queries. "
-                "Uses the Tempo API endpoint: GET /api/metrics/query_range. "
-                "Returns metrics computed at regular intervals (controlled by 'step' parameter). "
-                "Use this for graphing metrics over time or analyzing trends. "
-                "Basic syntax: {selector} | function(attribute) [by (grouping)]\n\n"
-                "TraceQL metrics can help answer questions like:\n"
-                "- How many database calls across all systems are downstream of your application?\n"
-                "- What services beneath a given endpoint are failing?\n"
-                "- What services beneath an endpoint are slow?\n\n"
-                "TraceQL metrics help you answer these questions by parsing your traces in aggregate."
-            ),
-            parameters={
-                "q": ToolParameter(
-                    description=(
-                        "TraceQL metrics query. Supported functions: rate, count_over_time, "
-                        "sum_over_time, max_over_time, min_over_time, avg_over_time, "
-                        "quantile_over_time, histogram_over_time, compare. "
-                        "Can use topk or bottomk modifiers. "
-                        "Syntax: {selector} | function(attribute) [by (grouping)]. "
-                        'Example: {resource.service.name="api"} | avg_over_time(duration)'
-                    ),
-                    type="string",
-                    required=True,
-                ),
-                "step": ToolParameter(
-                    description="Time series granularity (e.g., '1m', '5m', '1h')",
-                    type="string",
-                    required=False,
-                ),
-                "start": ToolParameter(
-                    description=standard_start_datetime_tool_param_description(
-                        DEFAULT_GRAPH_TIME_SPAN_SECONDS
-                    ),
-                    type="string",
-                    required=False,
-                ),
-                "end": ToolParameter(
-                    description=STANDARD_END_DATETIME_TOOL_PARAM_DESCRIPTION,
-                    type="string",
-                    required=False,
-                ),
-                "exemplars": ToolParameter(
-                    description="Maximum number of exemplars to return",
-                    type="integer",
-                    required=False,
-                ),
-            },
-        )
-        self._toolset = toolset
-
-    def _invoke(self, params: Dict, context: ToolInvokeContext) -> StructuredToolResult:
-        api = GrafanaTempoAPI(self._toolset.grafana_config)
-
-        start, end = BaseGrafanaTempoToolset.adjust_start_end_time(params)
-
-        # Calculate appropriate step
-        step_param = params.get("step")
-        step_seconds = duration_string_to_seconds(step_param) if step_param else None
-        adjusted_step = adjust_step_for_max_points(
-            end - start,
-            int(MAX_GRAPH_POINTS),
-            step_seconds,
-        )
-        step = seconds_to_duration_string(adjusted_step)
-
-        try:
-            result = api.query_metrics_range(
-                q=params["q"],
-                step=step,
-                start=start,
-                end=end,
-                exemplars=params.get("exemplars"),
-            )
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=yaml.dump(result, default_flow_style=False),
-                params=params,
-            )
-        except Exception as e:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=str(e),
-                params=params,
-            )
-
-    def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"{toolset_name_for_one_liner(self._toolset.name)}: Retrieved TraceQL metrics time series"
-
-
 class GrafanaTempoToolset(BaseGrafanaTempoToolset):
     def __init__(self):
         super().__init__(
             name="grafana/tempo",
             description="Fetches kubernetes traces from Tempo",
             icon_url="https://grafana.com/static/assets/img/blog/tempo.png",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/grafanatempo/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/grafanatempo.html",
             tools=[
                 FetchTracesSimpleComparison(self),
-                SearchTracesByQuery(self),
-                SearchTracesByTags(self),
-                QueryTraceById(self),
-                SearchTagNames(self),
-                SearchTagValues(self),
-                QueryMetricsInstant(self),
-                QueryMetricsRange(self),
+                GetTempoTraces(self),
+                GetTempoTraceById(self),
+                GetTempoTags(self),
             ],
         )
         template_file_path = os.path.abspath(
diff -ur baseline-holmes/holmes/plugins/toolsets/grafana/toolset_grafana.py holmes2/holmes/plugins/toolsets/grafana/toolset_grafana.py
--- baseline-holmes/holmes/plugins/toolsets/grafana/toolset_grafana.py	2025-11-05 16:43:28.288975319 -0800
+++ holmes2/holmes/plugins/toolsets/grafana/toolset_grafana.py	2025-10-17 15:09:28.695390417 -0700
@@ -1,247 +1,106 @@
-import os
-from typing import ClassVar, Dict, Optional, Type, cast
-from urllib.parse import urljoin
-from abc import ABC
-from holmes.core.tools import (
-    StructuredToolResult,
-    Tool,
-    ToolInvokeContext,
-    ToolParameter,
-    StructuredToolResultStatus,
-)
+from typing import Dict, List
+from urllib.parse import urlencode, urljoin
+from holmes.core.tools import Tool, ToolParameter
 from holmes.plugins.toolsets.grafana.base_grafana_toolset import BaseGrafanaToolset
 import requests  # type: ignore
+import logging
 
-from holmes.plugins.toolsets.grafana.common import (
-    get_base_url,
-    GrafanaConfig,
-    build_headers,
-)
 from holmes.plugins.toolsets.utils import toolset_name_for_one_liner
 
 
-class GrafanaDashboardConfig(GrafanaConfig):
-    """Configuration specific to Grafana Dashboard toolset with api/health as default healthcheck"""
-
-    healthcheck: Optional[str] = "api/health"
-
-
-class GrafanaToolset(BaseGrafanaToolset):
-    config_class: ClassVar[Type[GrafanaDashboardConfig]] = GrafanaDashboardConfig
-
-    def __init__(self):
-        super().__init__(
-            name="grafana/dashboards",
-            description="Provides tools for interacting with Grafana dashboards",
-            icon_url="https://w7.pngwing.com/pngs/434/923/png-transparent-grafana-hd-logo-thumbnail.png",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/grafanadashboards/",
-            tools=[
-                SearchDashboards(self),
-                GetDashboardByUID(self),
-                GetHomeDashboard(self),
-                GetDashboardTags(self),
-            ],
-        )
-
-        self._load_llm_instructions_from_file(
-            os.path.dirname(__file__), "toolset_grafana_dashboard.jinja2"
-        )
-
-    @property
-    def grafana_config(self) -> GrafanaDashboardConfig:
-        return cast(GrafanaDashboardConfig, self._grafana_config)
-
-
-class BaseGrafanaTool(Tool, ABC):
-    """Base class for Grafana tools with common HTTP request functionality."""
-
-    def __init__(self, toolset: GrafanaToolset, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._toolset = toolset
-
-    def _make_grafana_request(
-        self,
-        endpoint: str,
-        params: dict,
-        query_params: Optional[Dict] = None,
-        timeout: int = 30,
-    ) -> StructuredToolResult:
-        """Make a GET request to Grafana API and return structured result.
-
-        Args:
-            endpoint: API endpoint path (e.g., "/api/search")
-            params: Original parameters passed to the tool
-            query_params: Optional query parameters for the request
-
-        Returns:
-            StructuredToolResult with the API response data
-        """
-        url = urljoin(get_base_url(self._toolset.grafana_config), endpoint)
-        headers = build_headers(
-            api_key=self._toolset.grafana_config.api_key,
-            additional_headers=self._toolset.grafana_config.headers,
-        )
-
-        response = requests.get(
-            url, headers=headers, params=query_params, timeout=timeout
-        )
-        response.raise_for_status()
-        data = response.json()
-
-        return StructuredToolResult(
-            status=StructuredToolResultStatus.SUCCESS,
-            data=data,
-            url=url,
-            params=params,
-        )
-
-
-class SearchDashboards(BaseGrafanaTool):
-    def __init__(self, toolset: GrafanaToolset):
+class ListAndBuildGrafanaDashboardURLs(Tool):
+    def __init__(self, toolset: BaseGrafanaToolset):
         super().__init__(
-            toolset=toolset,
-            name="grafana_search_dashboards",
-            description="Search for Grafana dashboards and folders using the /api/search endpoint",
+            name="list_and_build_grafana_dashboard_urls",
+            description="Lists all available Grafana dashboard urls",
             parameters={
-                "query": ToolParameter(
-                    description="Search text to filter dashboards",
-                    type="string",
-                    required=False,
-                ),
-                "tag": ToolParameter(
-                    description="Search dashboards by tag",
+                "cluster_name": ToolParameter(
+                    description="The cluster name. Defaults to None.",
                     type="string",
                     required=False,
                 ),
-                "type": ToolParameter(
-                    description="Filter by type: 'dash-folder' or 'dash-db'",
+                "namespace": ToolParameter(
+                    description="The namespace for filtering dashboards.",
                     type="string",
                     required=False,
                 ),
-                "dashboardIds": ToolParameter(
-                    description="List of dashboard IDs to filter (comma-separated)",
+                "node_name": ToolParameter(
+                    description="The node name to filter for node-related dashboards.",
                     type="string",
                     required=False,
                 ),
-                "dashboardUIDs": ToolParameter(
-                    description="List of dashboard UIDs to search for (comma-separated)",
+                "pod_name": ToolParameter(
+                    description="The pod name to filter dashboards.",
                     type="string",
                     required=False,
                 ),
-                "folderUIDs": ToolParameter(
-                    description="List of folder UIDs to search within (comma-separated)",
-                    type="string",
-                    required=False,
-                ),
-                "starred": ToolParameter(
-                    description="Return only starred dashboards",
-                    type="boolean",
-                    required=False,
-                ),
-                "limit": ToolParameter(
-                    description="Maximum results (default 1000, max 5000)",
-                    type="integer",
-                    required=False,
-                ),
-                "page": ToolParameter(
-                    description="Page number for pagination",
-                    type="integer",
-                    required=False,
-                ),
-            },
-        )
-
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
-        query_params = {}
-        if params.get("query"):
-            query_params["query"] = params["query"]
-        if params.get("tag"):
-            query_params["tag"] = params["tag"]
-        if params.get("type"):
-            query_params["type"] = params["type"]
-        if params.get("dashboardIds"):
-            # Check if dashboardIds also needs to be passed as multiple params
-            dashboard_ids = params["dashboardIds"].split(",")
-            query_params["dashboardIds"] = [
-                dashboard_id.strip()
-                for dashboard_id in dashboard_ids
-                if dashboard_id.strip()
-            ]
-        if params.get("dashboardUIDs"):
-            # Handle dashboardUIDs as a list - split comma-separated values
-            dashboard_uids = params["dashboardUIDs"].split(",")
-            query_params["dashboardUIDs"] = [
-                uid.strip() for uid in dashboard_uids if uid.strip()
-            ]
-        if params.get("folderUIDs"):
-            # Check if folderUIDs also needs to be passed as multiple params
-            folder_uids = params["folderUIDs"].split(",")
-            query_params["folderUIDs"] = [
-                uid.strip() for uid in folder_uids if uid.strip()
-            ]
-        if params.get("starred") is not None:
-            query_params["starred"] = str(params["starred"]).lower()
-        if params.get("limit"):
-            query_params["limit"] = params["limit"]
-        if params.get("page"):
-            query_params["page"] = params["page"]
-
-        return self._make_grafana_request("/api/search", params, query_params)
-
-    def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"{toolset_name_for_one_liner(self._toolset.name)}: Search Dashboards"
-
-
-class GetDashboardByUID(BaseGrafanaTool):
-    def __init__(self, toolset: GrafanaToolset):
-        super().__init__(
-            toolset=toolset,
-            name="grafana_get_dashboard_by_uid",
-            description="Get a dashboard by its UID using the /api/dashboards/uid/:uid endpoint",
-            parameters={
-                "uid": ToolParameter(
-                    description="The unique identifier of the dashboard",
-                    type="string",
-                    required=True,
-                ),
             },
         )
+        self._toolset = toolset
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
-        uid = params["uid"]
-        return self._make_grafana_request(f"/api/dashboards/uid/{uid}", params)
+    def _invoke(self, params: Dict) -> str:  # type: ignore
+        url = urljoin(
+            self._toolset._grafana_config.url, "/api/search?query=&type=dash-db"
+        )
+        headers = {"Authorization": f"Bearer {self._toolset._grafana_config.api_key}"}
+
+        try:
+            response = requests.get(url, headers=headers)
+            response.raise_for_status()
+            dashboards = response.json()
+            formatted_dashboards: List[str] = []
+            base_url = (
+                self._toolset._grafana_config.external_url
+                or self._toolset._grafana_config.url
+            )
+            for dash in dashboards:
+                dashboard_url = urljoin(
+                    base_url,
+                    f"/d/{dash['uid']}/{dash['uri'].split('/')[-1]}",
+                )
+
+                params_dict = {
+                    "var-cluster": params.get("cluster_name", ""),
+                    "var-namespace": params.get("namespace", ""),
+                    "var-pod": params.get("pod_name", ""),
+                    "var-node": params.get("node_name", ""),
+                    "var-datasource": self._toolset._grafana_config.grafana_datasource_uid,
+                    "refresh": "5s",
+                }
+
+                # If filtering for nodes, ensure only node-related dashboards are included
+                if params.get("node_name") and "node" not in dash["title"].lower():
+                    continue
+
+                # we add all params since if the dashboard isnt configured for a param it will ignore it if it is added
+                query_string = urlencode({k: v for k, v in params_dict.items() if v})
+                dashboard_url = (
+                    f"{dashboard_url}?{query_string}" if query_string else dashboard_url
+                )
+
+                formatted_dashboards.append(
+                    f"Title: {dash['title']}\nURL: {dashboard_url}\n"
+                )
+
+            return "\n".join(formatted_dashboards) or "No dashboards found."
+        except requests.RequestException as e:
+            logging.error(f"Error fetching dashboards: {str(e)}")
+            return f"Error fetching dashboards: {str(e)}"
 
     def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"{toolset_name_for_one_liner(self._toolset.name)}: Get Dashboard {params.get('uid', '')}"
-
-
-class GetHomeDashboard(BaseGrafanaTool):
-    def __init__(self, toolset: GrafanaToolset):
-        super().__init__(
-            toolset=toolset,
-            name="grafana_get_home_dashboard",
-            description="Get the home dashboard using the /api/dashboards/home endpoint",
-            parameters={},
+        return (
+            f"{toolset_name_for_one_liner(self._toolset.name)}: List Grafana Dashboards"
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
-        return self._make_grafana_request("/api/dashboards/home", params)
 
-    def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"{toolset_name_for_one_liner(self._toolset.name)}: Get Home Dashboard"
-
-
-class GetDashboardTags(BaseGrafanaTool):
-    def __init__(self, toolset: GrafanaToolset):
+class GrafanaToolset(BaseGrafanaToolset):
+    def __init__(self):
         super().__init__(
-            toolset=toolset,
-            name="grafana_get_dashboard_tags",
-            description="Get all tags used across dashboards using the /api/dashboards/tags endpoint",
-            parameters={},
+            name="grafana/grafana",
+            description="Provides tools for interacting with Grafana dashboards",
+            icon_url="https://w7.pngwing.com/pngs/434/923/png-transparent-grafana-hd-logo-thumbnail.png",
+            docs_url="",
+            tools=[
+                ListAndBuildGrafanaDashboardURLs(self),
+            ],
         )
-
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
-        return self._make_grafana_request("/api/dashboards/tags", params)
-
-    def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"{toolset_name_for_one_liner(self._toolset.name)}: Get Dashboard Tags"
diff -ur baseline-holmes/holmes/plugins/toolsets/grafana/trace_parser.py holmes2/holmes/plugins/toolsets/grafana/trace_parser.py
--- baseline-holmes/holmes/plugins/toolsets/grafana/trace_parser.py	2025-11-05 16:43:28.289976274 -0800
+++ holmes2/holmes/plugins/toolsets/grafana/trace_parser.py	2025-10-17 15:09:28.703644353 -0700
@@ -187,7 +187,7 @@
                 else "\n"
             )
             trace_str += f"\tstartTime={unix_nano_to_rfc3339(int(trace.get('startTimeUnixNano')))}"
-            trace_str += f" rootServiceName={trace.get('rootServiceName')}"
+            trace_str += f" rootServiceName={trace.get('trootServiceName')}"
             trace_str += f" rootTraceName={trace.get('rootTraceName')}"
             traces_str.append(trace_str)
         return "\n".join(traces_str)
diff -ur baseline-holmes/holmes/plugins/toolsets/helm.yaml holmes2/holmes/plugins/toolsets/helm.yaml
--- baseline-holmes/holmes/plugins/toolsets/helm.yaml	2025-11-05 16:43:28.290092315 -0800
+++ holmes2/holmes/plugins/toolsets/helm.yaml	2025-10-17 15:09:28.720505388 -0700
@@ -1,7 +1,7 @@
 toolsets:
   helm/core:
     description: "Read access to cluster's Helm charts and releases"
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/helm/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/helm.html"
     icon_url: "https://helm.sh/img/helm.svg"
     tags:
       - core
Only in holmes2/holmes/plugins/toolsets/internet: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/internet/internet.py holmes2/holmes/plugins/toolsets/internet/internet.py
--- baseline-holmes/holmes/plugins/toolsets/internet/internet.py	2025-11-05 16:43:28.290414856 -0800
+++ holmes2/holmes/plugins/toolsets/internet/internet.py	2025-10-17 15:09:28.778026815 -0700
@@ -6,7 +6,6 @@
 from requests import RequestException, Timeout  # type: ignore
 from holmes.core.tools import (
     Tool,
-    ToolInvokeContext,
     ToolParameter,
     Toolset,
     ToolsetTag,
@@ -16,7 +15,7 @@
 from bs4 import BeautifulSoup
 
 import requests  # type: ignore
-from holmes.core.tools import StructuredToolResult, StructuredToolResultStatus
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 from holmes.plugins.toolsets.utils import toolset_name_for_one_liner
 
 
@@ -187,7 +186,7 @@
             toolset=toolset,  # type: ignore
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         url: str = params["url"]
 
         additional_headers = (
@@ -198,7 +197,7 @@
         if not content:
             logging.error(f"Failed to retrieve content from {url}")
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Failed to retrieve content from {url}",
                 params=params,
             )
@@ -210,7 +209,7 @@
             content = html_to_markdown(content)
 
         return StructuredToolResult(
-            status=StructuredToolResultStatus.SUCCESS,
+            status=ToolResultStatus.SUCCESS,
             data=content,
             params=params,
         )
@@ -269,7 +268,7 @@
             tools=[
                 FetchWebpage(self),
             ],
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/internet/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/internet.html",
             tags=[
                 ToolsetTag.CORE,
             ],
diff -ur baseline-holmes/holmes/plugins/toolsets/internet/notion.py holmes2/holmes/plugins/toolsets/internet/notion.py
--- baseline-holmes/holmes/plugins/toolsets/internet/notion.py	2025-11-05 16:43:28.290601772 -0800
+++ holmes2/holmes/plugins/toolsets/internet/notion.py	2025-10-17 15:09:28.784100851 -0700
@@ -4,7 +4,6 @@
 from typing import Any, Dict, Tuple
 from holmes.core.tools import (
     Tool,
-    ToolInvokeContext,
     ToolParameter,
     ToolsetTag,
 )
@@ -14,7 +13,7 @@
 )
 from holmes.core.tools import (
     StructuredToolResult,
-    StructuredToolResultStatus,
+    ToolResultStatus,
 )
 from holmes.plugins.toolsets.utils import toolset_name_for_one_liner
 
@@ -45,7 +44,7 @@
             return f"https://api.notion.com/v1/blocks/{notion_id}/children"
         return url  # Return original URL if no match is found
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         url: str = params["url"]
 
         # Get headers from the toolset configuration
@@ -58,13 +57,13 @@
         if not content:
             logging.error(f"Failed to retrieve content from {url}")
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Failed to retrieve content from {url}",
                 params=params,
             )
 
         return StructuredToolResult(
-            status=StructuredToolResultStatus.SUCCESS,
+            status=ToolResultStatus.SUCCESS,
             data=self.parse_notion_content(content),
             params=params,
         )
@@ -119,7 +118,7 @@
             name="notion",
             description="Fetch notion webpages",
             icon_url="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Notion-logo.svg/2048px-Notion-logo.svg.png",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/notion/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/notion.html",
             tools=[
                 FetchNotion(self),
             ],
Only in holmes2/holmes/plugins/toolsets/investigator: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/investigator/core_investigation.py holmes2/holmes/plugins/toolsets/investigator/core_investigation.py
--- baseline-holmes/holmes/plugins/toolsets/investigator/core_investigation.py	2025-11-05 16:43:28.290912188 -0800
+++ holmes2/holmes/plugins/toolsets/investigator/core_investigation.py	2025-10-17 15:09:28.799323941 -0700
@@ -3,39 +3,20 @@
 from typing import Any, Dict
 
 from uuid import uuid4
-
 from holmes.core.todo_tasks_formatter import format_tasks
 from holmes.core.tools import (
-    StructuredToolResult,
-    StructuredToolResultStatus,
-    Tool,
-    ToolInvokeContext,
-    ToolParameter,
     Toolset,
     ToolsetTag,
+    ToolParameter,
+    Tool,
+    StructuredToolResult,
+    ToolResultStatus,
 )
 from holmes.plugins.toolsets.investigator.model import Task, TaskStatus
 
-TODO_WRITE_TOOL_NAME = "TodoWrite"
-
-
-def parse_tasks(todos_data: Any) -> list[Task]:
-    tasks = []
-
-    for todo_item in todos_data:
-        if isinstance(todo_item, dict):
-            task = Task(
-                id=todo_item.get("id", str(uuid4())),
-                content=todo_item.get("content", ""),
-                status=TaskStatus(todo_item.get("status", "pending")),
-            )
-            tasks.append(task)
-
-    return tasks
-
 
 class TodoWriteTool(Tool):
-    name: str = TODO_WRITE_TOOL_NAME
+    name: str = "TodoWrite"
     description: str = "Save investigation tasks to break down complex problems into manageable sub-tasks. ALWAYS provide the COMPLETE list of all tasks, not just the ones being updated."
     parameters: Dict[str, ToolParameter] = {
         "todos": ToolParameter(
@@ -47,11 +28,7 @@
                 properties={
                     "id": ToolParameter(type="string", required=True),
                     "content": ToolParameter(type="string", required=True),
-                    "status": ToolParameter(
-                        type="string",
-                        required=True,
-                        enum=["pending", "in_progress", "completed"],
-                    ),
+                    "status": ToolParameter(type="string", required=True),
                 },
             ),
         ),
@@ -80,28 +57,39 @@
         content_width = max(max_content_width, len("Content"))
         status_width = max(max_status_display_width, len("Status"))
 
+        # Build table
         separator = f"+{'-' * (id_width + 2)}+{'-' * (content_width + 2)}+{'-' * (status_width + 2)}+"
         header = f"| {'ID':<{id_width}} | {'Content':<{content_width}} | {'Status':<{status_width}} |"
-        tasks_to_display = []
+
+        # Log the table
+        logging.info("Updated Investigation Tasks:")
+        logging.info(separator)
+        logging.info(header)
+        logging.info(separator)
 
         for task in tasks:
             status_display = f"{status_icons[task.status.value]} {task.status.value}"
             row = f"| {task.id:<{id_width}} | {task.content:<{content_width}} | {status_display:<{status_width}} |"
-            tasks_to_display.append(row)
+            logging.info(row)
 
-        logging.info(
-            f"Task List:\n{separator}\n{header}\n{separator}\n"
-            + "\n".join(tasks_to_display)
-            + f"\n{separator}"
-        )
+        logging.info(separator)
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             todos_data = params.get("todos", [])
 
-            tasks = parse_tasks(todos_data=todos_data)
+            tasks = []
+
+            for todo_item in todos_data:
+                if isinstance(todo_item, dict):
+                    task = Task(
+                        id=todo_item.get("id", str(uuid4())),
+                        content=todo_item.get("content", ""),
+                        status=TaskStatus(todo_item.get("status", "pending")),
+                    )
+                    tasks.append(task)
 
-            logging.debug(f"Tasks: {len(tasks)}")
+            logging.info(f"Tasks: {len(tasks)}")
 
             self.print_tasks_table(tasks)
             formatted_tasks = format_tasks(tasks)
@@ -113,7 +101,7 @@
                 response_data += "No tasks currently in the investigation plan."
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=response_data,
                 params=params,
             )
@@ -121,13 +109,14 @@
         except Exception as e:
             logging.exception("error using todowrite tool")
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Failed to process tasks: {str(e)}",
                 params=params,
             )
 
     def get_parameterized_one_liner(self, params: Dict) -> str:
-        return "Update investigation tasks"
+        todos = params.get("todos", [])
+        return f"Write {todos} investigation tasks"
 
 
 class CoreInvestigationToolset(Toolset):
@@ -137,11 +126,12 @@
         super().__init__(
             name="core_investigation",
             description="Core investigation tools for task management and planning",
-            enabled=True,
+            enabled=False,  # Modified for KAITO compatibility - allow config override
             tools=[TodoWriteTool()],
             tags=[ToolsetTag.CORE],
-            is_default=True,
+            is_default=False,  # Modified for KAITO compatibility - allow config override
         )
+        logging.info("Core investigation toolset loaded")
 
     def get_example_config(self) -> Dict[str, Any]:
         return {}
diff -ur baseline-holmes/holmes/plugins/toolsets/kafka.py holmes2/holmes/plugins/toolsets/kafka.py
--- baseline-holmes/holmes/plugins/toolsets/kafka.py	2025-11-05 16:43:28.291294479 -0800
+++ holmes2/holmes/plugins/toolsets/kafka.py	2025-10-17 15:09:28.790358636 -0700
@@ -27,9 +27,8 @@
     CallablePrerequisite,
     StructuredToolResult,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     Toolset,
     ToolsetTag,
 )
@@ -154,13 +153,13 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             kafka_cluster_name = get_param_or_raise(params, "kafka_cluster_name")
             client = self.get_kafka_client(kafka_cluster_name)
             if client is None:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error="No admin_client on toolset. This toolset is misconfigured.",
                     params=params,
                 )
@@ -189,7 +188,7 @@
             if errors_text:
                 result_text = result_text + "\n\n" + errors_text
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=result_text,
                 params=params,
             )
@@ -197,7 +196,7 @@
             error_msg = f"Failed to list consumer groups: {str(e)}"
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
@@ -227,14 +226,14 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         group_id = params["group_id"]
         try:
             kafka_cluster_name = get_param_or_raise(params, "kafka_cluster_name")
             client = self.get_kafka_client(kafka_cluster_name)
             if client is None:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error="No admin_client on toolset. This toolset is misconfigured.",
                     params=params,
                 )
@@ -244,13 +243,13 @@
             if futures.get(group_id):
                 group_metadata = futures.get(group_id).result()
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.SUCCESS,
+                    status=ToolResultStatus.SUCCESS,
                     data=yaml.dump(convert_to_dict(group_metadata)),
                     params=params,
                 )
             else:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error="Group not found",
                     params=params,
                 )
@@ -258,7 +257,7 @@
             error_msg = f"Failed to describe consumer group {group_id}: {str(e)}"
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
@@ -283,20 +282,20 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             kafka_cluster_name = get_param_or_raise(params, "kafka_cluster_name")
             client = self.get_kafka_client(kafka_cluster_name)
             if client is None:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error="No admin_client on toolset. This toolset is misconfigured.",
                     params=params,
                 )
 
             topics = client.list_topics()
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=yaml.dump(convert_to_dict(topics)),
                 params=params,
             )
@@ -304,7 +303,7 @@
             error_msg = f"Failed to list topics: {str(e)}"
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
@@ -339,14 +338,14 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         topic_name = params["topic_name"]
         try:
             kafka_cluster_name = get_param_or_raise(params, "kafka_cluster_name")
             client = self.get_kafka_client(kafka_cluster_name)
             if client is None:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error="No admin_client on toolset. This toolset is misconfigured.",
                     params=params,
                 )
@@ -366,7 +365,7 @@
                 result["configuration"] = convert_to_dict(config)
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=yaml.dump(result),
                 params=params,
             )
@@ -374,7 +373,7 @@
             error_msg = f"Failed to describe topic {topic_name}: {str(e)}"
             logging.error(error_msg, exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
@@ -462,14 +461,14 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         topic_name = params["topic_name"]
         try:
             kafka_cluster_name = get_param_or_raise(params, "kafka_cluster_name")
             client = self.get_kafka_client(kafka_cluster_name)
             if client is None:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error="No admin_client on toolset. This toolset is misconfigured.",
                     params=params,
                 )
@@ -521,7 +520,7 @@
                 result_text = result_text + "\n\n" + errors_text
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=result_text,
                 params=params,
             )
@@ -531,7 +530,7 @@
             )
             logging.error(error_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=error_msg,
                 params=params,
             )
@@ -550,10 +549,10 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         cluster_names = list(self.toolset.clients.keys())
         return StructuredToolResult(
-            status=StructuredToolResultStatus.SUCCESS,
+            status=ToolResultStatus.SUCCESS,
             data="Available Kafka Clusters:\n" + "\n".join(cluster_names),
             params=params,
         )
@@ -572,7 +571,7 @@
             name="kafka/admin",
             description="Fetches metadata from multiple Kafka clusters",
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/kafka/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/kafka.html",
             icon_url="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT-cR1JrBgJxB_SPVKUIRwtiHnR8qBvLeHXjQ&s",
             tags=[ToolsetTag.CORE],
             tools=[
diff -ur baseline-holmes/holmes/plugins/toolsets/kubernetes_logs.py holmes2/holmes/plugins/toolsets/kubernetes_logs.py
--- baseline-holmes/holmes/plugins/toolsets/kubernetes_logs.py	2025-11-05 16:43:28.291667894 -0800
+++ holmes2/holmes/plugins/toolsets/kubernetes_logs.py	2025-10-17 15:09:28.676618105 -0700
@@ -10,7 +10,7 @@
 from holmes.core.tools import (
     StaticPrerequisite,
     StructuredToolResult,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     ToolsetTag,
 )
 from holmes.plugins.toolsets.logging_utils.logging_api import (
@@ -63,7 +63,7 @@
         super().__init__(
             name="kubernetes/logs",
             description="Read Kubernetes pod logs using a unified API",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/kubernetes/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/kubernetes.html#logs",
             icon_url="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRPKA-U9m5BxYQDF1O7atMfj9EMMXEoGu4t0Q&s",
             prerequisites=[prerequisite],
             is_default=True,
@@ -140,7 +140,7 @@
             # Ensure both results are not None (they should always be set by the loop)
             if current_logs_result is None or previous_logs_result is None:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error="Internal error: Failed to fetch logs",
                     params=params.model_dump(),
                 )
@@ -162,7 +162,7 @@
             ):
                 # Both commands failed - return error from current logs
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error=current_logs_result.error,
                     params=params.model_dump(),
                     return_code=return_code,
@@ -206,7 +206,7 @@
             if len(filtered_logs) == 0:
                 # Return NO_DATA status when there are no logs
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.NO_DATA,
+                    status=ToolResultStatus.NO_DATA,
                     data="\n".join(
                         metadata_lines
                     ),  # Still include metadata for context
@@ -218,7 +218,7 @@
             response_data = formatted_logs + "\n" + "\n".join(metadata_lines)
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=response_data,
                 params=params.model_dump(),
                 return_code=return_code,
@@ -226,7 +226,7 @@
         except Exception as e:
             logging.exception(f"Error fetching logs for pod {params.pod_name}")
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Error fetching logs: {str(e)}",
                 params=params.model_dump(),
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/kubernetes_logs.yaml holmes2/holmes/plugins/toolsets/kubernetes_logs.yaml
--- baseline-holmes/holmes/plugins/toolsets/kubernetes_logs.yaml	2025-11-05 16:43:28.291884435 -0800
+++ holmes2/holmes/plugins/toolsets/kubernetes_logs.yaml	2025-10-17 15:09:28.772407901 -0700
@@ -1,17 +1,13 @@
 toolsets:
   kubernetes/logs:
     description: "Read pod logs"
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/kubernetes/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/kubernetes.html#logs"
     icon_url: "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRPKA-U9m5BxYQDF1O7atMfj9EMMXEoGu4t0Q&s"
     tags:
       - core
     prerequisites:
       - command: "kubectl version --client"
 
-    # Note: Log tools use transformers with llm_summarize to automatically
-    # summarize large log outputs when a fast model is configured. This helps
-    # focus on errors, patterns, and key information while reducing context usage.
-
     tools:
       - name: "kubectl_previous_logs"
         description: "Run `kubectl logs --previous` on a single Kubernetes pod. Used to fetch logs for a pod that crashed and see logs from before the crash. Never give a deployment name or a resource that is not a pod."
@@ -28,38 +24,10 @@
       - name: "kubectl_logs"
         description: "Run `kubectl logs` on a single Kubernetes pod. Never give a deployment name or a resource that is not a pod."
         command: "kubectl logs {{pod_name}} -n {{ namespace }}"
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 1000
-              prompt: |
-                Summarize these pod logs focusing on:
-                - Errors, exceptions, and warning messages
-                - Recent activity patterns and trends
-                - Any authentication, connection, or startup issues
-                - Performance indicators (response times, throughput)
-                - Group similar log entries together
-                - When possible, mention exact error codes or keywords for easier searching
-                - Be concise: aim for  50% of the original text; prioritize aggregates and actionable outliers
-                - Include grep-ready keys/values; avoid repeating entire logs or unchanged defaults
 
       - name: "kubectl_logs_all_containers"
         description: "Run `kubectl logs` on all containers within a single Kubernetes pod."
         command: "kubectl logs {{pod_name}} -n {{ namespace }} --all-containers"
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 1000
-              prompt: |
-                Summarize these multi-container pod logs focusing on:
-                - Errors, exceptions, and warning messages by container
-                - Inter-container communication patterns
-                - Any authentication, connection, or startup issues
-                - Performance indicators and resource usage patterns
-                - Group similar log entries together by container
-                - When possible, mention exact error codes or keywords for easier searching
-                - Strive for  50% of the original size; keep results compact and grep-friendly (one line per aggregate)
-                - Prioritize aggregates and actionable outliers over comprehensive details
 
       - name: "kubectl_container_logs"
         description: "Run `kubectl logs` on a single container within a Kubernetes pod. This is to get the logs of a specific container in a multi-container pod."
diff -ur baseline-holmes/holmes/plugins/toolsets/kubernetes.yaml holmes2/holmes/plugins/toolsets/kubernetes.yaml
--- baseline-holmes/holmes/plugins/toolsets/kubernetes.yaml	2025-11-05 16:43:28.291533645 -0800
+++ holmes2/holmes/plugins/toolsets/kubernetes.yaml	2025-10-17 15:09:28.681091528 -0700
@@ -1,17 +1,13 @@
 toolsets:
   kubernetes/core:
     description: "Read access to cluster resources (excluding secrets and other sensitive data)"
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/kubernetes/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/kubernetes.html#core"
     icon_url: "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRPKA-U9m5BxYQDF1O7atMfj9EMMXEoGu4t0Q&s"
     tags:
       - core
     prerequisites:
       - command: "kubectl version --client"
 
-    # Note: Many tools in this toolset use transformers with llm_summarize
-    # to automatically summarize large kubectl outputs when a fast model is configured.
-    # This reduces context window usage while preserving key information for debugging.
-
     tools:
       - name: "kubectl_describe"
         description: >
@@ -21,20 +17,6 @@
             - 'describe pod xyz-123'
             - 'show service xyz-123 in namespace my-ns'
         command: "kubectl describe {{ kind }} {{ name }}{% if namespace %} -n {{ namespace }}{% endif %}"
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 1000
-              prompt: |
-                Summarize this kubectl describe output focusing on:
-                - What needs attention or immediate action
-                - Resource status and health indicators
-                - Any errors, warnings, or non-standard states
-                - Key configuration details that could affect functionality
-                - When possible, mention exact field names so the user can grep for specific details
-                - Be concise: aim for  50% of the original length; avoid repeating defaults/healthy/unchanged details
-                - Prefer aggregates and counts; list only outliers and actionable items
-                - Keep grep-friendly: include exact field names/values that matter
 
       - name: "kubectl_get_by_name"
         description: "Run `kubectl get <kind> <name> --show-labels`"
@@ -43,36 +25,10 @@
       - name: "kubectl_get_by_kind_in_namespace"
         description: "Run `kubectl get <kind> -n <namespace> --show-labels` to get all resources of a given type in namespace"
         command: "kubectl get --show-labels -o wide {{ kind }} -n {{namespace}}"
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 1000
-              prompt: |
-                Summarize this kubectl output focusing on:
-                - What needs attention or immediate action
-                - Group similar resources into aggregate descriptions
-                - Make sure to mention outliers, errors, and non-standard states
-                - List healthy resources as aggregate descriptions
-                - When listing unhealthy resources, also try to use aggregate descriptions when possible
-                - When possible, mention exact keywords so the user can rerun the command with | grep <keyword> and drill down
-                - Be concise and avoid expansion: target  50% of input size; prefer counts + outliers over full listings
 
       - name: "kubectl_get_by_kind_in_cluster"
         description: "Run `kubectl get -A <kind> --show-labels` to get all resources of a given type in the cluster"
         command: "kubectl get -A --show-labels -o wide {{ kind }}"
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 1000
-              prompt: |
-                Summarize this kubectl output focusing on:
-                - What needs attention or immediate action
-                - Group similar resources into a single line and description
-                - Make sure to mention outliers, errors, and non-standard states
-                - List healthy resources as aggregate descriptions
-                - When listing unhealthy resources, also try to use aggregate descriptions when possible
-                - When possible, mention exact keywords so the user can rerun the command with | grep <keyword> and drill down on the parts they care about
-                - Strive for  50% of the original size; keep results compact and grep-friendly (one line per aggregate)
 
       - name: "kubectl_find_resource"
         description: "Run `kubectl get {{ kind }} -A --show-labels | grep {{ keyword }}` to find a resource where you know a substring of the name, IP, namespace, or labels"
@@ -86,25 +42,95 @@
         description: "Retrieve the events for a specific Kubernetes resource. `resource_type` can be any kubernetes resource type: 'pod', 'service', 'deployment', 'job', 'node', etc."
         command: "kubectl events --for {{resource_type}}/{{ resource_name }}{% if namespace %} -n {{ namespace }}{% endif %}"
 
+      - name: "kubectl_memory_requests_all_namespaces"
+        description: "Fetch and display memory requests for all pods across all namespaces in MiB, summing requests across multiple containers where applicable and handling binary, decimal, and millibyte units correctly."
+        command: |
+          kubectl get pods --all-namespaces -o custom-columns="NAMESPACE:.metadata.namespace,NAME:.metadata.name,MEMORY_REQUEST:.spec.containers[*].resources.requests.memory" --no-headers | \
+          awk '
+            function convert_to_mib(value) {
+              if (value ~ /^[0-9]+e[0-9]+$/) return (value + 0) / (1024 * 1024); # Scientific notation
+              if (value ~ /m$/) return (value + 0) / (1024^2 * 1000);           # Millibytes (m)
+              if (value ~ /Ei$/) return (value + 0) * 1024^6 / (1024^2);        # Binary units
+              if (value ~ /Pi$/) return (value + 0) * 1024^5 / (1024^2);
+              if (value ~ /Ti$/) return (value + 0) * 1024^4 / (1024^2);
+              if (value ~ /Gi$/) return (value + 0) * 1024^3 / (1024^2);
+              if (value ~ /Mi$/) return (value + 0);
+              if (value ~ /Ki$/) return (value + 0) / 1024;
+              if (value ~ /E$/) return (value + 0) * 1000^6 / (1024^2);         # Decimal units
+              if (value ~ /P$/) return (value + 0) * 1000^5 / (1024^2);
+              if (value ~ /T$/) return (value + 0) * 1000^4 / (1024^2);
+              if (value ~ /G$/) return (value + 0) * 1000^3 / (1024^2);
+              if (value ~ /M$/) return (value + 0) * 1000^2 / (1024^2);
+              if (value ~ /k$/) return (value + 0) * 1000 / (1024^2);
+              return (value + 0) / (1024 * 1024);                               # Default: bytes
+            }
+            function sum_memory(requests) {
+              gsub(/^[ \t]+|[ \t]+$/, "", requests);
+              if (requests == "" || requests == "<none>") return 0;
+              split(requests, arr, ",");
+              total = 0;
+              for (i in arr) {
+                if (arr[i] != "<none>") total += convert_to_mib(arr[i]);
+              }
+              return total;
+            }
+            {
+              namespace = $1;
+              name = $2;
+              requests = $3;
+              for (i=4; i<=NF; i++) {
+                requests = requests " " $i;
+              }
+              print namespace, name, sum_memory(requests) " Mi";
+            }' | sort -k3 -nr
+
+      - name: "kubectl_memory_requests_namespace"
+        description: "Fetch and display memory requests for all pods in a specified namespace in MiB, summing requests across multiple containers where applicable and handling binary, decimal, and millibyte units correctly."
+        command: |
+          kubectl get pods -n {{ namespace }} -o custom-columns="NAMESPACE:.metadata.namespace,NAME:.metadata.name,MEMORY_REQUEST:.spec.containers[*].resources.requests.memory" --no-headers | \
+          awk '
+            function convert_to_mib(value) {
+              if (value ~ /^[0-9]+e[0-9]+$/) return (value + 0) / (1024 * 1024); # Scientific notation
+              if (value ~ /m$/) return (value + 0) / (1024^2 * 1000);           # Millibytes (m)
+              if (value ~ /Ei$/) return (value + 0) * 1024^6 / (1024^2);        # Binary units
+              if (value ~ /Pi$/) return (value + 0) * 1024^5 / (1024^2);
+              if (value ~ /Ti$/) return (value + 0) * 1024^4 / (1024^2);
+              if (value ~ /Gi$/) return (value + 0) * 1024^3 / (1024^2);
+              if (value ~ /Mi$/) return (value + 0);
+              if (value ~ /Ki$/) return (value + 0) / 1024;
+              if (value ~ /E$/) return (value + 0) * 1000^6 / (1024^2);         # Decimal units
+              if (value ~ /P$/) return (value + 0) * 1000^5 / (1024^2);
+              if (value ~ /T$/) return (value + 0) * 1000^4 / (1024^2);
+              if (value ~ /G$/) return (value + 0) * 1000^3 / (1024^2);
+              if (value ~ /M$/) return (value + 0) * 1000^2 / (1024^2);
+              if (value ~ /k$/) return (value + 0) * 1000 / (1024^2);
+              return (value + 0) / (1024 * 1024);                               # Default: bytes
+            }
+            function sum_memory(requests) {
+              gsub(/^[ \t]+|[ \t]+$/, "", requests);
+              if (requests == "" || requests == "<none>") return 0;
+              split(requests, arr, ",");
+              total = 0;
+              for (i in arr) {
+                if (arr[i] != "<none>") total += convert_to_mib(arr[i]);
+              }
+              return total;
+            }
+            {
+              namespace = $1;
+              name = $2;
+              requests = $3;
+              for (i=4; i<=NF; i++) {
+                requests = requests " " $i;
+              }
+              print namespace, name, sum_memory(requests) " Mi";
+            }' | sort -k3 -nr
+
       - name: "kubernetes_jq_query"
         user_description: "Query Kubernetes Resources: kubectl get {{kind}} --all-namespaces -o json | jq -r {{jq_expr}}"
         description: >
           Use kubectl to get json for all resources of a specific kind pipe the results to jq to filter them. Do not worry about escaping the jq_expr it will be done by the system on an unescaped expression that you give. e.g. give an expression like .items[] | .spec.containers[].image | select(test("^gcr.io/") | not)
         command: kubectl get {{ kind }} --all-namespaces -o json | jq -r {{ jq_expr }}
-        transformers:
-          - name: llm_summarize
-            config:
-              input_threshold: 1000
-              prompt: |
-                Summarize this jq query output focusing on:
-                - Key patterns and commonalities in the data
-                - Notable outliers, anomalies, or items that need attention
-                - Group similar results into aggregate descriptions when possible
-                - Highlight any empty results, null values, or missing data
-                - When applicable, mention specific resource names, namespaces, or values that stand out
-                - Organize findings in a structured way that helps with troubleshooting
-                - Be concise: aim for  50% of the original text; prioritize aggregates and actionable outliers
-                - Include grep-ready keys/values; avoid repeating entire objects or unchanged defaults
 
       - name: "kubernetes_count"
         user_description: "Count Kubernetes Resources: kubectl get {{kind}} --all-namespaces -o json | jq -c -r {{ jq_expr }}"
@@ -171,7 +197,7 @@
 
   kubernetes/live-metrics:
     description: "Provides real-time metrics for pods and nodes"
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/kubernetes/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/kubernetes.html#live-metrics"
     icon_url: "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRPKA-U9m5BxYQDF1O7atMfj9EMMXEoGu4t0Q&s"
     llm_instructions: |
       The kubectl_top_pods or kubectl_top_nodes do not return time series data or metrics that can be used for graphs
@@ -193,7 +219,7 @@
 
   kubernetes/kube-prometheus-stack:
     description: "Fetches prometheus definition"
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/kubernetes/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/kubernetes.html#prometheus-stack"
     icon_url: "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRPKA-U9m5BxYQDF1O7atMfj9EMMXEoGu4t0Q&s"
     tags:
       - core
@@ -204,7 +230,7 @@
 
   kubernetes/krew-extras: # To make this work, install kube-lineage with krew
     description: "Fetches children/dependents and parents/dependencies resources using kube-lineage installed via `kubectl krew`"
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/kubernetes/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/kubernetes.html#resource-lineage-extras-with-krew"
     icon_url: "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRPKA-U9m5BxYQDF1O7atMfj9EMMXEoGu4t0Q&s"
     tags:
       - cli
@@ -220,7 +246,7 @@
 
   kubernetes/kube-lineage-extras: # To make this work, build kube-lineage from source
     description: "Fetches children/dependents and parents/dependencies resources using kube-lineage"
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/kubernetes/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/kubernetes.html#resource-lineage-extras"
     icon_url: "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRPKA-U9m5BxYQDF1O7atMfj9EMMXEoGu4t0Q&s"
     tags:
       - cluster
Only in holmes2/holmes/plugins/toolsets/logging_utils: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/logging_utils/logging_api.py holmes2/holmes/plugins/toolsets/logging_utils/logging_api.py
--- baseline-holmes/holmes/plugins/toolsets/logging_utils/logging_api.py	2025-11-05 16:43:28.292182768 -0800
+++ holmes2/holmes/plugins/toolsets/logging_utils/logging_api.py	2025-10-17 15:09:28.680350909 -0700
@@ -1,36 +1,26 @@
 from abc import ABC, abstractmethod
 from datetime import datetime, timedelta
 import logging
-from math import ceil
 from typing import Optional, Set
 from enum import Enum
 
 from pydantic import BaseModel, field_validator
 from datetime import timezone
-from holmes.core.llm import LLM
 from holmes.core.tools import (
     StructuredToolResult,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
     Toolset,
 )
-from holmes.core.tools_utils.token_counting import count_tool_response_tokens
 from holmes.plugins.toolsets.utils import get_param_or_raise
 
 # Default values for log fetching
 DEFAULT_LOG_LIMIT = 100
 SECONDS_PER_DAY = 24 * 60 * 60
 DEFAULT_TIME_SPAN_SECONDS = 7 * SECONDS_PER_DAY  # 1 week in seconds
-DEFAULT_GRAPH_TIME_SPAN_SECONDS = 1 * 60 * 60  # 1 hour in seconds
 
 POD_LOGGING_TOOL_NAME = "fetch_pod_logs"
 
-TRUNCATION_PROMPT_PREFIX = "[... PREVIOUS LOGS ABOVE THIS LINE HAVE BEEN TRUNCATED]"
-MIN_NUMBER_OF_CHARACTERS_TO_TRUNCATE: int = (
-    50 + len(TRUNCATION_PROMPT_PREFIX)
-)  # prevents the truncation algorithm from going too slow once the actual token count gets close to the expected limit
-
 
 class LoggingCapability(str, Enum):
     """Optional advanced logging capabilities"""
@@ -83,68 +73,6 @@
         return ""
 
 
-def truncate_logs(
-    logging_structured_tool_result: StructuredToolResult,
-    llm: LLM,
-    token_limit: int,
-    structured_params: FetchPodLogsParams,
-):
-    original_token_count = count_tool_response_tokens(
-        llm=llm, structured_tool_result=logging_structured_tool_result
-    )
-    token_count = original_token_count
-    text = None
-    while token_count > token_limit:
-        # Loop because we are counting tokens but trimming characters. This means we try to trim a number of
-        # characters proportional to the number of tokens but we may still have too many tokens
-        if not text:
-            text = logging_structured_tool_result.get_stringified_data()
-        if not text:
-            # Weird scenario where the result exceeds the token allowance but there is not data.
-            # Exit and do nothing because I don't know how to handle such scenario.
-            logging.warning(
-                f"The calculated token count for logs is {token_count} but the limit is {token_limit}. However the data field is empty so there are no logs to truncate."
-            )
-            return
-        ratio = token_count / token_limit
-        character_count = len(text)
-        number_of_characters_to_truncate = character_count - ceil(
-            character_count / ratio
-        )
-        number_of_characters_to_truncate = max(
-            MIN_NUMBER_OF_CHARACTERS_TO_TRUNCATE, number_of_characters_to_truncate
-        )
-
-        if len(text) <= number_of_characters_to_truncate:
-            logging.warning(
-                f"The calculated token count for logs is {token_count} (max allowed tokens={token_limit}) but the logs are only {len(text)} characters which is below the intended truncation of {number_of_characters_to_truncate} characters. Logs will no longer be truncated"
-            )
-            return
-        else:
-            linefeed_truncation_offset = max(
-                text[number_of_characters_to_truncate:].find("\n"), 0
-            )  # keep log lines atomic
-
-            # Tentatively add the truncation prefix.
-            # When counting tokens, we want to include the TRUNCATION_PROMPT_PREFIX because it will be part of the tool response.
-            # Because we're truncating based on character counts but ultimately checking tokens count,
-            # it is possible that the character truncation is incorrect and more need to be truncated.
-            # This will be caught in the next iteration and the truncation prefix will be truncated
-            # because MIN_NUMBER_OF_CHARACTERS_TO_TRUNCATE cannot be smaller than TRUNCATION_PROMPT_PREFIX
-            text = (
-                TRUNCATION_PROMPT_PREFIX
-                + text[number_of_characters_to_truncate + linefeed_truncation_offset :]
-            )
-            logging_structured_tool_result.data = text
-            token_count = count_tool_response_tokens(
-                llm=llm, structured_tool_result=logging_structured_tool_result
-            )
-    if token_count < original_token_count:
-        logging.info(
-            f"Logs for pod {structured_params.pod_name}/{structured_params.namespace} have been truncated from {original_token_count} tokens down to {token_count} tokens."
-        )
-
-
 class PodLoggingTool(Tool):
     """Common tool for fetching pod logs across different logging backends"""
 
@@ -246,7 +174,7 @@
 
         return params
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: dict) -> StructuredToolResult:
         structured_params = FetchPodLogsParams(
             namespace=get_param_or_raise(params, "namespace"),
             pod_name=get_param_or_raise(params, "pod_name"),
@@ -261,13 +189,6 @@
             params=structured_params,
         )
 
-        truncate_logs(
-            logging_structured_tool_result=result,
-            llm=context.llm,
-            token_limit=context.max_token_count,
-            structured_params=structured_params,
-        )
-
         return result
 
     def get_parameterized_one_liner(self, params: dict) -> str:
Only in holmes2/holmes/plugins/toolsets/mcp: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/mcp/toolset_mcp.py holmes2/holmes/plugins/toolsets/mcp/toolset_mcp.py
--- baseline-holmes/holmes/plugins/toolsets/mcp/toolset_mcp.py	2025-11-05 16:43:28.292498850 -0800
+++ holmes2/holmes/plugins/toolsets/mcp/toolset_mcp.py	2025-10-17 15:09:28.714288686 -0700
@@ -1,10 +1,9 @@
 from holmes.core.tools import (
-    ToolInvokeContext,
     Toolset,
     Tool,
     ToolParameter,
     StructuredToolResult,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     CallablePrerequisite,
 )
 
@@ -25,12 +24,12 @@
     url: str
     headers: Optional[Dict[str, str]] = None
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             return asyncio.run(self._invoke_async(params))
         except Exception as e:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=str(e.args),
                 params=params,
                 invocation=f"MCPtool {self.name} with params {params}",
@@ -47,9 +46,9 @@
                 )
                 return StructuredToolResult(
                     status=(
-                        StructuredToolResultStatus.ERROR
+                        ToolResultStatus.ERROR
                         if tool_result.isError
-                        else StructuredToolResultStatus.SUCCESS
+                        else ToolResultStatus.SUCCESS
                     ),
                     data=merged_text,
                     params=params,
Only in baseline-holmes/holmes/plugins/toolsets: newrelic
Only in holmes2/holmes/plugins/toolsets: newrelic.py
Only in holmes2/holmes/plugins/toolsets/opensearch: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/opensearch/opensearch_logs.py holmes2/holmes/plugins/toolsets/opensearch/opensearch_logs.py
--- baseline-holmes/holmes/plugins/toolsets/opensearch/opensearch_logs.py	2025-11-05 16:43:28.293504722 -0800
+++ holmes2/holmes/plugins/toolsets/opensearch/opensearch_logs.py	2025-10-17 15:09:28.689732503 -0700
@@ -8,7 +8,7 @@
 from holmes.core.tools import (
     CallablePrerequisite,
     StructuredToolResult,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     ToolsetTag,
 )
 from holmes.plugins.toolsets.logging_utils.logging_api import (
@@ -42,7 +42,7 @@
         super().__init__(
             name="opensearch/logs",
             description="OpenSearch integration to fetch logs",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/opensearch-logs/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/opensearch_logs.html",
             icon_url="https://opensearch.org/wp-content/uploads/2025/01/opensearch_mark_default.png",
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
             tools=[],  # Initialize with empty tools first
@@ -79,7 +79,7 @@
     def fetch_pod_logs(self, params: FetchPodLogsParams) -> StructuredToolResult:
         if not self.opensearch_config:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="Missing OpenSearch configuration",
                 params=params.model_dump(),
             )
@@ -126,13 +126,13 @@
                     config=self.opensearch_config,
                 )
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.SUCCESS,
+                    status=ToolResultStatus.SUCCESS,
                     data=logs,
                     params=params.model_dump(),
                 )
             else:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     return_code=logs_response.status_code,
                     error=logs_response.text,
                     params=params.model_dump(),
@@ -141,21 +141,21 @@
         except requests.Timeout:
             logging.warning("Timeout while fetching OpenSearch logs", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="Request timed out while fetching OpenSearch logs",
                 params=params.model_dump(),
             )
         except RequestException as e:
             logging.warning("Failed to fetch OpenSearch logs", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Network error while fetching OpenSearch logs: {str(e)}",
                 params=params.model_dump(),
             )
         except Exception as e:
             logging.warning("Failed to process OpenSearch logs", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Unexpected error: {str(e)}",
                 params=params.model_dump(),
             )
Only in baseline-holmes/holmes/plugins/toolsets/opensearch: opensearch_ppl_query_docs.jinja2
Only in baseline-holmes/holmes/plugins/toolsets/opensearch: opensearch_query_assist_instructions.jinja2
Only in baseline-holmes/holmes/plugins/toolsets/opensearch: opensearch_query_assist.py
diff -ur baseline-holmes/holmes/plugins/toolsets/opensearch/opensearch_traces.py holmes2/holmes/plugins/toolsets/opensearch/opensearch_traces.py
--- baseline-holmes/holmes/plugins/toolsets/opensearch/opensearch_traces.py	2025-11-05 16:43:28.294062554 -0800
+++ holmes2/holmes/plugins/toolsets/opensearch/opensearch_traces.py	2025-10-17 15:09:28.689229298 -0700
@@ -1,13 +1,13 @@
 import os
 import logging
 
+from typing import Any, Dict
 
 import requests  # type: ignore
 from cachetools import TTLCache  # type: ignore
 from holmes.core.tools import (
     CallablePrerequisite,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
     ToolsetTag,
 )
@@ -19,8 +19,8 @@
     add_auth_header,
     get_search_url,
 )
-from holmes.core.tools import StructuredToolResult, StructuredToolResultStatus
-from holmes.plugins.toolsets.utils import get_param_or_raise, toolset_name_for_one_liner
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
+from holmes.plugins.toolsets.utils import toolset_name_for_one_liner
 
 TRACES_FIELDS_CACHE_KEY = "cached_traces_fields"
 
@@ -35,7 +35,7 @@
         self._toolset = toolset
         self._cache = None
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             if not self._cache and self._toolset.opensearch_config.fields_ttl_seconds:
                 self._cache = TTLCache(
@@ -47,7 +47,7 @@
                 if cached_response:
                     logging.debug("traces fields returned from cache")
                     return StructuredToolResult(
-                        status=StructuredToolResultStatus.SUCCESS,
+                        status=ToolResultStatus.SUCCESS,
                         data=cached_response,
                         params=params,
                     )
@@ -80,7 +80,7 @@
             if self._cache:
                 self._cache[TRACES_FIELDS_CACHE_KEY] = response
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=response,
                 params=params,
             )
@@ -89,21 +89,21 @@
                 "Timeout while fetching opensearch traces fields", exc_info=True
             )
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="Request timed out while fetching opensearch traces fields",
                 params=params,
             )
         except RequestException as e:
             logging.warning("Failed to fetch opensearch traces fields", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Network error while opensearch traces fields: {str(e)}",
                 params=params,
             )
         except Exception as e:
             logging.warning("Failed to process opensearch traces fields", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Unexpected error: {str(e)}",
                 params=params,
             )
@@ -128,10 +128,10 @@
         self._toolset = toolset
         self._cache = None
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         err_msg = ""
         try:
-            body = json.loads(get_param_or_raise(params, "query"))
+            body = json.loads(params.get("query"))
             full_query = body
             full_query["size"] = int(
                 os.environ.get("OPENSEARCH_TRACES_SEARCH_SIZE", "5000")
@@ -154,7 +154,7 @@
 
             logs_response.raise_for_status()
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=json.dumps(logs_response.json()),
                 params=params,
             )
@@ -163,14 +163,14 @@
                 "Timeout while fetching opensearch traces search", exc_info=True
             )
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Request timed out while fetching opensearch traces search {err_msg}",
                 params=params,
             )
         except RequestException as e:
             logging.warning("Failed to fetch opensearch traces search", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Network error while opensearch traces search {err_msg} : {str(e)}",
                 params=params,
             )
@@ -179,7 +179,7 @@
                 "Failed to process opensearch traces search ", exc_info=True
             )
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Unexpected error {err_msg}: {str(e)}",
                 params=params,
             )
@@ -196,7 +196,7 @@
         super().__init__(
             name="opensearch/traces",
             description="OpenSearch integration to fetch traces",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/opensearch-status/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/opensearch-traces.html",
             icon_url="https://opensearch.org/assets/brand/PNG/Mark/opensearch_mark_default.png",
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
             tools=[
diff -ur baseline-holmes/holmes/plugins/toolsets/opensearch/opensearch.py holmes2/holmes/plugins/toolsets/opensearch/opensearch.py
--- baseline-holmes/holmes/plugins/toolsets/opensearch/opensearch.py	2025-11-05 16:43:28.293312931 -0800
+++ holmes2/holmes/plugins/toolsets/opensearch/opensearch.py	2025-10-17 15:09:28.682726557 -0700
@@ -8,9 +8,8 @@
     CallablePrerequisite,
     StructuredToolResult,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     Toolset,
     ToolsetTag,
 )
@@ -94,11 +93,11 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         client = get_client(self.toolset.clients, host=params.get("host", ""))
         shards = client.client.cat.shards()
         return StructuredToolResult(
-            status=StructuredToolResultStatus.SUCCESS,
+            status=ToolResultStatus.SUCCESS,
             data=str(shards),
             params=params,
         )
@@ -123,13 +122,13 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         client = get_client(self.toolset.clients, host=params.get("host"))
         response = client.client.cluster.get_settings(
             include_defaults=True, flat_settings=True
         )
         return StructuredToolResult(
-            status=StructuredToolResultStatus.SUCCESS,
+            status=ToolResultStatus.SUCCESS,
             data=str(response),
             params=params,
         )
@@ -154,11 +153,11 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         client = get_client(self.toolset.clients, host=params.get("host", ""))
         health = client.client.cluster.health()
         return StructuredToolResult(
-            status=StructuredToolResultStatus.SUCCESS,
+            status=ToolResultStatus.SUCCESS,
             data=str(health),
             params=params,
         )
@@ -177,10 +176,10 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         hosts = [host for client in self.toolset.clients for host in client.hosts]
         return StructuredToolResult(
-            status=StructuredToolResultStatus.SUCCESS,
+            status=ToolResultStatus.SUCCESS,
             data=str(hosts),
             params=params,
         )
@@ -198,7 +197,7 @@
             name="opensearch/status",
             enabled=False,
             description="Provide cluster metadata information like health, shards, settings.",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/opensearch-status/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/opensearch.html",
             icon_url="https://opensearch.org/assets/brand/PNG/Mark/opensearch_mark_default.png",
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
             tools=[
Only in baseline-holmes/holmes/plugins/toolsets: openshift.yaml
Only in holmes2/holmes/plugins/toolsets/prometheus: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/prometheus/prometheus_instructions.jinja2 holmes2/holmes/plugins/toolsets/prometheus/prometheus_instructions.jinja2
--- baseline-holmes/holmes/plugins/toolsets/prometheus/prometheus_instructions.jinja2	2025-11-05 16:43:28.294912760 -0800
+++ holmes2/holmes/plugins/toolsets/prometheus/prometheus_instructions.jinja2	2025-10-17 15:09:28.829519998 -0700
@@ -1,27 +1,6 @@
 
 # Prometheus/PromQL queries
-
-## Efficient Metric Discovery (when needed)
-* When you need to discover metrics, use `get_metric_names` with filters - it's the fastest method
-* Combine multiple patterns with regex OR (|) to reduce API calls:
-  - `{__name__=~"node_cpu.*|node_memory.*|node_disk.*"}` - get all node resource metrics in one call
-  - `{__name__=~"container.*|pod.*|kube.*"}` - get all Kubernetes-related metrics
-  - `{namespace=~"example1|example2|example3"}` - metrics from multiple namespaces
-* Use `get_metric_metadata` after discovering names to get types/descriptions if needed
-* Use `get_label_values` to discover pods, namespaces, jobs: e.g., get_label_values(label="pod")
-* Only use `get_series` when you need full label sets (slower than other methods)
-
-## Retrying queries that return too much data
-* When a Prometheus query returns too much data (e.g., truncation error), you MUST retry with a more specific query or less data points or topk/bottomk
-* NEVER EVER EVER answer a question based on Prometheus data that was truncated as you might be missing important information and give the totally wrong answer
-* Prefer telling the user you can't answer the question because of too much data rather than answering based on incomplete data
-* You are also able to show graphs to the user (using the promql embed functionality mentioned below) so you can show users graphs and THEY can interpret the data themselves, even if you can't answer.
-* Do NOT hestitate to try alternative queries and try to reduce the amount of data returned until you get a successful query
-* Be extremely, extremely cautious when answering based on get_label_values because the existence of a label value says NOTHING about the metric value itself (is it high, low, or perhaps the label exists in Prometheus but its an older series not present right now)
-* DO NOT give answers about metrics based on what 'is typically the case' or 'common knowledge' - if you can't see the actual metric value, you MUST NEVER EVER answer about it - just tell the user your limitations due to the size of the data
-
-## Alert Investigation & Query Execution
-* When investigating a Prometheus alert, ALWAYS call list_prometheus_rules to get the alert definition
+* ALWAYS call list_prometheus_rules to get the alert definition
 * Use Prometheus to query metrics from the alert promql
 * Use prometheus to execute promql queries with the tools `execute_prometheus_instant_query` and `execute_prometheus_range_query`
 * To create queries, use 'start_timestamp' and 'end_timestamp' as graphs start and end times
@@ -37,29 +16,9 @@
 ** Avoid global averages like `sum(rate(<metric>_sum)) / sum(rate(<metric>_count))` because it hides data and is not generally informative
 * Timestamps MUST be in string date format. For example: '2025-03-15 10:10:08.610862+00:00'
 * Post processing will parse your response, re-run the query from the tool output and create a chart visible to the user
-* When unsure about available metrics, use `get_metric_names` with appropriate filters (combine multiple patterns with | for efficiency). Then use `get_metric_metadata` if you need descriptions/types
+* Only generate and execute a prometheus query after checking what metrics are available with the `list_available_metrics` tool
 * Check that any node, service, pod, container, app, namespace, etc. mentioned in the query exist in the kubernetes cluster before making a query. Use any appropriate kubectl tool(s) for this
 * The toolcall will return no data to you. That is expected. You MUST however ensure that the query is successful.
-
-## Handling High-Cardinality Metrics
-* CRITICAL: When querying metrics that may return many time series (>10), ALWAYS use aggregation to limit results
-* ALWAYS use `topk()` or `bottomk()` to limit the number of series returned
-* Standard pattern for high-cardinality queries:
-  - Use `topk(5, <your_query>)` to get the top 5 series
-  - Example: `topk(5, rate(container_cpu_usage_seconds_total{namespace="example"}[5m]))`
-  - This prevents context overflow and focuses on the most relevant data
-* To also capture the aggregate of remaining series as "other":
-  ```
-  topk(5, rate(container_cpu_usage_seconds_total{namespace="example"}[5m])) or label_replace((sum(rate(container_cpu_usage_seconds_total{namespace="example"}[5m])) - sum(topk(5, rate(container_cpu_usage_seconds_total{namespace="example"}[5m])))), "pod", "other", "", "")
-  ```
-* Common high-cardinality scenarios requiring topk():
-  - Pod-level metrics in namespaces with many pods
-  - Container-level CPU/memory metrics
-  - HTTP metrics with many endpoints or status codes
-  - Any query returning more than 10 time series
-* For initial exploration, you may use instant queries with `count()` to check cardinality:
-  - Example: `count(count by (pod) (container_cpu_usage_seconds_total{namespace="example"}))`
-  - If count > 10, use topk() in your range query
 * When doing queries, always extend the time range, to 15 min before and after the alert start time
 * ALWAYS embed the execution results into your answer
 * ALWAYS embed a Prometheus graph in the response. The graph should visualize data related to the incident.
diff -ur baseline-holmes/holmes/plugins/toolsets/prometheus/prometheus.py holmes2/holmes/plugins/toolsets/prometheus/prometheus.py
--- baseline-holmes/holmes/plugins/toolsets/prometheus/prometheus.py	2025-11-05 16:43:28.294728469 -0800
+++ holmes2/holmes/plugins/toolsets/prometheus/prometheus.py	2025-10-17 15:09:28.829877120 -0700
@@ -1,30 +1,27 @@
 import json
 import logging
+import boto3
 import os
+import re
 import time
-import dateutil.parser
-from typing import Any, Dict, Optional, Tuple, Type, Union
+from typing import Any, Dict, List, Optional, Tuple, Type, Union
 from urllib.parse import urljoin
 
 import requests  # type: ignore
 from pydantic import BaseModel, field_validator, Field, model_validator
 from requests import RequestException
-from prometrix.connect.aws_connect import AWSPrometheusConnect
-from prometrix.models.prometheus_config import PrometheusConfig as BasePrometheusConfig
+from requests_aws4auth import AWS4Auth
+
 from holmes.core.tools import (
     CallablePrerequisite,
     StructuredToolResult,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     Toolset,
     ToolsetTag,
 )
-from holmes.core.tools_utils.token_counting import count_tool_response_tokens
-from holmes.core.tools_utils.tool_context_window_limiter import get_pct_token_count
 from holmes.plugins.toolsets.consts import STANDARD_END_DATETIME_TOOL_PARAM_DESCRIPTION
-from holmes.plugins.toolsets.prometheus.utils import parse_duration_to_seconds
 from holmes.plugins.toolsets.service_discovery import PrometheusDiscovery
 from holmes.plugins.toolsets.utils import (
     get_param_or_raise,
@@ -33,72 +30,32 @@
     toolset_name_for_one_liner,
 )
 from holmes.utils.cache import TTLCache
-from holmes.common.env_vars import IS_OPENSHIFT, MAX_GRAPH_POINTS
+from holmes.common.env_vars import IS_OPENSHIFT
 from holmes.common.openshift import load_openshift_token
 from holmes.plugins.toolsets.logging_utils.logging_api import (
-    DEFAULT_GRAPH_TIME_SPAN_SECONDS,
+    DEFAULT_TIME_SPAN_SECONDS,
 )
 from holmes.utils.keygen_utils import generate_random_key
 
 PROMETHEUS_RULES_CACHE_KEY = "cached_prometheus_rules"
-PROMETHEUS_METADATA_API_LIMIT = 100  # Default limit for Prometheus metadata APIs (series, labels, metadata) to prevent overwhelming responses
-# Default timeout values for PromQL queries
-DEFAULT_QUERY_TIMEOUT_SECONDS = 20
-MAX_QUERY_TIMEOUT_SECONDS = 180
-# Default timeout for metadata API calls (discovery endpoints)
-DEFAULT_METADATA_TIMEOUT_SECONDS = 20
-MAX_METADATA_TIMEOUT_SECONDS = 60
-# Default time window for metadata APIs (in hours)
-DEFAULT_METADATA_TIME_WINDOW_HRS = 1
 
 
 class PrometheusConfig(BaseModel):
     # URL is optional because it can be set with an env var
     prometheus_url: Optional[str]
     healthcheck: str = "-/healthy"
-
-    # New config for default time window for metadata APIs
-    default_metadata_time_window_hrs: int = DEFAULT_METADATA_TIME_WINDOW_HRS  # Default: only show metrics active in the last hour
-
-    # Query timeout configuration
-    default_query_timeout_seconds: int = (
-        DEFAULT_QUERY_TIMEOUT_SECONDS  # Default timeout for PromQL queries
-    )
-    max_query_timeout_seconds: int = (
-        MAX_QUERY_TIMEOUT_SECONDS  # Maximum allowed timeout for PromQL queries
-    )
-
-    # Metadata API timeout configuration
-    default_metadata_timeout_seconds: int = (
-        DEFAULT_METADATA_TIMEOUT_SECONDS  # Default timeout for metadata/discovery APIs
-    )
-    max_metadata_timeout_seconds: int = (
-        MAX_METADATA_TIMEOUT_SECONDS  # Maximum allowed timeout for metadata APIs
-    )
-
-    # DEPRECATED: These config values are deprecated and will be removed in a future version
-    # Using None as default so we can detect if user explicitly set them
-    metrics_labels_time_window_hrs: Optional[int] = (
-        None  # DEPRECATED - use default_metadata_time_window_hrs instead
-    )
-    metrics_labels_cache_duration_hrs: Optional[int] = (
-        None  # DEPRECATED - no longer used
-    )
-    fetch_labels_with_labels_api: Optional[bool] = None  # DEPRECATED - no longer used
-    fetch_metadata_with_series_api: Optional[bool] = None  # DEPRECATED - no longer used
-
+    # Setting to None will remove the time window from the request for labels
+    metrics_labels_time_window_hrs: Union[int, None] = 48
+    # Setting to None will disable the cache
+    metrics_labels_cache_duration_hrs: Union[int, None] = 12
+    fetch_labels_with_labels_api: bool = False
+    fetch_metadata_with_series_api: bool = False
     tool_calls_return_data: bool = True
     headers: Dict = Field(default_factory=dict)
-    rules_cache_duration_seconds: Optional[int] = 1800  # 30 minutes
+    rules_cache_duration_seconds: Union[int, None] = 1800  # 30 minutes
     additional_labels: Optional[Dict[str, str]] = None
     prometheus_ssl_enabled: bool = True
 
-    # Custom limit to the max number of tokens that a query result can take to proactively
-    #   prevent token limit issues. Expressed in % of the model's context window.
-    # This limit only overrides the global limit for all tools  (TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_PCT)
-    #   if it is lower.
-    query_response_size_limit_pct: Optional[int] = None
-
     @field_validator("prometheus_url")
     def ensure_trailing_slash(cls, v: Optional[str]) -> Optional[str]:
         if v is not None and not v.endswith("/"):
@@ -107,26 +64,6 @@
 
     @model_validator(mode="after")
     def validate_prom_config(self):
-        # Check for deprecated config values and print warnings
-        deprecated_configs = []
-        if self.metrics_labels_time_window_hrs is not None:  # Check if explicitly set
-            deprecated_configs.append(
-                "metrics_labels_time_window_hrs (use default_metadata_time_window_hrs instead)"
-            )
-        if (
-            self.metrics_labels_cache_duration_hrs is not None
-        ):  # Check if explicitly set
-            deprecated_configs.append("metrics_labels_cache_duration_hrs")
-        if self.fetch_labels_with_labels_api is not None:  # Check if explicitly set
-            deprecated_configs.append("fetch_labels_with_labels_api")
-        if self.fetch_metadata_with_series_api is not None:  # Check if explicitly set
-            deprecated_configs.append("fetch_metadata_with_series_api")
-
-        if deprecated_configs:
-            logging.warning(
-                f"WARNING: The following Prometheus config values are deprecated and will be removed in a future version: "
-                f"{', '.join(deprecated_configs)}. These configs no longer affect behavior."
-            )
         # If openshift is enabled, and the user didn't configure auth headers, we will try to load the token from the service account.
         if IS_OPENSHIFT:
             if self.healthcheck == "-/healthy":
@@ -145,6 +82,9 @@
     def is_amp(self) -> bool:
         return False
 
+    def get_auth(self) -> Any:
+        return None
+
 
 class AMPConfig(PrometheusConfig):
     aws_access_key: Optional[str] = None
@@ -153,165 +93,142 @@
     aws_service_name: str = "aps"
     healthcheck: str = "api/v1/query?query=up"
     prometheus_ssl_enabled: bool = False
-    assume_role_arn: Optional[str] = None
-
-    # Refresh the AWS client (and its STS creds) every N seconds (default: 15 minutes)
-    refresh_interval_seconds: int = 900
-
-    _aws_client: Optional[AWSPrometheusConnect] = None
-    _aws_client_created_at: float = 0.0
 
     def is_amp(self) -> bool:
         return True
 
-    def _should_refresh_client(self) -> bool:
-        if not self._aws_client:
-            return True
-        return (
-            time.time() - self._aws_client_created_at
-        ) >= self.refresh_interval_seconds
-
-    def get_aws_client(self) -> Optional[AWSPrometheusConnect]:
-        if not self._aws_client or self._should_refresh_client():
-            try:
-                base_config = BasePrometheusConfig(
-                    url=self.prometheus_url,
-                    disable_ssl=not self.prometheus_ssl_enabled,
-                    additional_labels=self.additional_labels,
-                )
-                self._aws_client = AWSPrometheusConnect(
-                    access_key=self.aws_access_key,
-                    secret_key=self.aws_secret_access_key,
-                    token=None,
-                    region=self.aws_region,
-                    service_name=self.aws_service_name,
-                    assume_role_arn=self.assume_role_arn,
-                    config=base_config,
-                )
-                self._aws_client_created_at = time.time()
-            except Exception:
-                logging.exception("Failed to create/refresh AWS client")
-                return self._aws_client
-        return self._aws_client
+    def _build_irsa_auth(self) -> Optional[AWS4Auth]:
+        """Try IRSA (or default AWS provider chain)."""
+        session = boto3.Session()
+        creds = session.get_credentials()
+        if creds is None:
+            return None
+        frozen = creds.get_frozen_credentials()
+        return AWS4Auth(
+            frozen.access_key,
+            frozen.secret_key,
+            self.aws_region,
+            self.aws_service_name,
+            session_token=frozen.token,
+        )
+
+    def _build_static_aws_auth(self) -> Optional[AWS4Auth]:
+        """Fallback: static credentials from config."""
+        if self.aws_access_key and self.aws_secret_access_key:
+            return AWS4Auth(
+                self.aws_access_key,
+                self.aws_secret_access_key,
+                self.aws_region,
+                self.aws_service_name,
+            )
+        return None
+
+    def get_auth(self):
+        # Prefer IRSA, fallback to static
+        irsa_auth = self._build_irsa_auth()
+        if irsa_auth:
+            return irsa_auth
+        static_auth = self._build_static_aws_auth()
+        if static_auth:
+            return static_auth
+        raise RuntimeError(
+            "No AWS credentials available. Tried IRSA and static keys. "
+            "Ensure IRSA is configured on the service account or provide aws_access_key/aws_secret_access_key."
+        )
 
 
 class BasePrometheusTool(Tool):
     toolset: "PrometheusToolset"
 
 
-def do_request(
-    config,  # PrometheusConfig | AMPConfig
-    url: str,
-    params: Optional[Dict] = None,
-    data: Optional[Dict] = None,
-    timeout: int = 60,
-    verify: Optional[bool] = None,
-    headers: Optional[Dict] = None,
-    method: str = "GET",
-) -> requests.Response:
-    """
-    Route a request through either:
-      - AWSPrometheusConnect (SigV4) when config is AMPConfig
-      - plain requests otherwise
-
-    method defaults to GET so callers can omit it for reads.
-    """
-    if verify is None:
-        verify = config.prometheus_ssl_enabled
-    if headers is None:
-        headers = config.headers or {}
-
-    if isinstance(config, AMPConfig):
-        client = config.get_aws_client()  # cached AWSPrometheusConnect
-        # Note: timeout parameter is not supported by prometrix's signed_request
-        # AWS/AMP requests will not respect the timeout setting
-        return client.signed_request(  # type: ignore
-            method=method,
-            url=url,
-            data=data,
-            params=params,
-            verify=verify,
-            headers=headers,
-        )
-
-    # Non-AMP: plain HTTP
-    return requests.request(
-        method=method,
-        url=url,
-        headers=headers,
-        params=params,
-        data=data,
-        timeout=timeout,
-        verify=verify,
+def filter_metrics_by_type(metrics: Dict, expected_type: str):
+    return {
+        metric_name: metric_data
+        for metric_name, metric_data in metrics.items()
+        if expected_type in metric_data.get("type", "")
+        or metric_data.get("type", "") == "?"
+    }
+
+
+def filter_metrics_by_name(metrics: Dict, pattern: str) -> Dict:
+    regex = re.compile(pattern)
+    return {
+        metric_name: metric_data
+        for metric_name, metric_data in metrics.items()
+        if regex.search(metric_name)
+    }
+
+
+METRICS_SUFFIXES_TO_STRIP = ["_bucket", "_count", "_sum"]
+
+
+def fetch_metadata(
+    prometheus_url: str,
+    headers: Optional[Dict],
+    auth=None,
+    verify_ssl: bool = True,
+) -> Dict:
+    metadata_url = urljoin(prometheus_url, "api/v1/metadata")
+    metadata_response = requests.get(
+        metadata_url, headers=headers, timeout=60, verify=verify_ssl, auth=auth
     )
 
+    metadata_response.raise_for_status()
 
-def result_has_data(result: Dict) -> bool:
-    data = result.get("data", {})
-    if len(data.get("result", [])) > 0:
-        return True
-    return False
+    metadata = metadata_response.json()["data"]
 
+    metrics = {}
+    for metric_name, meta_list in metadata.items():
+        if meta_list:
+            metric_type = meta_list[0].get("type", "unknown")
+            metric_description = meta_list[0].get("help", "unknown")
+            metrics[metric_name] = {
+                "type": metric_type,
+                "description": metric_description,
+                "labels": set(),
+            }
 
-def adjust_step_for_max_points(
-    start_timestamp: str,
-    end_timestamp: str,
-    step: Optional[float] = None,
-    max_points_override: Optional[float] = None,
-) -> float:
-    """
-    Adjusts the step parameter to ensure the number of data points doesn't exceed max_points.
-
-    Args:
-        start_timestamp: RFC3339 formatted start time
-        end_timestamp: RFC3339 formatted end time
-        step: The requested step duration in seconds (None for auto-calculation)
-        max_points_override: Optional override for max points (must be <= MAX_GRAPH_POINTS)
-
-    Returns:
-        Adjusted step value in seconds that ensures points <= max_points
-    """
-    # Use override if provided and valid, otherwise use default
-    max_points = MAX_GRAPH_POINTS
-    if max_points_override is not None:
-        if max_points_override > MAX_GRAPH_POINTS:
-            logging.warning(
-                f"max_points override ({max_points_override}) exceeds system limit ({MAX_GRAPH_POINTS}), using {MAX_GRAPH_POINTS}"
-            )
-            max_points = MAX_GRAPH_POINTS
-        elif max_points_override < 1:
-            logging.warning(
-                f"max_points override ({max_points_override}) is invalid, using default {MAX_GRAPH_POINTS}"
-            )
-            max_points = MAX_GRAPH_POINTS
-        else:
-            max_points = max_points_override
-            logging.debug(f"Using max_points override: {max_points}")
+    return metrics
 
-    start_dt = dateutil.parser.parse(start_timestamp)
-    end_dt = dateutil.parser.parse(end_timestamp)
 
-    time_range_seconds = (end_dt - start_dt).total_seconds()
+def fetch_metadata_with_series_api(
+    prometheus_url: str,
+    metric_name: str,
+    headers: Dict,
+    auth=None,
+    verify_ssl: bool = True,
+) -> Dict:
+    url = urljoin(prometheus_url, "api/v1/series")
+    params: Dict = {"match[]": f'{{__name__=~".*{metric_name}.*"}}', "limit": "10000"}
 
-    # If no step provided, calculate a reasonable default
-    # Aim for ~60 data points across the time range (1 per minute for hourly, etc)
-    if step is None:
-        step = max(1, time_range_seconds / 60)
-        logging.debug(
-            f"No step provided, defaulting to {step}s for {time_range_seconds}s range"
-        )
+    response = requests.get(
+        url, headers=headers, timeout=60, params=params, auth=auth, verify=verify_ssl
+    )
+    response.raise_for_status()
+    metrics = response.json()["data"]
 
-    current_points = time_range_seconds / step
+    metadata: Dict = {}
+    for metric_data in metrics:
+        metric_name = metric_data.get("__name__")
+        if not metric_name:
+            continue
+
+        metric = metadata.get(metric_name)
+        if not metric:
+            metric = {"description": "?", "type": "?", "labels": set()}
+            metadata[metric_name] = metric
 
-    # If current points exceed max, adjust the step
-    if current_points > max_points:
-        adjusted_step = time_range_seconds / max_points
-        logging.info(
-            f"Adjusting step from {step}s to {adjusted_step}s to limit points from {current_points:.0f} to {max_points}"
-        )
-        return adjusted_step
+        labels = {k for k in metric_data.keys() if k != "__name__"}
+        metric["labels"].update(labels)
+
+    return metadata
 
-    return step
+
+def result_has_data(result: Dict) -> bool:
+    data = result.get("data", {})
+    if len(data.get("result", [])) > 0:
+        return True
+    return False
 
 
 def add_prometheus_auth(prometheus_auth_header: Optional[str]) -> Dict[str, Any]:
@@ -321,143 +238,181 @@
     return results
 
 
-def create_data_summary_for_large_result(
-    result_data: Dict, query: str, data_size_tokens: int, is_range_query: bool = False
-) -> Dict[str, Any]:
-    """
-    Create a summary for large Prometheus results instead of returning full data.
-
-    Args:
-        result_data: The Prometheus data result
-        query: The original PromQL query
-        data_size_tokens: Size of the data in tokens
-        is_range_query: Whether this is a range query (vs instant query)
-
-    Returns:
-        Dictionary with summary information and suggestions
-    """
-    if is_range_query:
-        series_list = result_data.get("result", [])
-        num_items = len(series_list)
-
-        # Calculate exact total data points across all series
-        total_points = 0
-        for series in series_list:  # Iterate through ALL series for exact count
-            points = len(series.get("values", []))
-            total_points += points
-
-        # Analyze label keys and their cardinality
-        label_cardinality: Dict[str, set] = {}
-        for series in series_list:
-            metric = series.get("metric", {})
-            for label_key, label_value in metric.items():
-                if label_key not in label_cardinality:
-                    label_cardinality[label_key] = set()
-                label_cardinality[label_key].add(label_value)
-
-        # Convert sets to counts for the summary
-        label_summary = {
-            label: len(values) for label, values in label_cardinality.items()
-        }
-        # Sort by cardinality (highest first) for better insights
-        label_summary = dict(
-            sorted(label_summary.items(), key=lambda x: x[1], reverse=True)
-        )
+def fetch_metrics_labels_with_series_api(
+    prometheus_url: str,
+    headers: Dict[str, str],
+    cache: Optional[TTLCache],
+    metrics_labels_time_window_hrs: Union[int, None],
+    metric_name: str,
+    auth=None,
+    verify_ssl: bool = True,
+) -> dict:
+    """This is a slow query. Takes 5+ seconds to run"""
+    cache_key = f"metrics_labels_series_api:{metric_name}"
+    if cache:
+        cached_result = cache.get(cache_key)
+        if cached_result:
+            return cached_result
+
+    series_url = urljoin(prometheus_url, "api/v1/series")
+    params: dict = {"match[]": f'{{__name__=~".*{metric_name}.*"}}', "limit": "10000"}
+
+    if metrics_labels_time_window_hrs is not None:
+        params["end"] = int(time.time())
+        params["start"] = params["end"] - (metrics_labels_time_window_hrs * 60 * 60)
 
-        return {
-            "message": f"Data too large to return ({data_size_tokens:,} tokens). Query returned {num_items} time series with {total_points:,} total data points.",
-            "series_count": num_items,
-            "total_data_points": total_points,
-            "data_size_tokens": data_size_tokens,
-            "label_cardinality": label_summary,
-            "suggestion": f'Consider using topk({min(5, num_items)}, {query}) to limit results to the top {min(5, num_items)} series. To also capture remaining data as \'other\': topk({min(5, num_items)}, {query}) or label_replace((sum({query}) - sum(topk({min(5, num_items)}, {query}))), "pod", "other", "", "")',
+    series_response = requests.get(
+        url=series_url,
+        headers=headers,
+        params=params,
+        auth=auth,
+        timeout=60,
+        verify=verify_ssl,
+    )
+    series_response.raise_for_status()
+    series = series_response.json()["data"]
+
+    metrics_labels: dict = {}
+    for serie in series:
+        metric_name = serie["__name__"]
+        # Add all labels except __name__
+        labels = {k for k in serie.keys() if k != "__name__"}
+        if metric_name in metrics_labels:
+            metrics_labels[metric_name].update(labels)
+        else:
+            metrics_labels[metric_name] = labels
+    if cache:
+        cache.set(cache_key, metrics_labels)
+
+    return metrics_labels
+
+
+def fetch_metrics_labels_with_labels_api(
+    prometheus_url: str,
+    cache: Optional[TTLCache],
+    metrics_labels_time_window_hrs: Union[int, None],
+    metric_names: List[str],
+    headers: Dict,
+    auth=None,
+    verify_ssl: bool = True,
+) -> dict:
+    metrics_labels = {}
+
+    for metric_name in metric_names:
+        cache_key = f"metrics_labels_labels_api:{metric_name}"
+        if cache:
+            cached_result = cache.get(cache_key)
+            if cached_result:
+                metrics_labels[metric_name] = cached_result
+
+        url = urljoin(prometheus_url, "api/v1/labels")
+        params: dict = {
+            "match[]": f'{{__name__="{metric_name}"}}',
         }
+        if metrics_labels_time_window_hrs is not None:
+            params["end"] = int(time.time())
+            params["start"] = params["end"] - (metrics_labels_time_window_hrs * 60 * 60)
+
+        response = requests.get(
+            url=url,
+            headers=headers,
+            params=params,
+            auth=auth,
+            timeout=60,
+            verify=verify_ssl,
+        )
+        response.raise_for_status()
+        labels = response.json()["data"]
+        filtered_labels = {label for label in labels if label != "__name__"}
+        metrics_labels[metric_name] = filtered_labels
+
+        if cache:
+            cache.set(cache_key, filtered_labels)
+
+    return metrics_labels
+
+
+def fetch_metrics(
+    prometheus_url: str,
+    cache: Optional[TTLCache],
+    metrics_labels_time_window_hrs: Union[int, None],
+    metric_name: str,
+    should_fetch_labels_with_labels_api: bool,
+    should_fetch_metadata_with_series_api: bool,
+    headers: Dict,
+    auth=None,
+    verify_ssl: bool = True,
+) -> dict:
+    metrics = None
+    should_fetch_labels = True
+    if should_fetch_metadata_with_series_api:
+        metrics = fetch_metadata_with_series_api(
+            prometheus_url=prometheus_url,
+            metric_name=metric_name,
+            headers=headers,
+            auth=auth,
+            verify_ssl=verify_ssl,
+        )
+        should_fetch_labels = False  # series API returns the labels
     else:
-        # Instant query
-        result_type = result_data.get("resultType", "")
-        result_list = result_data.get("result", [])
-        num_items = len(result_list)
-
-        # Analyze label keys and their cardinality
-        instant_label_cardinality: Dict[str, set] = {}
-        for item in result_list:
-            if isinstance(item, dict):
-                metric = item.get("metric", {})
-                for label_key, label_value in metric.items():
-                    if label_key not in instant_label_cardinality:
-                        instant_label_cardinality[label_key] = set()
-                    instant_label_cardinality[label_key].add(label_value)
-
-        # Convert sets to counts for the summary
-        label_summary = {
-            label: len(values) for label, values in instant_label_cardinality.items()
-        }
-        # Sort by cardinality (highest first) for better insights
-        label_summary = dict(
-            sorted(label_summary.items(), key=lambda x: x[1], reverse=True)
+        metrics = fetch_metadata(
+            prometheus_url=prometheus_url,
+            headers=headers,
+            auth=auth,
+            verify_ssl=verify_ssl,
         )
+        metrics = filter_metrics_by_name(metrics, metric_name)
 
-        return {
-            "message": f"Data too large to return ({data_size_tokens:,} tokens). Query returned {num_items} results.",
-            "result_count": num_items,
-            "result_type": result_type,
-            "data_size_tokens": data_size_tokens,
-            "label_cardinality": label_summary,
-            "suggestion": f'Consider using topk({min(5, num_items)}, {query}) to limit results. To also capture remaining data as \'other\': topk({min(5, num_items)}, {query}) or label_replace((sum({query}) - sum(topk({min(5, num_items)}, {query}))), "instance", "other", "", "")',
-        }
+    if should_fetch_labels:
+        metrics_labels = {}
+        if should_fetch_labels_with_labels_api:
+            metrics_labels = fetch_metrics_labels_with_labels_api(
+                prometheus_url=prometheus_url,
+                cache=cache,
+                metrics_labels_time_window_hrs=metrics_labels_time_window_hrs,
+                metric_names=list(metrics.keys()),
+                headers=headers,
+                auth=auth,
+                verify_ssl=verify_ssl,
+            )
+        else:
+            metrics_labels = fetch_metrics_labels_with_series_api(
+                prometheus_url=prometheus_url,
+                cache=cache,
+                metrics_labels_time_window_hrs=metrics_labels_time_window_hrs,
+                metric_name=metric_name,
+                headers=headers,
+                auth=auth,
+                verify_ssl=verify_ssl,
+            )
 
+        for metric_name in metrics:
+            if metric_name in metrics_labels:
+                metrics[metric_name]["labels"] = metrics_labels[metric_name]
 
-class MetricsBasedResponse(BaseModel):
-    status: str
-    error_message: Optional[str] = None
-    data: Optional[str] = None
-    random_key: str
-    tool_name: str
-    description: str
-    query: str
-    start: Optional[str] = None
-    end: Optional[str] = None
-    step: Optional[float] = None
-    output_type: Optional[str] = None
-    data_summary: Optional[dict[str, Any]] = None
-
-
-def create_structured_tool_result(
-    params: dict, response: MetricsBasedResponse
-) -> StructuredToolResult:
-    status = StructuredToolResultStatus.SUCCESS
-    if response.error_message or response.status.lower() in ("failed", "error"):
-        status = StructuredToolResultStatus.ERROR
-    elif not response.data:
-        status = StructuredToolResultStatus.NO_DATA
-
-    return StructuredToolResult(
-        status=status,
-        data=response.model_dump_json(indent=2),
-        params=params,
-    )
+    return metrics
 
 
 class ListPrometheusRules(BasePrometheusTool):
     def __init__(self, toolset: "PrometheusToolset"):
         super().__init__(
             name="list_prometheus_rules",
-            description="List all defined Prometheus rules (api/v1/rules). Will show the Prometheus rules description, expression and annotations",
+            description="List all defined prometheus rules. Will show the prometheus rules description, expression and annotations",
             parameters={},
             toolset=toolset,
         )
         self._cache = None
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         if not self.toolset.config or not self.toolset.config.prometheus_url:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="Prometheus is not configured. Prometheus URL is missing",
                 params=params,
             )
         if self.toolset.config.is_amp():
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="Tool not supported in AMP",
                 params=params,
             )
@@ -470,7 +425,7 @@
                     logging.debug("rules returned from cache")
 
                     return StructuredToolResult(
-                        status=StructuredToolResultStatus.SUCCESS,
+                        status=ToolResultStatus.SUCCESS,
                         data=cached_rules,
                         params=params,
                     )
@@ -479,14 +434,13 @@
 
             rules_url = urljoin(prometheus_url, "api/v1/rules")
 
-            rules_response = do_request(
-                config=self.toolset.config,
+            rules_response = requests.get(
                 url=rules_url,
                 params=params,
-                timeout=40,
+                auth=self.toolset.config.get_auth(),
+                timeout=180,
                 verify=self.toolset.config.prometheus_ssl_enabled,
                 headers=self.toolset.config.headers,
-                method="GET",
             )
             rules_response.raise_for_status()
             data = rules_response.json()["data"]
@@ -494,28 +448,28 @@
             if self._cache:
                 self._cache.set(PROMETHEUS_RULES_CACHE_KEY, data)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=data,
                 params=params,
             )
         except requests.Timeout:
             logging.warning("Timeout while fetching prometheus rules", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="Request timed out while fetching rules",
                 params=params,
             )
         except RequestException as e:
             logging.warning("Failed to fetch prometheus rules", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Network error while fetching rules: {str(e)}",
                 params=params,
             )
         except Exception as e:
             logging.warning("Failed to process prometheus rules", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Unexpected error: {str(e)}",
                 params=params,
             )
@@ -524,553 +478,117 @@
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Fetch Rules"
 
 
-class GetMetricNames(BasePrometheusTool):
-    """Thin wrapper around /api/v1/label/__name__/values - the fastest way to discover metric names"""
-
+class ListAvailableMetrics(BasePrometheusTool):
     def __init__(self, toolset: "PrometheusToolset"):
         super().__init__(
-            name="get_metric_names",
-            description=(
-                "Get list of metric names using /api/v1/label/__name__/values. "
-                "FASTEST method for metric discovery when you need to explore available metrics. "
-                f"Returns up to {PROMETHEUS_METADATA_API_LIMIT} unique metric names (limit={PROMETHEUS_METADATA_API_LIMIT}). If {PROMETHEUS_METADATA_API_LIMIT} results returned, more may exist - use a more specific filter. "
-                f"ALWAYS use match[] parameter to filter metrics - without it you'll get random {PROMETHEUS_METADATA_API_LIMIT} metrics which is rarely useful. "
-                "Note: Does not return metric metadata (type, description, labels). "
-                "By default returns metrics active in the last 1 hour (configurable via default_metadata_time_window_hrs)."
-            ),
+            name="list_available_metrics",
+            description="List all the available metrics to query from prometheus, including their types (counter, gauge, histogram, summary) and available labels.",
             parameters={
-                "match": ToolParameter(
-                    description=(
-                        "REQUIRED: PromQL selector to filter metrics. Use regex OR (|) to check multiple patterns in one call - much faster than multiple calls! Examples: "
-                        "'{__name__=~\"node_cpu.*|node_memory.*|node_disk.*\"}' for all node resource metrics, "
-                        "'{__name__=~\"container_cpu.*|container_memory.*|container_network.*\"}' for all container metrics, "
-                        "'{__name__=~\"kube_pod.*|kube_deployment.*|kube_service.*\"}' for multiple Kubernetes object metrics, "
-                        "'{__name__=~\".*cpu.*|.*memory.*|.*disk.*\"}' for all resource metrics, "
-                        "'{namespace=~\"kube-system|default|monitoring\"}' for metrics from multiple namespaces, "
-                        "'{job=~\"prometheus|node-exporter|kube-state-metrics\"}' for metrics from multiple jobs."
-                    ),
-                    type="string",
-                    required=True,
-                ),
-                "start": ToolParameter(
-                    description="Start timestamp (RFC3339 or Unix). Default: 1 hour ago",
+                "type_filter": ToolParameter(
+                    description="Optional filter to only return a specific metric type. Can be one of counter, gauge, histogram, summary",
                     type="string",
                     required=False,
                 ),
-                "end": ToolParameter(
-                    description="End timestamp (RFC3339 or Unix). Default: now",
+                "name_filter": ToolParameter(
+                    description="Only the metrics partially or fully matching this name will be returned",
                     type="string",
-                    required=False,
+                    required=True,
                 ),
             },
             toolset=toolset,
         )
+        self._cache = None
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         if not self.toolset.config or not self.toolset.config.prometheus_url:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="Prometheus is not configured. Prometheus URL is missing",
                 params=params,
             )
-        try:
-            match_param = params.get("match")
-            if not match_param:
-                return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
-                    error="Match parameter is required to filter metrics",
-                    params=params,
-                )
-
-            url = urljoin(
-                self.toolset.config.prometheus_url, "api/v1/label/__name__/values"
-            )
-            query_params = {
-                "limit": str(PROMETHEUS_METADATA_API_LIMIT),
-                "match[]": match_param,
-            }
-
-            # Add time parameters - use provided values or defaults
-            if params.get("end"):
-                query_params["end"] = params["end"]
-            else:
-                query_params["end"] = str(int(time.time()))
-
-            if params.get("start"):
-                query_params["start"] = params["start"]
-            elif self.toolset.config.default_metadata_time_window_hrs:
-                # Use default time window
-                query_params["start"] = str(
-                    int(time.time())
-                    - (self.toolset.config.default_metadata_time_window_hrs * 3600)
-                )
-
-            response = do_request(
-                config=self.toolset.config,
-                url=url,
-                params=query_params,
-                timeout=self.toolset.config.default_metadata_timeout_seconds,
-                verify=self.toolset.config.prometheus_ssl_enabled,
-                headers=self.toolset.config.headers,
-                method="GET",
-            )
-            response.raise_for_status()
-            data = response.json()
-
-            # Check if results were truncated
-            if (
-                "data" in data
-                and isinstance(data["data"], list)
-                and len(data["data"]) == PROMETHEUS_METADATA_API_LIMIT
-            ):
-                data["_truncated"] = True
-                data["_message"] = (
-                    f"Results truncated at limit={PROMETHEUS_METADATA_API_LIMIT}. Use a more specific match filter to see additional metrics."
-                )
-
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=data,
-                params=params,
+        if not self._cache and self.toolset.config.metrics_labels_cache_duration_hrs:
+            self._cache = TTLCache(
+                self.toolset.config.metrics_labels_cache_duration_hrs * 3600  # type: ignore
             )
-        except Exception as e:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=str(e),
-                params=params,
+        try:
+            prometheus_url = self.toolset.config.prometheus_url
+            metrics_labels_time_window_hrs = (
+                self.toolset.config.metrics_labels_time_window_hrs
             )
 
-    def get_parameterized_one_liner(self, params) -> str:
-        return f"{toolset_name_for_one_liner(self.toolset.name)}: Get Metric Names"
-
-
-class GetLabelValues(BasePrometheusTool):
-    """Get values for a specific label across all metrics"""
-
-    def __init__(self, toolset: "PrometheusToolset"):
-        super().__init__(
-            name="get_label_values",
-            description=(
-                "Get all values for a specific label using /api/v1/label/{label}/values. "
-                "Use this to discover pods, namespaces, jobs, instances, etc. "
-                f"Returns up to {PROMETHEUS_METADATA_API_LIMIT} unique values (limit={PROMETHEUS_METADATA_API_LIMIT}). If {PROMETHEUS_METADATA_API_LIMIT} results returned, more may exist - use match[] to filter. "
-                "Supports optional match[] parameter to filter. "
-                "By default returns values from metrics active in the last 1 hour (configurable via default_metadata_time_window_hrs)."
-            ),
-            parameters={
-                "label": ToolParameter(
-                    description="Label name to get values for (e.g., 'pod', 'namespace', 'job', 'instance')",
-                    type="string",
-                    required=True,
-                ),
-                "match": ToolParameter(
-                    description=(
-                        "Optional PromQL selector to filter (e.g., '{__name__=~\"kube.*\"}', "
-                        "'{namespace=\"default\"}')."
-                    ),
-                    type="string",
-                    required=False,
-                ),
-                "start": ToolParameter(
-                    description="Start timestamp (RFC3339 or Unix). Default: 1 hour ago",
-                    type="string",
-                    required=False,
-                ),
-                "end": ToolParameter(
-                    description="End timestamp (RFC3339 or Unix). Default: now",
-                    type="string",
-                    required=False,
-                ),
-            },
-            toolset=toolset,
-        )
-
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
-        if not self.toolset.config or not self.toolset.config.prometheus_url:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error="Prometheus is not configured. Prometheus URL is missing",
-                params=params,
-            )
-        try:
-            label = params.get("label")
-            if not label:
+            name_filter = params.get("name_filter")
+            if not name_filter:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
-                    error="Label parameter is required",
+                    status=ToolResultStatus.ERROR,
+                    error="Error: cannot run tool 'list_available_metrics'. The param 'name_filter' is required but is missing.",
                     params=params,
                 )
 
-            url = urljoin(
-                self.toolset.config.prometheus_url, f"api/v1/label/{label}/values"
-            )
-            query_params = {"limit": str(PROMETHEUS_METADATA_API_LIMIT)}
-            if params.get("match"):
-                query_params["match[]"] = params["match"]
-
-            # Add time parameters - use provided values or defaults
-            if params.get("end"):
-                query_params["end"] = params["end"]
-            else:
-                query_params["end"] = str(int(time.time()))
-
-            if params.get("start"):
-                query_params["start"] = params["start"]
-            elif self.toolset.config.default_metadata_time_window_hrs:
-                # Use default time window
-                query_params["start"] = str(
-                    int(time.time())
-                    - (self.toolset.config.default_metadata_time_window_hrs * 3600)
-                )
-
-            response = do_request(
-                config=self.toolset.config,
-                url=url,
-                params=query_params,
-                timeout=self.toolset.config.default_metadata_timeout_seconds,
-                verify=self.toolset.config.prometheus_ssl_enabled,
-                headers=self.toolset.config.headers,
-                method="GET",
-            )
-            response.raise_for_status()
-            data = response.json()
-
-            # Check if results were truncated
-            if (
-                "data" in data
-                and isinstance(data["data"], list)
-                and len(data["data"]) == PROMETHEUS_METADATA_API_LIMIT
-            ):
-                data["_truncated"] = True
-                data["_message"] = (
-                    f"Results truncated at limit={PROMETHEUS_METADATA_API_LIMIT}. Use match[] parameter to filter label '{label}' values."
-                )
-
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=data,
-                params=params,
-            )
-        except Exception as e:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=str(e),
-                params=params,
-            )
-
-    def get_parameterized_one_liner(self, params) -> str:
-        label = params.get("label", "")
-        return f"{toolset_name_for_one_liner(self.toolset.name)}: Get {label} Values"
-
-
-class GetAllLabels(BasePrometheusTool):
-    """Get all label names that exist in Prometheus"""
-
-    def __init__(self, toolset: "PrometheusToolset"):
-        super().__init__(
-            name="get_all_labels",
-            description=(
-                "Get list of all label names using /api/v1/labels. "
-                "Use this to discover what labels are available across all metrics. "
-                f"Returns up to {PROMETHEUS_METADATA_API_LIMIT} label names (limit={PROMETHEUS_METADATA_API_LIMIT}). If {PROMETHEUS_METADATA_API_LIMIT} results returned, more may exist - use match[] to filter. "
-                "Supports optional match[] parameter to filter. "
-                "By default returns labels from metrics active in the last 1 hour (configurable via default_metadata_time_window_hrs)."
-            ),
-            parameters={
-                "match": ToolParameter(
-                    description=(
-                        "Optional PromQL selector to filter (e.g., '{__name__=~\"kube.*\"}', "
-                        "'{job=\"prometheus\"}')."
-                    ),
-                    type="string",
-                    required=False,
-                ),
-                "start": ToolParameter(
-                    description="Start timestamp (RFC3339 or Unix). Default: 1 hour ago",
-                    type="string",
-                    required=False,
-                ),
-                "end": ToolParameter(
-                    description="End timestamp (RFC3339 or Unix). Default: now",
-                    type="string",
-                    required=False,
-                ),
-            },
-            toolset=toolset,
-        )
-
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
-        if not self.toolset.config or not self.toolset.config.prometheus_url:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error="Prometheus is not configured. Prometheus URL is missing",
-                params=params,
-            )
-        try:
-            url = urljoin(self.toolset.config.prometheus_url, "api/v1/labels")
-            query_params = {"limit": str(PROMETHEUS_METADATA_API_LIMIT)}
-            if params.get("match"):
-                query_params["match[]"] = params["match"]
-
-            # Add time parameters - use provided values or defaults
-            if params.get("end"):
-                query_params["end"] = params["end"]
-            else:
-                query_params["end"] = str(int(time.time()))
-
-            if params.get("start"):
-                query_params["start"] = params["start"]
-            elif self.toolset.config.default_metadata_time_window_hrs:
-                # Use default time window
-                query_params["start"] = str(
-                    int(time.time())
-                    - (self.toolset.config.default_metadata_time_window_hrs * 3600)
-                )
-
-            response = do_request(
-                config=self.toolset.config,
-                url=url,
-                params=query_params,
-                timeout=self.toolset.config.default_metadata_timeout_seconds,
-                verify=self.toolset.config.prometheus_ssl_enabled,
+            metrics = fetch_metrics(
+                prometheus_url=prometheus_url,
+                cache=self._cache,
+                metrics_labels_time_window_hrs=metrics_labels_time_window_hrs,
+                metric_name=name_filter,
+                should_fetch_labels_with_labels_api=self.toolset.config.fetch_labels_with_labels_api,
+                should_fetch_metadata_with_series_api=self.toolset.config.fetch_metadata_with_series_api,
                 headers=self.toolset.config.headers,
-                method="GET",
-            )
-            response.raise_for_status()
-            data = response.json()
-
-            # Check if results were truncated
-            if (
-                "data" in data
-                and isinstance(data["data"], list)
-                and len(data["data"]) == PROMETHEUS_METADATA_API_LIMIT
-            ):
-                data["_truncated"] = True
-                data["_message"] = (
-                    f"Results truncated at limit={PROMETHEUS_METADATA_API_LIMIT}. Use match[] parameter to filter labels."
-                )
-
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=data,
-                params=params,
-            )
-        except Exception as e:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=str(e),
-                params=params,
+                auth=self.toolset.config.get_auth(),
+                verify_ssl=self.toolset.config.prometheus_ssl_enabled,
             )
 
-    def get_parameterized_one_liner(self, params) -> str:
-        return f"{toolset_name_for_one_liner(self.toolset.name)}: Get All Labels"
-
+            if params.get("type_filter"):
+                metrics = filter_metrics_by_type(metrics, params.get("type_filter"))
 
-class GetSeries(BasePrometheusTool):
-    """Get time series matching a selector"""
+            output = ["Metric | Description | Type | Labels"]
+            output.append("-" * 100)
 
-    def __init__(self, toolset: "PrometheusToolset"):
-        super().__init__(
-            name="get_series",
-            description=(
-                "Get time series using /api/v1/series. "
-                "Returns label sets for all time series matching the selector. "
-                "SLOWER than other discovery methods - use only when you need full label sets. "
-                f"Returns up to {PROMETHEUS_METADATA_API_LIMIT} series (limit={PROMETHEUS_METADATA_API_LIMIT}). If {PROMETHEUS_METADATA_API_LIMIT} results returned, more series exist - use more specific selector. "
-                "Requires match[] parameter with PromQL selector. "
-                "By default returns series active in the last 1 hour (configurable via default_metadata_time_window_hrs)."
-            ),
-            parameters={
-                "match": ToolParameter(
-                    description=(
-                        "PromQL selector to match series (e.g., 'up', 'node_cpu_seconds_total', "
-                        "'{__name__=~\"node.*\"}', '{job=\"prometheus\"}', "
-                        '\'{__name__="up",job="prometheus"}\').'
-                    ),
-                    type="string",
-                    required=True,
-                ),
-                "start": ToolParameter(
-                    description="Start timestamp (RFC3339 or Unix). Default: 1 hour ago",
-                    type="string",
-                    required=False,
-                ),
-                "end": ToolParameter(
-                    description="End timestamp (RFC3339 or Unix). Default: now",
-                    type="string",
-                    required=False,
-                ),
-            },
-            toolset=toolset,
-        )
-
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
-        if not self.toolset.config or not self.toolset.config.prometheus_url:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error="Prometheus is not configured. Prometheus URL is missing",
-                params=params,
-            )
-        try:
-            match = params.get("match")
-            if not match:
-                return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
-                    error="Match parameter is required",
-                    params=params,
+            for metric, info in sorted(metrics.items()):
+                labels_str = (
+                    ", ".join(sorted(info["labels"])) if info["labels"] else "none"
                 )
-
-            url = urljoin(self.toolset.config.prometheus_url, "api/v1/series")
-            query_params = {
-                "match[]": match,
-                "limit": str(PROMETHEUS_METADATA_API_LIMIT),
-            }
-
-            # Add time parameters - use provided values or defaults
-            if params.get("end"):
-                query_params["end"] = params["end"]
-            else:
-                query_params["end"] = str(int(time.time()))
-
-            if params.get("start"):
-                query_params["start"] = params["start"]
-            elif self.toolset.config.default_metadata_time_window_hrs:
-                # Use default time window
-                query_params["start"] = str(
-                    int(time.time())
-                    - (self.toolset.config.default_metadata_time_window_hrs * 3600)
-                )
-
-            response = do_request(
-                config=self.toolset.config,
-                url=url,
-                params=query_params,
-                timeout=self.toolset.config.default_metadata_timeout_seconds,
-                verify=self.toolset.config.prometheus_ssl_enabled,
-                headers=self.toolset.config.headers,
-                method="GET",
-            )
-            response.raise_for_status()
-            data = response.json()
-
-            # Check if results were truncated
-            if (
-                "data" in data
-                and isinstance(data["data"], list)
-                and len(data["data"]) == PROMETHEUS_METADATA_API_LIMIT
-            ):
-                data["_truncated"] = True
-                data["_message"] = (
-                    f"Results truncated at limit={PROMETHEUS_METADATA_API_LIMIT}. Use a more specific match selector to see additional series."
+                output.append(
+                    f"{metric} | {info['description']} | {info['type']} | {labels_str}"
                 )
 
+            table_output = "\n".join(output)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=data,
+                status=ToolResultStatus.SUCCESS,
+                data=table_output,
                 params=params,
             )
-        except Exception as e:
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=str(e),
-                params=params,
-            )
-
-    def get_parameterized_one_liner(self, params) -> str:
-        return f"{toolset_name_for_one_liner(self.toolset.name)}: Get Series"
-
-
-class GetMetricMetadata(BasePrometheusTool):
-    """Get metadata (type, description, unit) for metrics"""
 
-    def __init__(self, toolset: "PrometheusToolset"):
-        super().__init__(
-            name="get_metric_metadata",
-            description=(
-                "Get metric metadata using /api/v1/metadata. "
-                "Returns type, help text, and unit for metrics. "
-                "Use after discovering metric names to get their descriptions. "
-                f"Returns up to {PROMETHEUS_METADATA_API_LIMIT} metrics (limit={PROMETHEUS_METADATA_API_LIMIT}). If {PROMETHEUS_METADATA_API_LIMIT} results returned, more may exist - filter by specific metric name. "
-                "Supports optional metric name filter."
-            ),
-            parameters={
-                "metric": ToolParameter(
-                    description=(
-                        "Optional metric name to filter (e.g., 'up', 'node_cpu_seconds_total'). "
-                        "If not provided, returns metadata for all metrics."
-                    ),
-                    type="string",
-                    required=False,
-                ),
-            },
-            toolset=toolset,
-        )
-
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
-        if not self.toolset.config or not self.toolset.config.prometheus_url:
+        except requests.Timeout:
+            logging.warn("Timeout while fetching prometheus metrics", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error="Prometheus is not configured. Prometheus URL is missing",
+                status=ToolResultStatus.ERROR,
+                error="Request timed out while fetching metrics",
                 params=params,
             )
-        try:
-            url = urljoin(self.toolset.config.prometheus_url, "api/v1/metadata")
-            query_params = {"limit": str(PROMETHEUS_METADATA_API_LIMIT)}
-
-            if params.get("metric"):
-                query_params["metric"] = params["metric"]
-
-            response = do_request(
-                config=self.toolset.config,
-                url=url,
-                params=query_params,
-                timeout=self.toolset.config.default_metadata_timeout_seconds,
-                verify=self.toolset.config.prometheus_ssl_enabled,
-                headers=self.toolset.config.headers,
-                method="GET",
-            )
-            response.raise_for_status()
-            data = response.json()
-
-            # Check if results were truncated (metadata endpoint returns a dict, not a list)
-            if (
-                "data" in data
-                and isinstance(data["data"], dict)
-                and len(data["data"]) == PROMETHEUS_METADATA_API_LIMIT
-            ):
-                data["_truncated"] = True
-                data["_message"] = (
-                    f"Results truncated at limit={PROMETHEUS_METADATA_API_LIMIT}. Use metric parameter to filter by specific metric name."
-                )
-
+        except RequestException as e:
+            logging.warn("Failed to fetch prometheus metrics", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=data,
+                status=ToolResultStatus.ERROR,
+                error=f"Network error while fetching metrics: {str(e)}",
                 params=params,
             )
         except Exception as e:
+            logging.warn("Failed to process prometheus metrics", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=str(e),
+                status=ToolResultStatus.ERROR,
+                error=f"Unexpected error: {str(e)}",
                 params=params,
             )
 
     def get_parameterized_one_liner(self, params) -> str:
-        metric = params.get("metric", "all")
-        return (
-            f"{toolset_name_for_one_liner(self.toolset.name)}: Get Metadata ({metric})"
-        )
+        name_filter = params.get("name_filter", "")
+        return f"{toolset_name_for_one_liner(self.toolset.name)}: Search Metrics ({name_filter})"
 
 
 class ExecuteInstantQuery(BasePrometheusTool):
     def __init__(self, toolset: "PrometheusToolset"):
         super().__init__(
             name="execute_prometheus_instant_query",
-            description=(
-                f"Execute an instant PromQL query (single point in time). "
-                f"Default timeout is {DEFAULT_QUERY_TIMEOUT_SECONDS} seconds "
-                f"but can be increased up to {MAX_QUERY_TIMEOUT_SECONDS} seconds for complex/slow queries."
-            ),
+            description="Execute an instant PromQL query",
             parameters={
                 "query": ToolParameter(
                     description="The PromQL query",
@@ -1082,23 +600,14 @@
                     type="string",
                     required=True,
                 ),
-                "timeout": ToolParameter(
-                    description=(
-                        f"Query timeout in seconds. Default: {DEFAULT_QUERY_TIMEOUT_SECONDS}. "
-                        f"Maximum: {MAX_QUERY_TIMEOUT_SECONDS}. "
-                        f"Increase for complex queries that may take longer."
-                    ),
-                    type="number",
-                    required=False,
-                ),
             },
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         if not self.toolset.config or not self.toolset.config.prometheus_url:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="Prometheus is not configured. Prometheus URL is missing",
                 params=params,
             )
@@ -1110,26 +619,12 @@
 
             payload = {"query": query}
 
-            # Get timeout parameter and enforce limits
-            default_timeout = self.toolset.config.default_query_timeout_seconds
-            max_timeout = self.toolset.config.max_query_timeout_seconds
-            timeout = params.get("timeout", default_timeout)
-            if timeout > max_timeout:
-                timeout = max_timeout
-                logging.warning(
-                    f"Timeout requested ({params.get('timeout')}) exceeds maximum ({max_timeout}s), using {max_timeout}s"
-                )
-            elif timeout < 1:
-                timeout = default_timeout  # Min 1 second, but use default if invalid
-
-            response = do_request(
-                config=self.toolset.config,
+            response = requests.post(
                 url=url,
                 headers=self.toolset.config.headers,
+                auth=self.toolset.config.get_auth(),
                 data=payload,
-                timeout=timeout,
-                verify=self.toolset.config.prometheus_ssl_enabled,
-                method="POST",
+                timeout=60,
             )
 
             if response.status_code == 200:
@@ -1141,64 +636,24 @@
                     error_message = (
                         "The prometheus query returned no result. Is the query correct?"
                     )
-                response_data = MetricsBasedResponse(
-                    status=status,
-                    error_message=error_message,
-                    random_key=generate_random_key(),
-                    tool_name=self.name,
-                    description=description,
-                    query=query,
-                )
-                structured_tool_result: StructuredToolResult
-                # Check if data should be included based on size
-                if self.toolset.config.tool_calls_return_data:
-                    result_data = data.get("data", {})
-                    response_data.data = result_data
-
-                    structured_tool_result = create_structured_tool_result(
-                        params=params, response=response_data
-                    )
-                    token_count = count_tool_response_tokens(
-                        llm=context.llm, structured_tool_result=structured_tool_result
-                    )
+                response_data = {
+                    "status": status,
+                    "error_message": error_message,
+                    "random_key": generate_random_key(),
+                    "tool_name": self.name,
+                    "description": description,
+                    "query": query,
+                }
 
-                    token_limit = context.max_token_count
-                    if self.toolset.config.query_response_size_limit_pct:
-                        custom_token_limit = get_pct_token_count(
-                            percent_of_total_context_window=self.toolset.config.query_response_size_limit_pct,
-                            llm=context.llm,
-                        )
-                        if custom_token_limit < token_limit:
-                            token_limit = custom_token_limit
-
-                    # Provide summary if data is too large
-                    if token_count > token_limit:
-                        response_data.data = None
-                        response_data.data_summary = (
-                            create_data_summary_for_large_result(
-                                result_data,
-                                query,
-                                token_count,
-                                is_range_query=False,
-                            )
-                        )
-                        logging.info(
-                            f"Prometheus instant query returned large dataset: "
-                            f"{response_data.data_summary.get('result_count', 0)} results, "
-                            f"{token_count:,} tokens (limit: {token_limit:,}). "
-                            f"Returning summary instead of full data."
-                        )
-                        # Also add token info to the summary for debugging
-                        response_data.data_summary["_debug_info"] = (
-                            f"Data size: {token_count:,} tokens exceeded limit of {token_limit:,} tokens"
-                        )
-                    else:
-                        response_data.data = result_data
+                if self.toolset.config.tool_calls_return_data:
+                    response_data["data"] = data.get("data")
 
-                structured_tool_result = create_structured_tool_result(
-                    params=params, response=response_data
+                data_str = json.dumps(response_data, indent=2)
+                return StructuredToolResult(
+                    status=ToolResultStatus.SUCCESS,
+                    data=data_str,
+                    params=params,
                 )
-                return structured_tool_result
 
             # Handle known Prometheus error status codes
             error_msg = "Unknown error occurred"
@@ -1211,14 +666,14 @@
                 except json.JSONDecodeError:
                     pass
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error=f"Query execution failed. HTTP {response.status_code}: {error_msg}",
                     params=params,
                 )
 
             # For other status codes, just return the status code and content
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Query execution failed with unexpected status code: {response.status_code}. Response: {str(response.content)}",
                 params=params,
             )
@@ -1226,14 +681,14 @@
         except RequestException as e:
             logging.info("Failed to connect to Prometheus", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Connection error to Prometheus: {str(e)}",
                 params=params,
             )
         except Exception as e:
             logging.info("Failed to connect to Prometheus", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Unexpected error executing query: {str(e)}",
                 params=params,
             )
@@ -1247,12 +702,7 @@
     def __init__(self, toolset: "PrometheusToolset"):
         super().__init__(
             name="execute_prometheus_range_query",
-            description=(
-                f"Generates a graph and Execute a PromQL range query. "
-                f"Default timeout is {DEFAULT_QUERY_TIMEOUT_SECONDS} seconds "
-                f"but can be increased up to {MAX_QUERY_TIMEOUT_SECONDS} seconds for complex/slow queries. "
-                f"Default time range is last 1 hour."
-            ),
+            description="Generates a graph and Execute a PromQL range query",
             parameters={
                 "query": ToolParameter(
                     description="The PromQL query",
@@ -1266,7 +716,7 @@
                 ),
                 "start": ToolParameter(
                     description=standard_start_datetime_tool_param_description(
-                        DEFAULT_GRAPH_TIME_SPAN_SECONDS
+                        DEFAULT_TIME_SPAN_SECONDS
                     ),
                     type="string",
                     required=False,
@@ -1279,40 +729,21 @@
                 "step": ToolParameter(
                     description="Query resolution step width in duration format or float number of seconds",
                     type="number",
-                    required=False,
+                    required=True,
                 ),
                 "output_type": ToolParameter(
                     description="Specifies how to interpret the Prometheus result. Use 'Plain' for raw values, 'Bytes' to format byte values, 'Percentage' to scale 01 values into 0100%, or 'CPUUsage' to convert values to cores (e.g., 500 becomes 500m, 2000 becomes 2).",
                     type="string",
                     required=True,
                 ),
-                "timeout": ToolParameter(
-                    description=(
-                        f"Query timeout in seconds. Default: {DEFAULT_QUERY_TIMEOUT_SECONDS}. "
-                        f"Maximum: {MAX_QUERY_TIMEOUT_SECONDS}. "
-                        f"Increase for complex queries that may take longer."
-                    ),
-                    type="number",
-                    required=False,
-                ),
-                "max_points": ToolParameter(
-                    description=(
-                        f"Maximum number of data points to return. Default: {int(MAX_GRAPH_POINTS)}. "
-                        f"Can be reduced to get fewer data points (e.g., 50 for simpler graphs). "
-                        f"Cannot exceed system limit of {int(MAX_GRAPH_POINTS)}. "
-                        f"If your query would return more points than this limit, the step will be automatically adjusted."
-                    ),
-                    type="number",
-                    required=False,
-                ),
             },
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         if not self.toolset.config or not self.toolset.config.prometheus_url:
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error="Prometheus is not configured. Prometheus URL is missing",
                 params=params,
             )
@@ -1324,21 +755,9 @@
             (start, end) = process_timestamps_to_rfc3339(
                 start_timestamp=params.get("start"),
                 end_timestamp=params.get("end"),
-                default_time_span_seconds=DEFAULT_GRAPH_TIME_SPAN_SECONDS,
+                default_time_span_seconds=DEFAULT_TIME_SPAN_SECONDS,
             )
-            step = parse_duration_to_seconds(params.get("step"))
-            max_points = params.get(
-                "max_points"
-            )  # Get the optional max_points parameter
-
-            # adjust_step_for_max_points handles None case and converts to float
-            step = adjust_step_for_max_points(
-                start_timestamp=start,
-                end_timestamp=end,
-                step=step,
-                max_points_override=max_points,
-            )
-
+            step = params.get("step", "")
             description = params.get("description", "")
             output_type = params.get("output_type", "Plain")
             payload = {
@@ -1348,26 +767,12 @@
                 "step": step,
             }
 
-            # Get timeout parameter and enforce limits
-            default_timeout = self.toolset.config.default_query_timeout_seconds
-            max_timeout = self.toolset.config.max_query_timeout_seconds
-            timeout = params.get("timeout", default_timeout)
-            if timeout > max_timeout:
-                timeout = max_timeout
-                logging.warning(
-                    f"Timeout requested ({params.get('timeout')}) exceeds maximum ({max_timeout}s), using {max_timeout}s"
-                )
-            elif timeout < 1:
-                timeout = default_timeout  # Min 1 second, but use default if invalid
-
-            response = do_request(
-                config=self.toolset.config,
+            response = requests.post(
                 url=url,
                 headers=self.toolset.config.headers,
+                auth=self.toolset.config.get_auth(),
                 data=payload,
-                timeout=timeout,
-                verify=self.toolset.config.prometheus_ssl_enabled,
-                method="POST",
+                timeout=120,
             )
 
             if response.status_code == 200:
@@ -1379,69 +784,29 @@
                     error_message = (
                         "The prometheus query returned no result. Is the query correct?"
                     )
-                response_data = MetricsBasedResponse(
-                    status=status,
-                    error_message=error_message,
-                    random_key=generate_random_key(),
-                    tool_name=self.name,
-                    description=description,
-                    query=query,
-                    start=start,
-                    end=end,
-                    step=step,
-                    output_type=output_type,
-                )
-
-                structured_tool_result: StructuredToolResult
+                response_data = {
+                    "status": status,
+                    "error_message": error_message,
+                    "random_key": generate_random_key(),
+                    "tool_name": self.name,
+                    "description": description,
+                    "query": query,
+                    "start": start,
+                    "end": end,
+                    "step": step,
+                    "output_type": output_type,
+                }
 
-                # Check if data should be included based on size
                 if self.toolset.config.tool_calls_return_data:
-                    result_data = data.get("data", {})
-                    response_data.data = result_data
-                    structured_tool_result = create_structured_tool_result(
-                        params=params, response=response_data
-                    )
-
-                    token_count = count_tool_response_tokens(
-                        llm=context.llm, structured_tool_result=structured_tool_result
-                    )
+                    response_data["data"] = data.get("data")
+                data_str = json.dumps(response_data, indent=2)
 
-                    token_limit = context.max_token_count
-                    if self.toolset.config.query_response_size_limit_pct:
-                        custom_token_limit = get_pct_token_count(
-                            percent_of_total_context_window=self.toolset.config.query_response_size_limit_pct,
-                            llm=context.llm,
-                        )
-                        if custom_token_limit < token_limit:
-                            token_limit = custom_token_limit
-
-                    # Provide summary if data is too large
-                    if token_count > token_limit:
-                        response_data.data = None
-                        response_data.data_summary = (
-                            create_data_summary_for_large_result(
-                                result_data, query, token_count, is_range_query=True
-                            )
-                        )
-                        logging.info(
-                            f"Prometheus range query returned large dataset: "
-                            f"{response_data.data_summary.get('series_count', 0)} series, "
-                            f"{token_count:,} tokens (limit: {token_limit:,}). "
-                            f"Returning summary instead of full data."
-                        )
-                        # Also add character info to the summary for debugging
-                        response_data.data_summary["_debug_info"] = (
-                            f"Data size: {token_count:,} tokens exceeded limit of {token_limit:,} tokens"
-                        )
-                    else:
-                        response_data.data = result_data
-
-                structured_tool_result = create_structured_tool_result(
-                    params=params, response=response_data
+                return StructuredToolResult(
+                    status=ToolResultStatus.SUCCESS,
+                    data=data_str,
+                    params=params,
                 )
 
-                return structured_tool_result
-
             error_msg = "Unknown error occurred"
             if response.status_code in [400, 429]:
                 try:
@@ -1452,13 +817,13 @@
                 except json.JSONDecodeError:
                     pass
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
+                    status=ToolResultStatus.ERROR,
                     error=f"Query execution failed. HTTP {response.status_code}: {error_msg}",
                     params=params,
                 )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Query execution failed with unexpected status code: {response.status_code}. Response: {str(response.content)}",
                 params=params,
             )
@@ -1466,14 +831,14 @@
         except RequestException as e:
             logging.info("Failed to connect to Prometheus", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Connection error to Prometheus: {str(e)}",
                 params=params,
             )
         except Exception as e:
             logging.info("Failed to connect to Prometheus", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Unexpected error executing query: {str(e)}",
                 params=params,
             )
@@ -1490,16 +855,12 @@
         super().__init__(
             name="prometheus/metrics",
             description="Prometheus integration to fetch metadata and execute PromQL queries",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/prometheus/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/prometheus.html",
             icon_url="https://upload.wikimedia.org/wikipedia/commons/3/38/Prometheus_software_logo.svg",
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
             tools=[
                 ListPrometheusRules(toolset=self),
-                GetMetricNames(toolset=self),
-                GetLabelValues(toolset=self),
-                GetAllLabels(toolset=self),
-                GetSeries(toolset=self),
-                GetMetricMetadata(toolset=self),
+                ListAvailableMetrics(toolset=self),
                 ExecuteInstantQuery(toolset=self),
                 ExecuteRangeQuery(toolset=self),
             ],
@@ -1573,13 +934,12 @@
 
         url = urljoin(self.config.prometheus_url, self.config.healthcheck)
         try:
-            response = do_request(
-                config=self.config,
+            response = requests.get(
                 url=url,
                 headers=self.config.headers,
+                auth=self.config.get_auth(),
                 timeout=10,
                 verify=self.config.prometheus_ssl_enabled,
-                method="GET",
             )
 
             if response.status_code == 200:
@@ -1590,8 +950,12 @@
                     f"Failed to connect to Prometheus at {url}: HTTP {response.status_code}",
                 )
 
+        except RequestException:
+            return (
+                False,
+                f"Failed to initialize using url={url}",
+            )
         except Exception as e:
-            logging.debug("Failed to initialize Prometheus", exc_info=True)
             return (
                 False,
                 f"Failed to initialize using url={url}. Unexpected error: {str(e)}",
Only in baseline-holmes/holmes/plugins/toolsets/prometheus: utils.py
Only in holmes2/holmes/plugins/toolsets/rabbitmq: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/rabbitmq/toolset_rabbitmq.py holmes2/holmes/plugins/toolsets/rabbitmq/toolset_rabbitmq.py
--- baseline-holmes/holmes/plugins/toolsets/rabbitmq/toolset_rabbitmq.py	2025-11-05 16:43:28.295455883 -0800
+++ holmes2/holmes/plugins/toolsets/rabbitmq/toolset_rabbitmq.py	2025-10-17 15:09:28.706599163 -0700
@@ -7,9 +7,8 @@
     CallablePrerequisite,
     StructuredToolResult,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     Toolset,
     ToolsetTag,
 )
@@ -64,7 +63,7 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         if not self.toolset.config:
             raise ValueError("RabbitMQ is not configured.")
 
@@ -78,7 +77,7 @@
             if c.connection_status == ClusterConnectionStatus.SUCCESS
         ]
         return StructuredToolResult(
-            status=StructuredToolResultStatus.SUCCESS, data=available_clusters
+            status=ToolResultStatus.SUCCESS, data=available_clusters
         )
 
     def get_parameterized_one_liner(self, params) -> str:
@@ -102,21 +101,19 @@
             toolset=toolset,
         )
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         try:
             # Fetch node details which include partition info
             cluster_config = self._get_cluster_config(
                 cluster_id=params.get("cluster_id")
             )
             result = get_cluster_status(cluster_config)
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS, data=result
-            )
+            return StructuredToolResult(status=ToolResultStatus.SUCCESS, data=result)
 
         except Exception as e:
             logging.info("Failed to process RabbitMQ cluster status", exc_info=True)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Unexpected error fetching RabbitMQ cluster status: {str(e)}",
                 data=None,
             )
@@ -133,7 +130,7 @@
         super().__init__(
             name="rabbitmq/core",
             description="Provides tools to interact with RabbitMQ to diagnose cluster health, node status, and specifically network partitions (split-brain).",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/rabbitmq/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/rabbitmq.html",
             icon_url="https://cdn.worldvectorlogo.com/logos/rabbitmq.svg",
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
             tools=[
Only in holmes2/holmes/plugins/toolsets/robusta: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/robusta/robusta_instructions.jinja2 holmes2/holmes/plugins/toolsets/robusta/robusta_instructions.jinja2
--- baseline-holmes/holmes/plugins/toolsets/robusta/robusta_instructions.jinja2	2025-11-05 16:43:28.295814424 -0800
+++ holmes2/holmes/plugins/toolsets/robusta/robusta_instructions.jinja2	2025-10-17 15:09:28.786296126 -0700
@@ -1,10 +1,7 @@
 # Configuration and alerts history
-* Use fetch_configuration_changes_metadata to get historical configuration changes in a cluster or for a specific workload.
-* If a change seems important to the investigation, Use fetch_finding_by_id with the configuration change ID to get full details of the change.
-* You must ALWAYS call fetch_configuration_changes_metadata when investigating an alert
-* Never respond without calling fetch_configuration_changes_metadata
-* When investigating a resource (pod, deployment, or job), if no relevant information is available from the live cluster at the time of investigation, call the fetch_resource_issues_metadata function to retrieve its historical alert data.
-* You can use fetch_resource_issues_metadata to get issues context for a specific kubernetes resource. Start with a 4 hours window and try to expand to 24 hours windows if nothing comes up.
+* Use fetch_configuration_changes to get historical configuration changes
+* You must ALWAYS call fetch_configuration_changes when investigating an alert
+* Never respond without calling fetch_configuration_changes
 * When investigating an alert, look at historical configuration changes that happen 4 hours before the alert started
 * If you found a change that caused the alert, you MUST write: 'The issue was introduced by ...' with a short description of the change, and the date of it.
 For example:
diff -ur baseline-holmes/holmes/plugins/toolsets/robusta/robusta.py holmes2/holmes/plugins/toolsets/robusta/robusta.py
--- baseline-holmes/holmes/plugins/toolsets/robusta/robusta.py	2025-11-05 16:43:28.295720799 -0800
+++ holmes2/holmes/plugins/toolsets/robusta/robusta.py	2025-10-17 15:09:28.788762107 -0700
@@ -3,27 +3,21 @@
 import logging
 
 from typing import Optional, Dict, Any, List
-from holmes.common.env_vars import load_bool
-from holmes.core.supabase_dal import SupabaseDal, FindingType
+from holmes.core.supabase_dal import SupabaseDal
 from holmes.core.tools import (
     StaticPrerequisite,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
     Toolset,
     ToolsetTag,
 )
-from holmes.core.tools import StructuredToolResult, StructuredToolResultStatus
-
-PULL_EXTERNAL_FINDINGS = load_bool("PULL_EXTERNAL_FINDINGS", False)
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 
 PARAM_FINDING_ID = "id"
 START_TIME = "start_datetime"
 END_TIME = "end_datetime"
 NAMESPACE = "namespace"
 WORKLOAD = "workload"
-DEFAULT_LIMIT_CHANGE_ROWS = 100
-MAX_LIMIT_CHANGE_ROWS = 200
 
 
 class FetchRobustaFinding(Tool):
@@ -32,7 +26,7 @@
     def __init__(self, dal: Optional[SupabaseDal]):
         super().__init__(
             name="fetch_finding_by_id",
-            description="Fetches a robusta finding. Findings are events, like a Prometheus alert or a deployment update and configuration change.",
+            description="Fetches a robusta finding. Findings are events, like a Prometheus alert or a deployment update",
             parameters={
                 PARAM_FINDING_ID: ToolParameter(
                     description="The id of the finding to fetch",
@@ -51,19 +45,19 @@
             logging.error(error)
             return {"error": error}
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         finding_id = params[PARAM_FINDING_ID]
         try:
             finding = self._fetch_finding(finding_id)
             if finding:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.SUCCESS,
+                    status=ToolResultStatus.SUCCESS,
                     data=finding,
                     params=params,
                 )
             else:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.NO_DATA,
+                    status=ToolResultStatus.NO_DATA,
                     data=f"Could not find a finding with finding_id={finding_id}",
                     params=params,
                 )
@@ -74,13 +68,13 @@
             )
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 data=f"There was an internal error while fetching finding {finding_id}",
                 params=params,
             )
 
     def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"Robusta: Fetch finding data {params}"
+        return "Robusta: Fetch Alert Metadata"
 
 
 class FetchResourceRecommendation(Tool):
@@ -119,18 +113,18 @@
             )
         return None
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             recommendations = self._resource_recommendation(params)
             if recommendations:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.SUCCESS,
+                    status=ToolResultStatus.SUCCESS,
                     data=recommendations,
                     params=params,
                 )
             else:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.NO_DATA,
+                    status=ToolResultStatus.NO_DATA,
                     data=f"Could not find recommendations for {params}",
                     params=params,
                 )
@@ -138,7 +132,7 @@
             msg = f"There was an internal error while fetching recommendations for {params}. {str(e)}"
             logging.exception(msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 data=msg,
                 params=params,
             )
@@ -147,180 +141,62 @@
         return f"Robusta: Check Historical Resource Utilization: ({str(params)})"
 
 
-class FetchConfigurationChangesMetadataBase(Tool):
+class FetchConfigurationChanges(Tool):
     _dal: Optional[SupabaseDal]
 
-    def __init__(
-        self,
-        dal: Optional[SupabaseDal],
-        name: str,
-        description: str,
-        add_cluster_filter: bool = True,
-    ):
-        """
-        We need seperate tools for external and cluster configuration changes due to the different cluster parameters that are not on "external" changes like 'workload' and 'namespace'.
-        add_cluster_filter: adds the namespace and workload parameters for configuration changes tool.
-        """
-        parameters = {
-            START_TIME: ToolParameter(
-                description="The starting time boundary for the search period. String in RFC3339 format.",
-                type="string",
-                required=True,
-            ),
-            END_TIME: ToolParameter(
-                description="The ending time boundary for the search period. String in RFC3339 format.",
-                type="string",
-                required=True,
-            ),
-            "limit": ToolParameter(
-                description=f"Maximum number of rows to return. Default is {DEFAULT_LIMIT_CHANGE_ROWS} and the maximum is 200",
-                type="integer",
-                required=False,
-            ),
-        }
-
-        if add_cluster_filter:
-            parameters.update(
-                {
-                    "namespace": ToolParameter(
-                        description="The Kubernetes namespace name for filtering configuration changes",
-                        type="string",
-                        required=False,
-                    ),
-                    "workload": ToolParameter(
-                        description="Kubernetes resource name to filter configuration changes (e.g., Pod, Deployment, Job, etc.). Must be the full name. For Pods, include the exact generated suffix.",
-                        type="string",
-                        required=False,
-                    ),
-                }
-            )
-
+    def __init__(self, dal: Optional[SupabaseDal]):
         super().__init__(
-            name=name,
-            description=description,
-            parameters=parameters,
+            name="fetch_configuration_changes",
+            description="Fetch configuration changes in a given time range. By default, fetch all cluster changes. Can be filtered on a given namespace or a specific workload",
+            parameters={
+                START_TIME: ToolParameter(
+                    description="The starting time boundary for the search period. String in RFC3339 format.",
+                    type="string",
+                    required=True,
+                ),
+                END_TIME: ToolParameter(
+                    description="The starting time boundary for the search period. String in RFC3339 format.",
+                    type="string",
+                    required=True,
+                ),
+            },
         )
         self._dal = dal
 
-    def _fetch_issues(
-        self,
-        params: Dict,
-        cluster: Optional[str] = None,
-        finding_type: FindingType = FindingType.CONFIGURATION_CHANGE,
-    ) -> Optional[List[Dict]]:
+    def _fetch_change_history(self, params: Dict) -> Optional[List[Dict]]:
         if self._dal and self._dal.enabled:
-            return self._dal.get_issues_metadata(
+            return self._dal.get_configuration_changes(
                 start_datetime=params["start_datetime"],
                 end_datetime=params["end_datetime"],
-                limit=min(
-                    params.get("limit") or DEFAULT_LIMIT_CHANGE_ROWS,
-                    MAX_LIMIT_CHANGE_ROWS,
-                ),
-                ns=params.get("namespace"),
-                workload=params.get("workload"),
-                cluster=cluster,
-                finding_type=finding_type,
             )
         return None
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
-            changes = self._fetch_issues(params)
+            changes = self._fetch_change_history(params)
             if changes:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.SUCCESS,
+                    status=ToolResultStatus.SUCCESS,
                     data=changes,
                     params=params,
                 )
             else:
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.NO_DATA,
-                    data=f"{self.name} found no data. {params}",
+                    status=ToolResultStatus.NO_DATA,
+                    data=f"Could not find changes for {params}",
                     params=params,
                 )
         except Exception as e:
             msg = f"There was an internal error while fetching changes for {params}. {str(e)}"
             logging.exception(msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 data=msg,
                 params=params,
             )
 
     def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"Robusta: Search Change History {params}"
-
-
-class FetchConfigurationChangesMetadata(FetchConfigurationChangesMetadataBase):
-    def __init__(self, dal: Optional[SupabaseDal]):
-        super().__init__(
-            dal=dal,
-            name="fetch_configuration_changes_metadata",
-            description=(
-                "Fetch configuration changes metadata in a given time range. "
-                "By default, fetch all cluster changes. Can be filtered on a given namespace or a specific kubernetes resource. "
-                "Use fetch_finding_by_id to get detailed change of one specific configuration change."
-            ),
-        )
-
-
-class FetchExternalConfigurationChangesMetadata(FetchConfigurationChangesMetadataBase):
-    """
-    Fetch configuration changes from external sources, e.g., LaunchDarkly changes.
-    It needs to be a seperate tool due to the different cluster parameter used in the DAL method like workload and namespace.
-    """
-
-    def __init__(self, dal: Optional[SupabaseDal]):
-        super().__init__(
-            dal=dal,
-            name="fetch_external_configuration_changes_metadata",
-            description=(
-                "Fetch external configuration changes metadata in a given time range. "
-                "Fetches configuration changes from external sources. "
-                "Use fetch_finding_by_id to get detailed change of one specific configuration change."
-            ),
-            add_cluster_filter=False,
-        )
-
-    def _fetch_issues(self, params: Dict) -> Optional[List[Dict]]:  # type: ignore
-        return super()._fetch_issues(params, cluster="external")
-
-    def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"Robusta: Search External Change History {params}"
-
-
-class FetchResourceIssuesMetadata(FetchConfigurationChangesMetadataBase):
-    def __init__(self, dal: Optional[SupabaseDal]):
-        super().__init__(
-            dal=dal,
-            name="fetch_resource_issues_metadata",
-            description=(
-                "Fetch issues and alert metadata in a given time range. "
-                "Must be filtered on a given namespace and specific kubernetes resource, such as pod, deployment, job, etc. "
-                "Use fetch_finding_by_id to get further information on a specific issue or alert."
-            ),
-            add_cluster_filter=False,
-        )
-        self.parameters.update(
-            {
-                "namespace": ToolParameter(
-                    description="The Kubernetes namespace name for filtering issues and alerts",
-                    type="string",
-                    required=True,
-                ),
-                "workload": ToolParameter(
-                    description="Kubernetes resource name to filter issues and alerts (e.g., Pod, Deployment, Job, etc.). Must be the full name. For Pods, include the exact generated suffix.",
-                    type="string",
-                    required=True,
-                ),
-            }
-        )
-
-    def _fetch_issues(self, params: Dict) -> Optional[List[Dict]]:  # type: ignore
-        return super()._fetch_issues(params, finding_type=FindingType.ISSUE)
-
-    def get_parameterized_one_liner(self, params: Dict) -> str:
-        return f"Robusta: fetch resource issues metadata {params}"
+        return "Robusta: Search Change History"
 
 
 class RobustaToolset(Toolset):
@@ -334,23 +210,17 @@
                 enabled=dal.enabled, disabled_reason="Data access layer is disabled"
             )
 
-        tools = [
-            FetchRobustaFinding(dal),
-            FetchConfigurationChangesMetadata(dal),
-            FetchResourceRecommendation(dal),
-            FetchResourceIssuesMetadata(dal),
-        ]
-
-        if PULL_EXTERNAL_FINDINGS:
-            tools.append(FetchExternalConfigurationChangesMetadata(dal))
-
         super().__init__(
             icon_url="https://cdn.prod.website-files.com/633e9bac8f71dfb7a8e4c9a6/646be7710db810b14133bdb5_logo.svg",
             description="Fetches alerts metadata and change history",
-            docs_url="https://holmesgpt.dev/data-sources/builtin-toolsets/robusta/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/robusta.html",
             name="robusta",
             prerequisites=[dal_prereq],
-            tools=tools,
+            tools=[
+                FetchRobustaFinding(dal),
+                FetchConfigurationChanges(dal),
+                FetchResourceRecommendation(dal),
+            ],
             tags=[
                 ToolsetTag.CORE,
             ],
Only in holmes2/holmes/plugins/toolsets/runbook: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/runbook/runbook_fetcher.py holmes2/holmes/plugins/toolsets/runbook/runbook_fetcher.py
--- baseline-holmes/holmes/plugins/toolsets/runbook/runbook_fetcher.py	2025-11-05 16:43:28.296138339 -0800
+++ holmes2/holmes/plugins/toolsets/runbook/runbook_fetcher.py	2025-10-17 15:09:28.710457133 -0700
@@ -1,170 +1,61 @@
 import logging
-import os
 import textwrap
 from typing import Any, Dict, List, Optional
-from holmes.core.supabase_dal import SupabaseDal
+
 from holmes.core.tools import (
     StructuredToolResult,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
-    StructuredToolResultStatus,
+    ToolResultStatus,
     Toolset,
     ToolsetTag,
 )
 
-from holmes.plugins.runbooks import (
-    get_runbook_by_path,
-    load_runbook_catalog,
-    DEFAULT_RUNBOOK_SEARCH_PATH,
-)
+from holmes.plugins.runbooks import get_runbook_by_path, DEFAULT_RUNBOOK_SEARCH_PATH
 from holmes.plugins.toolsets.utils import toolset_name_for_one_liner
 
 
+# TODO(mainred): currently we support fetch runbooks hosted internally, in the future we may want to support fetching
+# runbooks from external sources as well.
 class RunbookFetcher(Tool):
     toolset: "RunbookToolset"
-    available_runbooks: List[str] = []
-    additional_search_paths: Optional[List[str]] = None
-    _dal: Optional[SupabaseDal] = None
-
-    def __init__(
-        self,
-        toolset: "RunbookToolset",
-        additional_search_paths: Optional[List[str]] = None,
-        dal: Optional[SupabaseDal] = None,
-    ):
-        catalog = load_runbook_catalog(dal=dal)
-        available_runbooks = []
-        if catalog:
-            available_runbooks = catalog.list_available_runbooks()
-
-        if additional_search_paths:
-            for search_path in additional_search_paths:
-                if not os.path.isdir(search_path):
-                    continue
-
-                for file in os.listdir(search_path):
-                    if file.endswith(".md") and file not in available_runbooks:
-                        available_runbooks.append(f"{file}")
-
-        runbook_list = ", ".join([f'"{rb}"' for rb in available_runbooks])
 
+    def __init__(self, toolset: "RunbookToolset"):
         super().__init__(
             name="fetch_runbook",
             description="Get runbook content by runbook link. Use this to get troubleshooting steps for incidents",
             parameters={
-                "runbook_id": ToolParameter(
-                    description=f"The runbook_id: either a UUID or a .md filename. Must be one of: {runbook_list}",
+                # use link as a more generic term for runbook path, considering we may have external links in the future
+                "link": ToolParameter(
+                    description="The link to the runbook",
                     type="string",
                     required=True,
                 ),
             },
-            toolset=toolset,  # type: ignore[call-arg]
-            available_runbooks=available_runbooks,  # type: ignore[call-arg]
-            additional_search_paths=additional_search_paths,  # type: ignore[call-arg]
+            toolset=toolset,  # type: ignore
         )
-        self._dal = dal
-
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
-        runbook_id: str = params.get("runbook_id", "")
-        is_md_file: bool = True if runbook_id.endswith(".md") else False
 
-        # Validate link is not empty
-        if not runbook_id or not runbook_id.strip():
-            err_msg = (
-                "Runbook link cannot be empty. Please provide a valid runbook path."
-            )
-            logging.error(err_msg)
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=err_msg,
-                params=params,
-            )
+    def _invoke(self, params: Any) -> StructuredToolResult:
+        link: str = params["link"]
 
-        if is_md_file:
-            return self._get_md_runbook(runbook_id, params)
-        else:
-            return self._get_robusta_runbook(runbook_id, params)
-
-    def _get_robusta_runbook(self, link: str, params: dict) -> StructuredToolResult:
-        if self._dal and self._dal.enabled:
-            try:
-                runbook_content = self._dal.get_runbook_content(link)
-                if runbook_content:
-                    return StructuredToolResult(
-                        status=StructuredToolResultStatus.SUCCESS,
-                        data=runbook_content.pretty(),
-                        params=params,
-                    )
-                else:
-                    err_msg = f"Runbook with UUID '{link}' not found in remote storage."
-                    logging.error(err_msg)
-                    return StructuredToolResult(
-                        status=StructuredToolResultStatus.ERROR,
-                        error=err_msg,
-                        params=params,
-                    )
-            except Exception as e:
-                err_msg = f"Failed to fetch runbook with UUID '{link}': {str(e)}"
-                logging.error(err_msg)
-                return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
-                    error=err_msg,
-                    params=params,
-                )
-        else:
-            err_msg = "Runbook link appears to be a UUID, but no remote data access layer (dal) is enabled."
-            logging.error(err_msg)
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=err_msg,
-                params=params,
-            )
-
-    def _get_md_runbook(self, link: str, params: dict) -> StructuredToolResult:
         search_paths = [DEFAULT_RUNBOOK_SEARCH_PATH]
-        if self.additional_search_paths:
-            search_paths.extend(self.additional_search_paths)
-        # Validate link is in the available runbooks list OR is a valid path within allowed directories
-        if link not in self.available_runbooks:
-            # Check if the link would resolve to a valid path within allowed directories
-            # This prevents path traversal attacks like ../../secret.md
-            is_valid_path = False
-            for search_path in search_paths:
-                candidate_path = os.path.join(search_path, link)
-                # Canonicalize both paths to resolve any .. or . components
-                real_search_path = os.path.realpath(search_path)
-                real_candidate_path = os.path.realpath(candidate_path)
-
-                # Check if the resolved path is within the allowed directory
-                if (
-                    real_candidate_path.startswith(real_search_path + os.sep)
-                    or real_candidate_path == real_search_path
-                ):
-                    if os.path.isfile(real_candidate_path):
-                        is_valid_path = True
-                        break
-
-            if not is_valid_path:
-                err_msg = f"Invalid runbook link '{link}'. Must be one of: {', '.join(self.available_runbooks) if self.available_runbooks else 'No runbooks available'}"
-                logging.error(err_msg)
-                return StructuredToolResult(
-                    status=StructuredToolResultStatus.ERROR,
-                    error=err_msg,
-                    params=params,
-                )
+        if self.toolset.config and "additional_search_paths" in self.toolset.config:
+            search_paths.extend(self.toolset.config["additional_search_paths"])
 
         runbook_path = get_runbook_by_path(link, search_paths)
+
         if runbook_path is None:
             err_msg = (
                 f"Runbook '{link}' not found in any of the search paths: {search_paths}"
             )
             logging.error(err_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=err_msg,
                 params=params,
             )
+
+        # Read and return the runbook content
         try:
             with open(runbook_path, "r") as file:
                 content = file.read()
@@ -203,7 +94,7 @@
                     </example>
                 """)
                 return StructuredToolResult(
-                    status=StructuredToolResultStatus.SUCCESS,
+                    status=ToolResultStatus.SUCCESS,
                     data=wrapped_content,
                     params=params,
                 )
@@ -211,23 +102,19 @@
             err_msg = f"Failed to read runbook {runbook_path}: {str(e)}"
             logging.error(err_msg)
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=err_msg,
                 params=params,
             )
 
     def get_parameterized_one_liner(self, params) -> str:
-        path: str = params.get("runbook_id", "")
+        path: str = params.get("link", "")
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Fetch Runbook {path}"
 
 
 class RunbookToolset(Toolset):
-    def __init__(
-        self,
-        dal: Optional[SupabaseDal],
-        additional_search_paths: Optional[List[str]] = None,
-    ):
-        # Store additional search paths in config for RunbookFetcher to access
+    def __init__(self, additional_search_paths: Optional[List[str]] = None):
+        # Store additional search paths in config
         config = {}
         if additional_search_paths:
             config["additional_search_paths"] = additional_search_paths
@@ -237,9 +124,9 @@
             description="Fetch runbooks",
             icon_url="https://platform.robusta.dev/demos/runbook.svg",
             tools=[
-                RunbookFetcher(self, additional_search_paths, dal),
+                RunbookFetcher(self),
             ],
-            docs_url="https://holmesgpt.dev/data-sources/",
+            docs_url="https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/runbook.html",
             tags=[
                 ToolsetTag.CORE,
             ],
diff -ur baseline-holmes/holmes/plugins/toolsets/service_discovery.py holmes2/holmes/plugins/toolsets/service_discovery.py
--- baseline-holmes/holmes/plugins/toolsets/service_discovery.py	2025-11-05 16:43:28.296240756 -0800
+++ holmes2/holmes/plugins/toolsets/service_discovery.py	2025-10-17 15:09:28.833743173 -0700
@@ -36,7 +36,7 @@
         port = svc.spec.ports[0].port
         url = f"http://{name}.{namespace}.svc.{CLUSTER_DOMAIN}:{port}"
         logging.info(
-            f"Discovered service with label-selector: `{label_selector}` at url: `{url}`"
+            f"discovered service with label-selector: `{label_selector}` at url: `{url}`"
         )
         return url
     except Exception:
Only in holmes2/holmes/plugins/toolsets/servicenow: __pycache__
diff -ur baseline-holmes/holmes/plugins/toolsets/servicenow/servicenow.py holmes2/holmes/plugins/toolsets/servicenow/servicenow.py
--- baseline-holmes/holmes/plugins/toolsets/servicenow/servicenow.py	2025-11-05 16:43:28.296659213 -0800
+++ holmes2/holmes/plugins/toolsets/servicenow/servicenow.py	2025-10-17 15:09:28.830575240 -0700
@@ -5,14 +5,13 @@
 from holmes.core.tools import (
     CallablePrerequisite,
     Tool,
-    ToolInvokeContext,
     ToolParameter,
     Toolset,
     ToolsetTag,
 )
 
 from pydantic import BaseModel, PrivateAttr
-from holmes.core.tools import StructuredToolResult, StructuredToolResultStatus
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 from holmes.plugins.toolsets.utils import (
     process_timestamps_to_rfc3339,
     standard_start_datetime_tool_param_description,
@@ -37,6 +36,7 @@
     def __init__(self):
         super().__init__(
             prerequisites=[CallablePrerequisite(callable=self.prerequisites_callable)],
+            experimental=True,
             tools=[
                 ReturnChangesInTimerange(toolset=self),
                 ReturnChange(toolset=self),
@@ -56,7 +56,7 @@
             self.config: Dict = ServiceNowConfig(**config).model_dump()
             self._session.headers.update(
                 {
-                    "x-sn-apikey": self.config.get("api_key"),  # type: ignore
+                    "x-sn-apikey": self.config.get("api_key"),
                 }
             )
 
@@ -86,9 +86,9 @@
         response.raise_for_status()
         res = response.json()
         return StructuredToolResult(
-            status=StructuredToolResultStatus.SUCCESS
+            status=ToolResultStatus.SUCCESS
             if res.get(field, [])
-            else StructuredToolResultStatus.NO_DATA,
+            else ToolResultStatus.NO_DATA,
             data=res,
             params=params,
         )
@@ -115,7 +115,7 @@
         start = params.get("start", "last hour")
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Get Change Requests ({start})"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         parsed_params = {}
         try:
             (start, _) = process_timestamps_to_rfc3339(
@@ -137,7 +137,7 @@
         except Exception as e:
             logging.exception(self.get_parameterized_one_liner(params))
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 data=f"Exception {self.name}: {str(e)}",
                 params=params,
             )
@@ -158,7 +158,7 @@
         sys_id = params.get("sys_id", "")
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Get Change Details ({sys_id})"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         try:
             url = "https://{instance}.service-now.com/api/now/v2/table/change_request/{sys_id}".format(
                 instance=self.toolset.config.get("instance"),
@@ -169,7 +169,7 @@
         except Exception as e:
             logging.exception(self.get_parameterized_one_liner(params))
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 data=f"Exception {self.name}: {str(e)}",
                 params=params,
             )
@@ -190,7 +190,7 @@
         keyword = params.get("keyword", "")
         return f"{toolset_name_for_one_liner(self.toolset.name)}: Search Changes ({keyword})"
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Any) -> StructuredToolResult:
         parsed_params = {}
         try:
             url = f"https://{self.toolset.config.get('instance')}.service-now.com/api/now/v2/table/change_request"
@@ -207,7 +207,7 @@
         except Exception as e:
             logging.exception(self.get_parameterized_one_liner(params))
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 data=f"Exception {self.name}: {str(e)}",
                 params=params,
             )
diff -ur baseline-holmes/holmes/plugins/toolsets/slab.yaml holmes2/holmes/plugins/toolsets/slab.yaml
--- baseline-holmes/holmes/plugins/toolsets/slab.yaml	2025-11-05 16:43:28.296767879 -0800
+++ holmes2/holmes/plugins/toolsets/slab.yaml	2025-10-17 15:09:28.677235850 -0700
@@ -1,7 +1,7 @@
 toolsets:
   slab:
     description: "Fetches slab pages"
-    docs_url: "https://holmesgpt.dev/data-sources/builtin-toolsets/slab/"
+    docs_url: "https://docs.robusta.dev/master/configuration/holmesgpt/toolsets/slab.html"
     icon_url: "https://platform.robusta.dev/demos/slab-mark.svg"
     tags:
       - core
diff -ur baseline-holmes/holmes/plugins/toolsets/utils.py holmes2/holmes/plugins/toolsets/utils.py
--- baseline-holmes/holmes/plugins/toolsets/utils.py	2025-11-05 16:43:28.296949920 -0800
+++ holmes2/holmes/plugins/toolsets/utils.py	2025-10-17 15:09:28.789346436 -0700
@@ -1,7 +1,5 @@
 import datetime
-import math
 import time
-import re
 from typing import Dict, Optional, Tuple, Union
 
 from dateutil import parser
@@ -136,92 +134,6 @@
     return (start, end)  # type: ignore
 
 
-def seconds_to_duration_string(seconds: int) -> str:
-    """Convert seconds into a compact duration string like '2h30m15s'.
-    If the value is less than 1 minute, return just the number of seconds (e.g. '45').
-    """
-    if seconds < 0:
-        raise ValueError("seconds must be non-negative")
-
-    parts = []
-    weeks, seconds = divmod(seconds, 7 * 24 * 3600)
-    days, seconds = divmod(seconds, 24 * 3600)
-    hours, seconds = divmod(seconds, 3600)
-    minutes, seconds = divmod(seconds, 60)
-
-    if weeks:
-        parts.append(f"{weeks}w")
-    if days:
-        parts.append(f"{days}d")
-    if hours:
-        parts.append(f"{hours}h")
-    if minutes:
-        parts.append(f"{minutes}m")
-    if seconds or not parts:
-        parts.append(f"{seconds}s")
-
-    return "".join(parts)
-
-
-def duration_string_to_seconds(duration_string: str) -> int:
-    """Convert a duration string like '2h30m15s' or '300' into total seconds.
-    A bare integer string is treated as seconds.
-    """
-    if not duration_string:
-        raise ValueError("duration_string cannot be empty")
-
-    # Pure number? Assume seconds
-    if duration_string.isdigit():
-        return int(duration_string)
-
-    pattern = re.compile(r"(?P<value>\d+)(?P<unit>[wdhms])")
-    matches = pattern.findall(duration_string)
-    if not matches:
-        raise ValueError(f"Invalid duration string: {duration_string}")
-
-    unit_multipliers = {
-        "w": 7 * 24 * 3600,
-        "d": 24 * 3600,
-        "h": 3600,
-        "m": 60,
-        "s": 1,
-    }
-
-    total_seconds = 0
-    for value, unit in matches:
-        if unit not in unit_multipliers:
-            raise ValueError(f"Unknown unit: {unit}")
-        total_seconds += int(value) * unit_multipliers[unit]
-
-    return total_seconds
-
-
-def adjust_step_for_max_points(
-    time_range_seconds: int,
-    max_points: int,
-    step: Optional[int] = None,
-) -> int:
-    """
-    Adjusts the step parameter to ensure the number of data points doesn't exceed max_points.
-
-    Args:
-        time_range_seconds: time range in seconds
-        step: The requested step duration in seconds
-        max_points: The requested maximum number of data points
-
-    Returns:
-        Adjusted step value in seconds that ensures points <= max_points
-    """
-    smallest_allowed_step = int(
-        math.ceil(float(time_range_seconds) / float(max_points))
-    )
-
-    if not step:
-        return smallest_allowed_step
-
-    return max(smallest_allowed_step, step)
-
-
 def get_param_or_raise(dict: Dict, param: str) -> str:
     value = dict.get(param)
     if not value:
Only in holmes2/holmes/utils: __pycache__
Only in baseline-holmes/holmes/utils: config_utils.py
Only in holmes2/holmes/utils/console: __pycache__
diff -ur baseline-holmes/holmes/utils/env.py holmes2/holmes/utils/env.py
--- baseline-holmes/holmes/utils/env.py	2025-11-05 16:43:28.298387374 -0800
+++ holmes2/holmes/utils/env.py	2025-10-17 15:09:28.836749067 -0700
@@ -6,13 +6,6 @@
 from pydantic import SecretStr
 
 
-def environ_get_safe_int(env_var: str, default: str = "0") -> int:
-    try:
-        return max(int(os.environ.get(env_var, default)), 0)
-    except ValueError:
-        return int(default)
-
-
 def get_env_replacement(value: str) -> Optional[str]:
     env_patterns = re.findall(r"{{\s*env\.([^}]*)\s*}}", value)
 
diff -ur baseline-holmes/holmes/utils/global_instructions.py holmes2/holmes/utils/global_instructions.py
--- baseline-holmes/holmes/utils/global_instructions.py	2025-11-05 16:43:28.298577082 -0800
+++ holmes2/holmes/utils/global_instructions.py	2025-10-17 15:09:28.835824282 -0700
@@ -1,85 +1,20 @@
-from typing import Optional, List, TYPE_CHECKING
-from pydantic import BaseModel
-from holmes.plugins.prompts import load_and_render_prompt
-from holmes.plugins.runbooks import RunbookCatalog
+from typing import List, Optional
 
-if TYPE_CHECKING:
-    from holmes.core.resource_instruction import ResourceInstructions
+from pydantic import BaseModel
 
 
 class Instructions(BaseModel):
     instructions: List[str] = []
 
 
-def _format_instructions_block(
-    items: List[str], header: str = "My instructions to check:"
-) -> str:
-    lines = [f"* {s}" for s in items if isinstance(s, str) and s.strip()]
-    if not lines:
-        return ""
-    bullets = "\n".join(lines) + "\n"
-    return f"{header}\n{bullets}"
-
-
-def _format_resource_instructions(
-    resource_instructions: Optional["ResourceInstructions"],
-) -> List[str]:  # type: ignore
-    items = []
-    if resource_instructions is not None:
-        if getattr(resource_instructions, "instructions", None):
-            items.extend(resource_instructions.instructions)
-        if getattr(resource_instructions, "documents", None):
-            for document in resource_instructions.documents:
-                items.append(f"fetch information from this URL: {document.url}")
-    return items
-
-
-def add_runbooks_to_user_prompt(
-    user_prompt: str,
-    runbook_catalog: Optional[RunbookCatalog],
-    global_instructions: Optional[Instructions] = None,
-    issue_instructions: Optional[List[str]] = None,
-    resource_instructions: Optional["ResourceInstructions"] = None,  # type: ignore
+def add_global_instructions_to_user_prompt(
+    user_prompt: str, global_instructions: Optional[Instructions]
 ) -> str:
     if (
-        not runbook_catalog
-        and not issue_instructions
-        and not resource_instructions
-        and not global_instructions
+        global_instructions
+        and global_instructions.instructions
+        and len(global_instructions.instructions[0]) > 0
     ):
-        return user_prompt
-
-    catalog_str = runbook_catalog.to_prompt_string() if runbook_catalog else ""
-
-    # Combine and format all instructions
-    combined_instructions = []
-    if issue_instructions:
-        combined_instructions.extend(issue_instructions)
-    combined_instructions.extend(_format_resource_instructions(resource_instructions))
-    issue_block = (
-        _format_instructions_block(combined_instructions)
-        if combined_instructions
-        else ""
-    )
-
-    gi_list = getattr(global_instructions, "instructions", None) or []
-    global_block = (
-        _format_instructions_block(
-            [s for s in gi_list if isinstance(s, str)], header=""
-        )
-        if gi_list
-        else ""
-    )
-
-    rendered = load_and_render_prompt(
-        "builtin://_runbook_instructions.jinja2",
-        context={
-            "runbook_catalog": catalog_str,
-            "custom_instructions": issue_block,
-            "global_instructions": global_block,
-        },
-    )
-
-    if user_prompt and not user_prompt.endswith("\n"):
-        user_prompt += "\n"
-    return f"{user_prompt}\n{rendered}"
+        instructions = "\n\n".join(global_instructions.instructions)
+        user_prompt += f"\n\nGlobal Instructions (use if relevant): {instructions}\n"
+    return user_prompt
diff -ur baseline-holmes/holmes/utils/holmes_status.py holmes2/holmes/utils/holmes_status.py
--- baseline-holmes/holmes/utils/holmes_status.py	2025-11-05 16:43:28.298664665 -0800
+++ holmes2/holmes/utils/holmes_status.py	2025-10-17 15:09:28.848917680 -0700
@@ -1,4 +1,3 @@
-import json
 from holmes.core.supabase_dal import SupabaseDal
 from holmes.config import Config
 from holmes import get_version  # type: ignore
@@ -17,7 +16,7 @@
     dal.upsert_holmes_status(
         {
             "cluster_id": config.cluster_name,
-            "model": json.dumps(config.get_models_list()),
+            "model": config.get_models_list(),
             "version": get_version(),
         }
     )
Only in baseline-holmes/holmes/utils: sentry_helper.py
diff -ur baseline-holmes/holmes/utils/stream.py holmes2/holmes/utils/stream.py
--- baseline-holmes/holmes/utils/stream.py	2025-11-05 16:43:28.299745829 -0800
+++ holmes2/holmes/utils/stream.py	2025-10-17 15:09:28.847092570 -0700
@@ -1,15 +1,10 @@
 import json
 from enum import Enum
-from typing import Generator, Optional, List, Union
+from typing import Generator, Optional, List
 import litellm
 from pydantic import BaseModel, Field
 from holmes.core.investigation_structured_output import process_response_into_sections
 from functools import partial
-import logging
-from litellm.litellm_core_utils.streaming_handler import CustomStreamWrapper
-from litellm.types.utils import ModelResponse, TextCompletionResponse
-
-from holmes.core.llm import TokenCountMetadata, get_llm_usage
 
 
 class StreamEvents(str, Enum):
@@ -18,9 +13,6 @@
     TOOL_RESULT = "tool_calling_result"
     ERROR = "error"
     AI_MESSAGE = "ai_message"
-    APPROVAL_REQUIRED = "approval_required"
-    TOKEN_COUNT = "token_count"
-    CONVERSATION_HISTORY_COMPACTED = "conversation_history_compacted"
 
 
 class StreamMessage(BaseModel):
@@ -69,7 +61,6 @@
                         "sections": sections or {},
                         "analysis": text_response,
                         "instructions": runbooks or [],
-                        "metadata": message.data.get("metadata") or {},
                     },
                 )
             else:
@@ -85,60 +76,15 @@
     try:
         for message in call_stream:
             if message.event == StreamEvents.ANSWER_END:
-                response_data = {
-                    "analysis": message.data.get("content"),
-                    "conversation_history": message.data.get("messages"),
-                    "follow_up_actions": followups,
-                    "metadata": message.data.get("metadata") or {},
-                }
-
-                yield create_sse_message(StreamEvents.ANSWER_END.value, response_data)
-            elif message.event == StreamEvents.APPROVAL_REQUIRED:
-                response_data = {
-                    "analysis": message.data.get("content"),
-                    "conversation_history": message.data.get("messages"),
-                    "follow_up_actions": followups,
-                }
-
-                response_data["requires_approval"] = True
-                response_data["pending_approvals"] = message.data.get(
-                    "pending_approvals", []
-                )
-
                 yield create_sse_message(
-                    StreamEvents.APPROVAL_REQUIRED.value, response_data
+                    StreamEvents.ANSWER_END.value,
+                    {
+                        "analysis": message.data.get("content"),
+                        "conversation_history": message.data.get("messages"),
+                        "follow_up_actions": followups,
+                    },
                 )
             else:
                 yield create_sse_message(message.event.value, message.data)
     except litellm.exceptions.RateLimitError as e:
         yield create_rate_limit_error_message(str(e))
-    except Exception as e:
-        logging.error(e)
-        if "Model is getting throttled" in str(e):  # happens for bedrock
-            yield create_rate_limit_error_message(str(e))
-        else:
-            yield create_sse_error_message(description=str(e), error_code=1, msg=str(e))
-
-
-def add_token_count_to_metadata(
-    tokens: TokenCountMetadata,
-    metadata: dict,
-    max_context_size: int,
-    maximum_output_token: int,
-    full_llm_response: Union[
-        ModelResponse, CustomStreamWrapper, TextCompletionResponse
-    ],
-):
-    metadata["usage"] = get_llm_usage(full_llm_response)
-    metadata["tokens"] = tokens.model_dump()
-    metadata["max_tokens"] = max_context_size
-    metadata["max_output_tokens"] = maximum_output_token
-
-
-def build_stream_event_token_count(metadata: dict) -> StreamMessage:
-    return StreamMessage(
-        event=StreamEvents.TOKEN_COUNT,
-        data={
-            "metadata": metadata,
-        },
-    )
diff -ur baseline-holmes/holmes/version.py holmes2/holmes/version.py
--- baseline-holmes/holmes/version.py	2025-11-05 16:43:28.300473535 -0800
+++ holmes2/holmes/version.py	2025-10-17 15:09:28.642199914 -0700
@@ -57,41 +57,11 @@
         return __version__
 
     # we are running from an unreleased dev version
-    archival_file_path = os.path.join(this_path, ".git_archival.json")
-    if os.path.exists(archival_file_path):
-        try:
-            with open(archival_file_path, "r") as f:
-                archival_data = json.load(f)
-                refs = archival_data.get("refs", "")
-                hash_short = archival_data.get("hash-short", "")
-
-                # Check if Git substitution didn't happen (placeholders are still present)
-                if "$Format:" in refs or "$Format:" in hash_short:
-                    # Placeholders not substituted, skip to next method
-                    pass
-                else:
-                    # Valid archival data found
-                    return f"dev-{refs}-{hash_short}"
-        except Exception:
-            pass
-
-    # Now try git commands for development environments
     try:
-        env = os.environ.copy()
-        # Set ceiling to prevent walking up beyond the project root
-        # We want to allow access to holmes/.git but not beyond holmes
-        project_root = os.path.dirname(this_path)  # holmes
-        env["GIT_CEILING_DIRECTORIES"] = os.path.dirname(
-            project_root
-        )  # holmes's parent
-
         # Get the latest git tag
         tag = (
             subprocess.check_output(
-                ["git", "describe", "--tags"],
-                stderr=subprocess.STDOUT,
-                cwd=this_path,
-                env=env,
+                ["git", "describe", "--tags"], stderr=subprocess.STDOUT, cwd=this_path
             )
             .decode()
             .strip()
@@ -103,7 +73,6 @@
                 ["git", "rev-parse", "--abbrev-ref", "HEAD"],
                 stderr=subprocess.STDOUT,
                 cwd=this_path,
-                env=env,
             )
             .decode()
             .strip()
@@ -115,7 +84,6 @@
                 ["git", "status", "--porcelain"],
                 stderr=subprocess.STDOUT,
                 cwd=this_path,
-                env=env,
             )
             .decode()
             .strip()
@@ -127,7 +95,19 @@
     except Exception:
         pass
 
-    return "dev-unknown"
+    # we are running without git history, but we still might have git archival data (e.g. if we were pip installed)
+    archival_file_path = os.path.join(this_path, ".git_archival.json")
+    if os.path.exists(archival_file_path):
+        try:
+            with open(archival_file_path, "r") as f:
+                archival_data = json.load(f)
+                return f"dev-{archival_data['refs']}-{archival_data['hash-short']}"
+        except Exception:
+            pass
+
+        return "dev-version"
+
+    return "unknown-version"
 
 
 @cache
