diff --git a/holmes/core/llm.py b/holmes/core/llm.py
index 8b912a21..f4f4f169 100644
--- a/holmes/core/llm.py
+++ b/holmes/core/llm.py
@@ -1,92 +1,32 @@
 import json
 import logging
-import os
 from abc import abstractmethod
-from math import floor
-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Type, Union
+from typing import Any, Dict, List, Optional, Type, Union
 
-import litellm
-from litellm.litellm_core_utils.streaming_handler import CustomStreamWrapper
-from litellm.types.utils import ModelResponse, TextCompletionResponse
+from litellm.types.utils import ModelResponse
 import sentry_sdk
-from pydantic import BaseModel, ConfigDict, SecretStr
-from typing_extensions import Self
-
-from holmes.clients.robusta_client import (
-    RobustaModel,
-    RobustaModelsResponse,
-    fetch_robusta_models,
-)
 
+from litellm.litellm_core_utils.streaming_handler import CustomStreamWrapper
+from pydantic import BaseModel
+import litellm
+import os
 from holmes.common.env_vars import (
-    FALLBACK_CONTEXT_WINDOW_SIZE,
-    LOAD_ALL_ROBUSTA_MODELS,
     REASONING_EFFORT,
-    ROBUSTA_AI,
-    ROBUSTA_API_ENDPOINT,
     THINKING,
-    EXTRA_HEADERS,
-    TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_PCT,
-    TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_TOKENS,
 )
-from holmes.core.supabase_dal import SupabaseDal
-from holmes.utils.env import environ_get_safe_int, replace_env_vars_values
-from holmes.utils.file_utils import load_yaml_file
 
-if TYPE_CHECKING:
-    from holmes.config import Config
 
-MODEL_LIST_FILE_LOCATION = os.environ.get(
-    "MODEL_LIST_FILE_LOCATION", "/etc/holmes/config/model_list.yaml"
-)
+def environ_get_safe_int(env_var, default="0"):
+    try:
+        return max(int(os.environ.get(env_var, default)), 0)
+    except ValueError:
+        return int(default)
 
 
 OVERRIDE_MAX_OUTPUT_TOKEN = environ_get_safe_int("OVERRIDE_MAX_OUTPUT_TOKEN")
 OVERRIDE_MAX_CONTENT_SIZE = environ_get_safe_int("OVERRIDE_MAX_CONTENT_SIZE")
 
 
-def get_context_window_compaction_threshold_pct() -> int:
-    """Get the compaction threshold percentage at runtime to support test overrides."""
-    return environ_get_safe_int("CONTEXT_WINDOW_COMPACTION_THRESHOLD_PCT", default="95")
-
-
-ROBUSTA_AI_MODEL_NAME = "Robusta"
-
-
-class TokenCountMetadata(BaseModel):
-    total_tokens: int
-    tools_tokens: int
-    system_tokens: int
-    user_tokens: int
-    tools_to_call_tokens: int
-    assistant_tokens: int
-    other_tokens: int
-
-
-class ModelEntry(BaseModel):
-    """ModelEntry represents a single LLM model configuration."""
-
-    model: str
-    # TODO: the name field seems to be redundant, can we remove it?
-    name: Optional[str] = None
-    api_key: Optional[SecretStr] = None
-    base_url: Optional[str] = None
-    is_robusta_model: Optional[bool] = None
-    custom_args: Optional[Dict[str, Any]] = None
-
-    # LLM configurations used services like Azure OpenAI Service
-    api_base: Optional[str] = None
-    api_version: Optional[str] = None
-
-    model_config = ConfigDict(
-        extra="allow",
-    )
-
-    @classmethod
-    def load_from_dict(cls, data: dict) -> Self:
-        return cls.model_validate(data)
-
-
 class LLM:
     @abstractmethod
     def __init__(self):
@@ -100,23 +40,8 @@ class LLM:
     def get_maximum_output_token(self) -> int:
         pass
 
-    def get_max_token_count_for_single_tool(self) -> int:
-        if (
-            0 < TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_PCT
-            and TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_PCT <= 100
-        ):
-            context_window_size = self.get_context_window_size()
-            calculated_max_tokens = int(
-                context_window_size * TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_PCT // 100
-            )
-            return min(calculated_max_tokens, TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_TOKENS)
-        else:
-            return TOOL_MAX_ALLOCATED_CONTEXT_WINDOW_TOKENS
-
     @abstractmethod
-    def count_tokens(
-        self, messages: list[dict], tools: Optional[list[dict[str, Any]]] = None
-    ) -> TokenCountMetadata:
+    def count_tokens_for_message(self, messages: list[dict]) -> int:
         pass
 
     @abstractmethod
@@ -136,55 +61,31 @@ class LLM:
 class DefaultLLM(LLM):
     model: str
     api_key: Optional[str]
-    api_base: Optional[str]
-    api_version: Optional[str]
+    base_url: Optional[str]
     args: Dict
-    is_robusta_model: bool
 
     def __init__(
         self,
         model: str,
         api_key: Optional[str] = None,
-        api_base: Optional[str] = None,
-        api_version: Optional[str] = None,
         args: Optional[Dict] = None,
-        tracer: Optional[Any] = None,
-        name: Optional[str] = None,
-        is_robusta_model: bool = False,
+        tracer=None,
     ):
         self.model = model
         self.api_key = api_key
-        self.api_base = api_base
-        self.api_version = api_version
         self.args = args or {}
         self.tracer = tracer
-        self.name = name
-        self.is_robusta_model = is_robusta_model
-        self.update_custom_args()
-        self.check_llm(
-            self.model, self.api_key, self.api_base, self.api_version, self.args
-        )
 
-    def update_custom_args(self):
-        self.max_context_size = self.args.get("custom_args", {}).get("max_context_size")
-        self.args.pop("custom_args", None)
+        if not self.args:
+            self.check_llm(self.model, self.api_key)
 
-    def check_llm(
-        self,
-        model: str,
-        api_key: Optional[str],
-        api_base: Optional[str],
-        api_version: Optional[str],
-        args: Optional[dict] = None,
-    ):
-        if self.is_robusta_model:
-            # The model is assumed correctly configured if it is a robusta model
-            # For robusta models, this code would fail because Holmes has no knowledge of the API keys
-            # to azure or bedrock as all completion API calls go through robusta's LLM proxy
-            return
-        args = args or {}
+    def check_llm(self, model: str, api_key: Optional[str]):
         logging.debug(f"Checking LiteLLM model {model}")
-        lookup = litellm.get_llm_provider(model)
+        # TODO: this WAS a hack to get around the fact that we can't pass in an api key to litellm.validate_environment
+        # so without this hack it always complains that the environment variable for the api key is missing
+        # to fix that, we always set an api key in the standard format that litellm expects (which is ${PROVIDER}_API_KEY)
+        # TODO: we can now handle this better - see https://github.com/BerriAI/litellm/issues/4375#issuecomment-2223684750
+        lookup = litellm.get_llm_provider(self.model)
         if not lookup:
             raise Exception(f"Unknown provider for model {model}")
         provider = lookup[1]
@@ -218,151 +119,85 @@ class DefaultLLM(LLM):
                     "environment variable for proper functionality. For more information, refer to the documentation: "
                     "https://docs.litellm.ai/docs/providers/watsonx#usage---models-in-deployment-spaces"
                 )
-        elif provider == "bedrock":
-            if os.environ.get("AWS_PROFILE") or os.environ.get(
-                "AWS_BEARER_TOKEN_BEDROCK"
-            ):
-                model_requirements = {"keys_in_environment": True, "missing_keys": []}
-            elif args.get("aws_access_key_id") and args.get("aws_secret_access_key"):
-                return  # break fast.
-            else:
-                model_requirements = litellm.validate_environment(
-                    model=model, api_key=api_key, api_base=api_base
-                )
+        elif provider == "bedrock" and (
+            os.environ.get("AWS_PROFILE") or os.environ.get("AWS_BEARER_TOKEN_BEDROCK")
+        ):
+            model_requirements = {"keys_in_environment": True, "missing_keys": []}
         else:
-            model_requirements = litellm.validate_environment(
-                model=model, api_key=api_key, api_base=api_base
-            )
-            # validate_environment does not accept api_version, and as a special case for Azure OpenAI Service,
-            # when all the other AZURE environments are set expect AZURE_API_VERSION, validate_environment complains
-            # the missing of it even after the api_version is set.
-            # TODO: There's an open PR in litellm to accept api_version in validate_environment, we can leverage this
-            # change if accepted to ignore the following check.
-            # https://github.com/BerriAI/litellm/pull/13808
-            if (
-                provider == "azure"
-                and ["AZURE_API_VERSION"] == model_requirements["missing_keys"]
-                and api_version is not None
-            ):
-                model_requirements["missing_keys"] = []
-                model_requirements["keys_in_environment"] = True
+            #
+            api_key_env_var = f"{provider.upper()}_API_KEY"
+            if api_key:
+                os.environ[api_key_env_var] = api_key
+            model_requirements = litellm.validate_environment(model=model)
 
         if not model_requirements["keys_in_environment"]:
             raise Exception(
                 f"model {model} requires the following environment variables: {model_requirements['missing_keys']}"
             )
 
-    def _get_model_name_variants_for_lookup(self) -> list[str]:
+    def _strip_model_prefix(self) -> str:
         """
-        Generate model name variants to try when looking up in litellm.model_cost.
-        Returns a list of names to try in order: exact, lowercase, without prefix, etc.
+        Helper function to strip 'openai/' prefix from model name if it exists.
+        model cost is taken from here which does not have the openai prefix
+        https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json
         """
-        names_to_try = [self.model, self.model.lower()]
+        model_name = self.model
+        prefixes = ["openai/", "bedrock/", "vertex_ai/", "anthropic/"]
 
-        # If there's a prefix, also try without it
-        if "/" in self.model:
-            base_model = self.model.split("/", 1)[1]
-            names_to_try.extend([base_model, base_model.lower()])
+        for prefix in prefixes:
+            if model_name.startswith(prefix):
+                return model_name[len(prefix) :]
 
-        # Remove duplicates while preserving order (dict.fromkeys maintains insertion order in Python 3.7+)
-        return list(dict.fromkeys(names_to_try))
+        return model_name
 
-    def get_context_window_size(self) -> int:
-        if self.max_context_size:
-            return self.max_context_size
+        # this unfortunately does not seem to work for azure if the deployment name is not a well-known model name
+        # if not litellm.supports_function_calling(model=model):
+        #    raise Exception(f"model {model} does not support function calling. You must use HolmesGPT with a model that supports function calling.")
 
+    def get_context_window_size(self) -> int:
         if OVERRIDE_MAX_CONTENT_SIZE:
             logging.debug(
                 f"Using override OVERRIDE_MAX_CONTENT_SIZE {OVERRIDE_MAX_CONTENT_SIZE}"
             )
             return OVERRIDE_MAX_CONTENT_SIZE
 
-        # Try each name variant
-        for name in self._get_model_name_variants_for_lookup():
-            try:
-                return litellm.model_cost[name]["max_input_tokens"]
-            except Exception:
-                continue
-
-        # Log which lookups we tried
-        logging.warning(
-            f"Couldn't find model {self.model} in litellm's model list (tried: {', '.join(self._get_model_name_variants_for_lookup())}), "
-            f"using default {FALLBACK_CONTEXT_WINDOW_SIZE} tokens for max_input_tokens. "
-            f"To override, set OVERRIDE_MAX_CONTENT_SIZE environment variable to the correct value for your model."
-        )
-        return FALLBACK_CONTEXT_WINDOW_SIZE
+        model_name = os.environ.get("MODEL_TYPE", self._strip_model_prefix())
+        try:
+            return litellm.model_cost[model_name]["max_input_tokens"]
+        except Exception:
+            logging.warning(
+                f"Couldn't find model's name {model_name} in litellm's model list, fallback to 128k tokens for max_input_tokens"
+            )
+            return 128000
 
     @sentry_sdk.trace
-    def count_tokens(
-        self, messages: list[dict], tools: Optional[list[dict[str, Any]]] = None
-    ) -> TokenCountMetadata:
-        # TODO: Add a recount:bool flag to save time. When the flag is false, reuse 'message["token_count"]' for individual messages.
-        # It's only necessary to recount message tokens at the beginning of a session because the LLM model may have changed.
-        # Changing the model requires recounting tokens because the tokenizer may be different
-        total_tokens = 0
-        tools_tokens = 0
-        system_tokens = 0
-        assistant_tokens = 0
-        user_tokens = 0
-        other_tokens = 0
-        tools_to_call_tokens = 0
+    def count_tokens_for_message(self, messages: list[dict]) -> int:
+        total_token_count = 0
         for message in messages:
-            # count message tokens individually because it gives us fine grain information about each tool call/message etc.
-            # However be aware that the sum of individual message tokens is not equal to the overall messages token
-            token_count = litellm.token_counter(  # type: ignore
-                model=self.model, messages=[message]
-            )
-            message["token_count"] = token_count
-            role = message.get("role")
-            if role == "system":
-                system_tokens += token_count
-            elif role == "user":
-                user_tokens += token_count
-            elif role == "tool":
-                tools_tokens += token_count
-            elif role == "assistant":
-                assistant_tokens += token_count
+            if "token_count" in message and message["token_count"]:
+                total_token_count += message["token_count"]
             else:
-                # although this should not be needed,
-                # it is defensive code so that all tokens are accounted for
-                # and can potentially make debugging easier
-                other_tokens += token_count
-
-        messages_token_count_without_tools = litellm.token_counter(  # type: ignore
-            model=self.model, messages=messages
-        )
-
-        total_tokens = litellm.token_counter(  # type: ignore
-            model=self.model,
-            messages=messages,
-            tools=tools,  # type: ignore
-        )
-        tools_to_call_tokens = max(0, total_tokens - messages_token_count_without_tools)
-
-        return TokenCountMetadata(
-            total_tokens=total_tokens,
-            system_tokens=system_tokens,
-            user_tokens=user_tokens,
-            tools_tokens=tools_tokens,
-            tools_to_call_tokens=tools_to_call_tokens,
-            other_tokens=other_tokens,
-            assistant_tokens=assistant_tokens,
-        )
-
-    def get_litellm_corrected_name_for_robusta_ai(self) -> str:
-        if self.is_robusta_model:
-            # For robusta models, self.model is the underlying provider/model used by Robusta AI
-            # To avoid litellm modifying the API URL according to the provider, the provider name
-            # is replaced with 'openai/' just before doing a completion() call
-            # Cf. https://docs.litellm.ai/docs/providers/openai_compatible
-            split_model_name = self.model.split("/")
-            return (
-                split_model_name[0]
-                if len(split_model_name) == 1
-                else f"openai/{split_model_name[1]}"
-            )
-        else:
-            return self.model
+                # message can be counted by this method only if message contains a "content" key
+                if "content" in message:
+                    if isinstance(message["content"], str):
+                        message_to_count = [
+                            {"type": "text", "text": message["content"]}
+                        ]
+                    elif isinstance(message["content"], list):
+                        message_to_count = [
+                            {"type": "text", "text": json.dumps(message["content"])}
+                        ]
+                    elif isinstance(message["content"], dict):
+                        if "type" not in message["content"]:
+                            message_to_count = [
+                                {"type": "text", "text": json.dumps(message["content"])}
+                            ]
+                    token_count = litellm.token_counter(
+                        model=self.model, messages=message_to_count
+                    )
+                    message["token_count"] = token_count
+                    total_token_count += token_count
+        return total_token_count
 
     def completion(
         self,
@@ -377,16 +212,13 @@ class DefaultLLM(LLM):
         tools_args = {}
         allowed_openai_params = None
 
-        if tools and len(tools) > 0 and tool_choice == "auto":
+        if tools and len(tools) > 0 and tool_choice:
             tools_args["tools"] = tools
             tools_args["tool_choice"] = tool_choice  # type: ignore
 
         if THINKING:
             self.args.setdefault("thinking", json.loads(THINKING))
 
-        if EXTRA_HEADERS:
-            self.args.setdefault("extra_headers", json.loads(EXTRA_HEADERS))
-
         if self.args.get("thinking", None):
             litellm.modify_params = True
 
@@ -402,13 +234,9 @@ class DefaultLLM(LLM):
 
         # Get the litellm module to use (wrapped or unwrapped)
         litellm_to_use = self.tracer.wrap_llm(litellm) if self.tracer else litellm
-
-        litellm_model_name = self.get_litellm_corrected_name_for_robusta_ai()
         result = litellm_to_use.completion(
-            model=litellm_model_name,
+            model=self.model,
             api_key=self.api_key,
-            base_url=self.api_base,
-            api_version=self.api_version,
             messages=messages,
             response_format=response_format,
             drop_params=drop_params,
@@ -426,33 +254,20 @@ class DefaultLLM(LLM):
             raise Exception(f"Unexpected type returned by the LLM {type(result)}")
 
     def get_maximum_output_token(self) -> int:
-        max_output_tokens = floor(min(64000, self.get_context_window_size() / 5))
-
         if OVERRIDE_MAX_OUTPUT_TOKEN:
             logging.debug(
                 f"Using OVERRIDE_MAX_OUTPUT_TOKEN {OVERRIDE_MAX_OUTPUT_TOKEN}"
             )
             return OVERRIDE_MAX_OUTPUT_TOKEN
 
-        # Try each name variant
-        for name in self._get_model_name_variants_for_lookup():
-            try:
-                litellm_max_output_tokens = litellm.model_cost[name][
-                    "max_output_tokens"
-                ]
-                if litellm_max_output_tokens < max_output_tokens:
-                    max_output_tokens = litellm_max_output_tokens
-                return max_output_tokens
-            except Exception:
-                continue
-
-        # Log which lookups we tried
-        logging.warning(
-            f"Couldn't find model {self.model} in litellm's model list (tried: {', '.join(self._get_model_name_variants_for_lookup())}), "
-            f"using {max_output_tokens} tokens for max_output_tokens. "
-            f"To override, set OVERRIDE_MAX_OUTPUT_TOKEN environment variable to the correct value for your model."
-        )
-        return max_output_tokens
+        model_name = os.environ.get("MODEL_TYPE", self._strip_model_prefix())
+        try:
+            return litellm.model_cost[model_name]["max_output_tokens"]
+        except Exception:
+            logging.warning(
+                f"Couldn't find model's name {model_name} in litellm's model list, fallback to 4096 tokens for max_output_tokens"
+            )
+            return 4096
 
     def _add_cache_control_to_last_message(
         self, messages: List[Dict[str, Any]]
@@ -461,12 +276,6 @@ class DefaultLLM(LLM):
         Add cache_control to the last non-user message for Anthropic prompt caching.
         Removes any existing cache_control from previous messages to avoid accumulation.
         """
-        # Skip cache_control for VertexAI/Gemini models as they don't support it with tools
-        if self.model and (
-            "vertex" in self.model.lower() or "gemini" in self.model.lower()
-        ):
-            return
-
         # First, remove any existing cache_control from all messages
         for msg in messages:
             content = msg.get("content")
@@ -496,7 +305,7 @@ class DefaultLLM(LLM):
         if content is None:
             return
 
-        if isinstance(content, str) and content:
+        if isinstance(content, str):
             # Convert string to structured format with cache_control
             target_msg["content"] = [
                 {
@@ -516,213 +325,3 @@ class DefaultLLM(LLM):
                 logging.debug(
                     f"Added cache_control to {target_msg.get('role')} message (structured content)"
                 )
-
-
-class LLMModelRegistry:
-    def __init__(self, config: "Config", dal: SupabaseDal) -> None:
-        self.config = config
-        self._llms: dict[str, ModelEntry] = {}
-        self._default_robusta_model = None
-        self.dal = dal
-
-        self._init_models()
-
-    @property
-    def default_robusta_model(self) -> Optional[str]:
-        return self._default_robusta_model
-
-    def _init_models(self):
-        self._llms = self._parse_models_file(MODEL_LIST_FILE_LOCATION)
-
-        if self._should_load_robusta_ai():
-            self.configure_robusta_ai_model()
-
-        if self._should_load_config_model():
-            self._llms[self.config.model] = self._create_model_entry(
-                model=self.config.model,
-                model_name=self.config.model,
-                base_url=self.config.api_base,
-                is_robusta_model=False,
-                api_key=self.config.api_key,
-                api_version=self.config.api_version,
-            )
-
-    def _should_load_config_model(self) -> bool:
-        if self.config.model is not None:
-            return True
-
-        # backward compatibility - in the past config.model was set by default to gpt-4o.
-        # so we need to check if the user has set an OPENAI_API_KEY to load the config model.
-        has_openai_key = os.environ.get("OPENAI_API_KEY")
-        if has_openai_key:
-            self.config.model = "gpt-4.1"
-            return True
-
-        return False
-
-    def configure_robusta_ai_model(self) -> None:
-        try:
-            if not self.config.cluster_name or not LOAD_ALL_ROBUSTA_MODELS:
-                self._load_default_robusta_config()
-                return
-
-            if not self.dal.account_id or not self.dal.enabled:
-                self._load_default_robusta_config()
-                return
-
-            account_id, token = self.dal.get_ai_credentials()
-
-            robusta_models: RobustaModelsResponse | None = fetch_robusta_models(
-                account_id, token
-            )
-            if not robusta_models or not robusta_models.models:
-                self._load_default_robusta_config()
-                return
-
-            default_model = None
-            for model_name, model_data in robusta_models.models.items():
-                logging.info(f"Loading Robusta AI model: {model_name}")
-                self._llms[model_name] = self._create_robusta_model_entry(
-                    model_name=model_name, model_data=model_data
-                )
-                if model_data.is_default:
-                    default_model = model_name
-
-            if default_model:
-                logging.info(f"Setting default Robusta AI model to: {default_model}")
-                self._default_robusta_model: str = default_model  # type: ignore
-
-        except Exception:
-            logging.exception("Failed to get all robusta models")
-            # fallback to default behavior
-            self._load_default_robusta_config()
-
-    def _load_default_robusta_config(self):
-        if self._should_load_robusta_ai():
-            logging.info("Loading default Robusta AI model")
-            self._llms[ROBUSTA_AI_MODEL_NAME] = ModelEntry(
-                name=ROBUSTA_AI_MODEL_NAME,
-                model="gpt-4o",  # TODO: tech debt, this isn't really
-                base_url=ROBUSTA_API_ENDPOINT,
-                is_robusta_model=True,
-            )
-            self._default_robusta_model = ROBUSTA_AI_MODEL_NAME
-
-    def _should_load_robusta_ai(self) -> bool:
-        if not self.config.should_try_robusta_ai:
-            return False
-
-        # ROBUSTA_AI were set in the env vars, so we can use it directly
-        if ROBUSTA_AI is not None:
-            return ROBUSTA_AI
-
-        # MODEL is set in the env vars, e.g. the user is using a custom model
-        # so we don't need to load the robusta AI model and keep the behavior backward compatible
-        if "MODEL" in os.environ:
-            return False
-
-        # if the user has provided a model list, we don't need to load the robusta AI model
-        if self._llms:
-            return False
-
-        return True
-
-    def get_model_params(self, model_key: Optional[str] = None) -> ModelEntry:
-        if not self._llms:
-            raise Exception("No llm models were loaded")
-
-        if model_key:
-            model_params = self._llms.get(model_key)
-            if model_params is not None:
-                logging.info(f"Using selected model: {model_key}")
-                return model_params.copy()
-
-            logging.error(f"Couldn't find model: {model_key} in model list")
-
-        if self._default_robusta_model:
-            model_params = self._llms.get(self._default_robusta_model)
-            if model_params is not None:
-                logging.info(
-                    f"Using default Robusta AI model: {self._default_robusta_model}"
-                )
-                return model_params.copy()
-
-            logging.error(
-                f"Couldn't find default Robusta AI model: {self._default_robusta_model} in model list"
-            )
-
-        model_key, first_model_params = next(iter(self._llms.items()))
-        logging.debug(f"Using first available model: {model_key}")
-        return first_model_params.copy()
-
-    def get_llm(self, name: str) -> LLM:  # TODO: fix logic
-        return self._llms[name]  # type: ignore
-
-    @property
-    def models(self) -> dict[str, ModelEntry]:
-        return self._llms
-
-    def _parse_models_file(self, path: str) -> dict[str, ModelEntry]:
-        models = load_yaml_file(path, raise_error=False, warn_not_found=False)
-        for _, params in models.items():
-            params = replace_env_vars_values(params)
-
-        llms = {}
-        for model_name, params in models.items():
-            llms[model_name] = ModelEntry.model_validate(params)
-
-        return llms
-
-    def _create_robusta_model_entry(
-        self, model_name: str, model_data: RobustaModel
-    ) -> ModelEntry:
-        entry = self._create_model_entry(
-            model=model_data.model,
-            model_name=model_name,
-            base_url=f"{ROBUSTA_API_ENDPOINT}/llm/{model_name}",
-            is_robusta_model=True,
-        )
-        entry.custom_args = model_data.holmes_args or {}  # type: ignore[assignment]
-        return entry
-
-    def _create_model_entry(
-        self,
-        model: str,
-        model_name: str,
-        base_url: Optional[str] = None,
-        is_robusta_model: Optional[bool] = None,
-        api_key: Optional[SecretStr] = None,
-        api_base: Optional[str] = None,
-        api_version: Optional[str] = None,
-    ) -> ModelEntry:
-        return ModelEntry(
-            name=model_name,
-            model=model,
-            base_url=base_url,
-            is_robusta_model=is_robusta_model,
-            api_key=api_key,
-            api_base=api_base,
-            api_version=api_version,
-        )
-
-
-def get_llm_usage(
-    llm_response: Union[ModelResponse, CustomStreamWrapper, TextCompletionResponse],
-) -> dict:
-    usage: dict = {}
-    if (
-        (
-            isinstance(llm_response, ModelResponse)
-            or isinstance(llm_response, TextCompletionResponse)
-        )
-        and hasattr(llm_response, "usage")
-        and llm_response.usage
-    ):  # type: ignore
-        usage["prompt_tokens"] = llm_response.usage.prompt_tokens  # type: ignore
-        usage["completion_tokens"] = llm_response.usage.completion_tokens  # type: ignore
-        usage["total_tokens"] = llm_response.usage.total_tokens  # type: ignore
-    elif isinstance(llm_response, CustomStreamWrapper):
-        complete_response = litellm.stream_chunk_builder(chunks=llm_response)  # type: ignore
-        if complete_response:
-            return get_llm_usage(complete_response)
-    return usage
diff --git a/holmes/core/prompt.py b/holmes/core/prompt.py
index f28fdea6..88e87b9f 100644
--- a/holmes/core/prompt.py
+++ b/holmes/core/prompt.py
@@ -55,10 +55,29 @@ def build_initial_ask_messages(
     """
     # Load and render system prompt internally
     system_prompt_template = "builtin://generic_ask.jinja2"
+    
+    # Add KAITO-specific anti-JSON and conciseness system prompt addition
+    anti_json_prompt = """
+
+KAITO ANTI-JSON ENFORCEMENT:
+- You are strictly PROHIBITED from outputting raw JSON as your final response
+- Tool calls are internal operations - the user should never see JSON tool call syntax
+- After tools execute, you MUST provide natural language answers
+- Example: If asked "how many pods?" respond "There are X pods" NOT {"name": "kubectl_get_by_kind_in_namespace", "arguments": {...}}
+- Your final response must be conversational and helpful, never JSON
+
+KAITO CONCISENESS ENFORCEMENT:
+- Keep diagnostic answers direct and concise
+- Lead with the core issue, then brief details if needed
+- For "what's wrong?" questions, state the problem immediately: "The pod was killed due to out of memory"
+- Avoid verbose explanations unless specifically requested  
+- Be actionable and specific rather than lengthy
+"""
+    
     template_context = {
         "toolsets": tool_executor.toolsets,
-        "runbooks_enabled": True if runbooks else False,
-        "system_prompt_additions": system_prompt_additions or "",
+        "runbooks": runbooks or {},
+        "system_prompt_additions": (system_prompt_additions or "") + anti_json_prompt,
     }
     system_prompt_rendered = load_and_render_prompt(
         system_prompt_template, template_context
@@ -69,7 +88,8 @@ def build_initial_ask_messages(
         console, initial_user_prompt, file_paths
     )
 
-    user_prompt_with_files += get_tasks_management_system_reminder()
+    # KAITO PATCH: Comment out TodoWrite system reminder to restore Wednesday behavior
+    # user_prompt_with_files += get_tasks_management_system_reminder()
     messages = [
         {"role": "system", "content": system_prompt_rendered},
         {"role": "user", "content": user_prompt_with_files},
diff --git a/holmes/core/tool_calling_llm.py b/holmes/core/tool_calling_llm.py
index 3c87f65c..3e9b95b7 100644
--- a/holmes/core/tool_calling_llm.py
+++ b/holmes/core/tool_calling_llm.py
@@ -1,14 +1,9 @@
 import concurrent.futures
 import json
 import logging
+import os
 import textwrap
-from typing import Dict, List, Optional, Type, Union, Callable, Any
-
-from holmes.core.models import (
-    ToolApprovalDecision,
-    ToolCallResult,
-    PendingToolApproval,
-)
+from typing import Dict, List, Optional, Type, Union
 
 import sentry_sdk
 from openai import BadRequestError
@@ -19,8 +14,8 @@ from pydantic import BaseModel, Field
 from rich.console import Console
 
 from holmes.common.env_vars import (
-    RESET_REPEATED_TOOL_CALL_CHECK_AFTER_COMPACTION,
     TEMPERATURE,
+    MAX_OUTPUT_TOKEN_RESERVATION,
     LOG_LLM_USAGE_RESPONSE,
 )
 
@@ -33,37 +28,21 @@ from holmes.core.investigation_structured_output import (
 )
 from holmes.core.issue import Issue
 from holmes.core.llm import LLM
+from holmes.core.performance_timing import PerformanceTiming
 from holmes.core.resource_instruction import ResourceInstructions
 from holmes.core.runbooks import RunbookManager
 from holmes.core.safeguards import prevent_overly_repeated_tool_call
-from holmes.core.tools import (
-    StructuredToolResult,
-    StructuredToolResultStatus,
-    ToolInvokeContext,
-)
-from holmes.core.tools_utils.tool_context_window_limiter import (
-    prevent_overly_big_tool_response,
-)
-from holmes.core.truncation.input_context_window_limiter import (
-    limit_input_context_window,
-)
+from holmes.core.tools import StructuredToolResult, ToolResultStatus
 from holmes.plugins.prompts import load_and_render_prompt
-from holmes.plugins.runbooks import RunbookCatalog
-from holmes.utils import sentry_helper
 from holmes.utils.global_instructions import (
     Instructions,
-    add_runbooks_to_user_prompt,
+    add_global_instructions_to_user_prompt,
 )
 from holmes.utils.tags import format_tags_in_string, parse_messages_tags
 from holmes.core.tools_utils.tool_executor import ToolExecutor
 from holmes.core.tracing import DummySpan
 from holmes.utils.colors import AI_COLOR
-from holmes.utils.stream import (
-    StreamEvents,
-    StreamMessage,
-    add_token_count_to_metadata,
-    build_stream_event_token_count,
-)
+from holmes.utils.stream import StreamEvents, StreamMessage
 
 # Create a named logger for cost tracking
 cost_logger = logging.getLogger("holmes.costs")
@@ -140,6 +119,148 @@ def _process_cost_info(
         logging.debug(f"Could not extract cost information: {e}")
 
 
+def format_tool_result_data(tool_result: StructuredToolResult) -> str:
+    tool_response = tool_result.data
+    if isinstance(tool_result.data, str):
+        tool_response = tool_result.data
+    else:
+        try:
+            if isinstance(tool_result.data, BaseModel):
+                tool_response = tool_result.data.model_dump_json(indent=2)
+            else:
+                tool_response = json.dumps(tool_result.data, indent=2)
+        except Exception:
+            tool_response = str(tool_result.data)
+    if tool_result.status == ToolResultStatus.ERROR:
+        tool_response = f"{tool_result.error or 'Tool execution failed'}:\n\n{tool_result.data or ''}".strip()
+    return tool_response
+
+
+# TODO: I think there's a bug here because we don't account for the 'role' or json structure like '{...}' when counting tokens
+# However, in practice it works because we reserve enough space for the output tokens that the minor inconsistency does not matter
+# We should fix this in the future
+# TODO: we truncate using character counts not token counts - this means we're overly agressive with truncation - improve it by considering
+# token truncation and not character truncation
+def truncate_messages_to_fit_context(
+    messages: list, max_context_size: int, maximum_output_token: int, count_tokens_fn
+) -> list:
+    """
+    Helper function to truncate tool messages to fit within context limits.
+
+    Args:
+        messages: List of message dictionaries with roles and content
+        max_context_size: Maximum context window size for the model
+        maximum_output_token: Maximum tokens reserved for model output
+        count_tokens_fn: Function to count tokens for a list of messages
+
+    Returns:
+        Modified list of messages with truncated tool responses
+
+    Raises:
+        Exception: If non-tool messages exceed available context space
+    """
+    messages_except_tools = [
+        message for message in messages if message["role"] != "tool"
+    ]
+    message_size_without_tools = count_tokens_fn(messages_except_tools)
+
+    tool_call_messages = [message for message in messages if message["role"] == "tool"]
+
+    reserved_for_output_tokens = min(maximum_output_token, MAX_OUTPUT_TOKEN_RESERVATION)
+    if message_size_without_tools >= (max_context_size - reserved_for_output_tokens):
+        logging.error(
+            f"The combined size of system_prompt and user_prompt ({message_size_without_tools} tokens) exceeds the model's context window for input."
+        )
+        raise Exception(
+            f"The combined size of system_prompt and user_prompt ({message_size_without_tools} tokens) exceeds the maximum context size of {max_context_size - reserved_for_output_tokens} tokens available for input."
+        )
+
+    if len(tool_call_messages) == 0:
+        return messages
+
+    available_space = (
+        max_context_size - message_size_without_tools - maximum_output_token
+    )
+    remaining_space = available_space
+    tool_call_messages.sort(key=lambda x: len(x["content"]))
+
+    # Allocate space starting with small tools and going to larger tools, while maintaining fairness
+    # Small tools can often get exactly what they need, while larger tools may need to be truncated
+    # We ensure fairness (no tool gets more than others that need it) and also maximize utilization (we don't leave space unused)
+    for i, msg in enumerate(tool_call_messages):
+        remaining_tools = len(tool_call_messages) - i
+        max_allocation = remaining_space // remaining_tools
+        needed_space = len(msg["content"])
+        allocated_space = min(needed_space, max_allocation)
+
+        if needed_space > allocated_space:
+            truncation_notice = "\n\n[TRUNCATED]"
+            # Ensure the indicator fits in the allocated space
+            if allocated_space > len(truncation_notice):
+                msg["content"] = (
+                    msg["content"][: allocated_space - len(truncation_notice)]
+                    + truncation_notice
+                )
+                logging.info(
+                    f"Truncating tool message '{msg['name']}' from {needed_space} to {allocated_space-len(truncation_notice)} tokens"
+                )
+            else:
+                msg["content"] = truncation_notice[:allocated_space]
+                logging.info(
+                    f"Truncating tool message '{msg['name']}' from {needed_space} to {allocated_space} tokens"
+                )
+            msg.pop("token_count", None)  # Remove token_count if present
+
+        remaining_space -= allocated_space
+    return messages
+
+
+class ToolCallResult(BaseModel):
+    tool_call_id: str
+    tool_name: str
+    description: str
+    result: StructuredToolResult
+    size: Optional[int] = None
+
+    def as_tool_call_message(self):
+        content = format_tool_result_data(self.result)
+        if self.result.params:
+            content = (
+                f"Params used for the tool call: {json.dumps(self.result.params)}. The tool call output follows on the next line.\n"
+                + content
+            )
+        return {
+            "tool_call_id": self.tool_call_id,
+            "role": "tool",
+            "name": self.tool_name,
+            "content": content,
+        }
+
+    def as_tool_result_response(self):
+        result_dump = self.result.model_dump()
+        result_dump["data"] = self.result.get_stringified_data()
+
+        return {
+            "tool_call_id": self.tool_call_id,
+            "tool_name": self.tool_name,
+            "description": self.description,
+            "role": "tool",
+            "result": result_dump,
+        }
+
+    def as_streaming_tool_result_response(self):
+        result_dump = self.result.model_dump()
+        result_dump["data"] = self.result.get_stringified_data()
+
+        return {
+            "tool_call_id": self.tool_call_id,
+            "role": "tool",
+            "description": self.description,
+            "name": self.tool_name,
+            "result": result_dump,
+        }
+
+
 class LLMResult(LLMCosts):
     tool_calls: Optional[List[ToolCallResult]] = None
     result: Optional[str] = None
@@ -148,7 +269,6 @@ class LLMResult(LLMCosts):
     # TODO: clean up these two
     prompt: Optional[str] = None
     messages: Optional[List[dict]] = None
-    metadata: Optional[Dict[Any, Any]] = None
 
     def get_tool_usage_summary(self):
         return "AI used info from issue and " + ",".join(
@@ -156,12 +276,6 @@ class LLMResult(LLMCosts):
         )
 
 
-class ToolCallWithDecision(BaseModel):
-    message_index: int
-    tool_call: ChatCompletionMessageToolCall
-    decision: Optional[ToolApprovalDecision]
-
-
 class ToolCallingLLM:
     llm: LLM
 
@@ -172,98 +286,6 @@ class ToolCallingLLM:
         self.max_steps = max_steps
         self.tracer = tracer
         self.llm = llm
-        self.approval_callback: Optional[
-            Callable[[StructuredToolResult], tuple[bool, Optional[str]]]
-        ] = None
-
-    def process_tool_decisions(
-        self, messages: List[Dict[str, Any]], tool_decisions: List[ToolApprovalDecision]
-    ) -> tuple[List[Dict[str, Any]], list[StreamMessage]]:
-        """
-        Process tool approval decisions and execute approved tools.
-
-        Args:
-            messages: Current conversation messages
-            tool_decisions: List of ToolApprovalDecision objects
-
-        Returns:
-            Updated messages list with tool execution results
-        """
-        events: list[StreamMessage] = []
-        if not tool_decisions:
-            return messages, events
-
-        # Create decision lookup
-        decisions_by_tool_call_id = {
-            decision.tool_call_id: decision for decision in tool_decisions
-        }
-
-        pending_tool_calls: list[ToolCallWithDecision] = []
-
-        for i in reversed(range(len(messages))):
-            msg = messages[i]
-            if msg.get("role") == "assistant" and msg.get("tool_calls"):
-                message_tool_calls = msg.get("tool_calls", [])
-                for tool_call in message_tool_calls:
-                    decision = decisions_by_tool_call_id.get(tool_call.get("id"), None)
-                    if tool_call.get("pending_approval"):
-                        del tool_call[
-                            "pending_approval"
-                        ]  # Cleanup so that a pending approval is not tagged on message in a future response
-                        pending_tool_calls.append(
-                            ToolCallWithDecision(
-                                tool_call=ChatCompletionMessageToolCall(**tool_call),
-                                decision=decision,
-                                message_index=i,
-                            )
-                        )
-
-        if not pending_tool_calls:
-            error_message = f"Received {len(tool_decisions)} tool decisions but no pending approvals found"
-            logging.error(error_message)
-            raise Exception(error_message)
-        for tool_call_with_decision in pending_tool_calls:
-            tool_call_message: dict
-            tool_call = tool_call_with_decision.tool_call
-            decision = tool_call_with_decision.decision
-            tool_result: Optional[ToolCallResult] = None
-            if decision and decision.approved:
-                tool_result = self._invoke_llm_tool_call(
-                    tool_to_call=tool_call,
-                    previous_tool_calls=[],
-                    trace_span=DummySpan(),  # TODO: replace with proper span
-                    tool_number=None,
-                    user_approved=True,
-                )
-            else:
-                # Tool was rejected or no decision found, add rejection message
-                tool_result = ToolCallResult(
-                    tool_call_id=tool_call.id,
-                    tool_name=tool_call.function.name,
-                    description=tool_call.function.name,
-                    result=StructuredToolResult(
-                        status=StructuredToolResultStatus.ERROR,
-                        error="Tool execution was denied by the user.",
-                    ),
-                )
-
-            events.append(
-                StreamMessage(
-                    event=StreamEvents.TOOL_RESULT,
-                    data=tool_result.as_streaming_tool_result_response(),
-                )
-            )
-
-            tool_call_message = tool_result.as_tool_call_message()
-
-            # It is expected that the tool call result directly follows the tool call request from the LLM
-            # The API call may contain a user ask which is appended to the messages so we can't just append
-            # tool call results; they need to be inserted right after the llm's message requesting tool calls
-            messages.insert(
-                tool_call_with_decision.message_index + 1, tool_call_message
-            )
-
-        return messages, events
 
     def prompt_call(
         self,
@@ -309,35 +331,37 @@ class ToolCallingLLM:
         trace_span=DummySpan(),
         tool_number_offset: int = 0,
     ) -> LLMResult:
-        tool_calls: list[
-            dict
-        ] = []  # Used for preventing repeated tool calls. potentially reset after compaction
-        all_tool_calls = []  # type: ignore
+        perf_timing = PerformanceTiming("tool_calling_llm.call")
+        tool_calls = []  # type: ignore
         costs = LLMCosts()
+
         tools = self.tool_executor.get_all_tools_openai_format(
             target_model=self.llm.model
         )
+        perf_timing.measure("get_all_tools_openai_format")
         max_steps = self.max_steps
         i = 0
-        metadata: Dict[Any, Any] = {}
+
         while i < max_steps:
             i += 1
+            perf_timing.measure(f"start iteration {i}")
             logging.debug(f"running iteration {i}")
             # on the last step we don't allow tools - we want to force a reply, not a request to run another tool
             tools = None if i == max_steps else tools
-            tool_choice = "auto" if tools else None
-
-            limit_result = limit_input_context_window(
-                llm=self.llm, messages=messages, tools=tools
-            )
-            messages = limit_result.messages
-            metadata = metadata | limit_result.metadata
-
-            if (
-                limit_result.conversation_history_compacted
-                and RESET_REPEATED_TOOL_CALL_CHECK_AFTER_COMPACTION
-            ):
-                tool_calls = []
+            tool_choice = os.environ.get("HOLMES_TOOL_CHOICE", "auto") if tools else None
+            logging.debug(f"HOLMES_TOOL_CHOICE env var: {os.environ.get('HOLMES_TOOL_CHOICE', 'NOT_SET')}, using tool_choice: {tool_choice}")
+
+            total_tokens = self.llm.count_tokens_for_message(messages)
+            max_context_size = self.llm.get_context_window_size()
+            maximum_output_token = self.llm.get_maximum_output_token()
+            perf_timing.measure("count tokens")
+
+            if (total_tokens + maximum_output_token) > max_context_size:
+                logging.warning("Token limit exceeded. Truncating tool responses.")
+                messages = self.truncate_messages_to_fit_context(
+                    messages, max_context_size, maximum_output_token
+                )
+                perf_timing.measure("truncate_messages_to_fit_context")
 
             logging.debug(f"sending messages={messages}\n\ntools={tools}")
 
@@ -355,6 +379,7 @@ class ToolCallingLLM:
                 # Extract and accumulate cost information
                 _process_cost_info(full_response, costs, "LLM call")
 
+                perf_timing.measure("llm.completion")
             # catch a known error that occurs with Azure and replace the error message with something more obvious to the user
             except BadRequestError as e:
                 if "Unrecognized request arguments supplied: tool_choice, tools" in str(
@@ -378,10 +403,9 @@ class ToolCallingLLM:
 
                 if incorrect_tool_call:
                     logging.warning(
-                        "Detected incorrect tool call. Structured output will be disabled. This can happen on models that do not support tool calling. For Azure AI, make sure the model name contains 'gpt-4.1' or other structured output compatible models. To disable this holmes behaviour, set REQUEST_STRUCTURED_OUTPUT_FROM_LLM to `false`."
+                        "Detected incorrect tool call. Structured output will be disabled. This can happen on models that do not support tool calling. For Azure AI, make sure the model name contains 'gpt-4o'. To disable this holmes behaviour, set REQUEST_STRUCTURED_OUTPUT_FROM_LLM to `false`."
                     )
                     # disable structured output going forward and and retry
-                    sentry_helper.capture_structured_output_incorrect_tool_call()
                     response_format = None
                     max_steps = max_steps + 1
                     continue
@@ -398,8 +422,8 @@ class ToolCallingLLM:
                 hasattr(response_message, "reasoning_content")
                 and response_message.reasoning_content
             ):
-                logging.info(
-                    f"[italic dim]AI reasoning:\n\n{response_message.reasoning_content}[/italic dim]\n"
+                logging.debug(
+                    f"[bold {AI_COLOR}]AI (reasoning) ðŸ¤”:[/bold {AI_COLOR}] {response_message.reasoning_content}\n"
                 )
 
             if not tools_to_call:
@@ -417,33 +441,23 @@ class ToolCallingLLM:
                     )
                     costs.total_cost += post_processing_cost
 
-                    tokens = self.llm.count_tokens(messages=messages, tools=tools)
-
-                    add_token_count_to_metadata(
-                        tokens=tokens,
-                        full_llm_response=full_response,
-                        max_context_size=limit_result.max_context_size,
-                        maximum_output_token=limit_result.maximum_output_token,
-                        metadata=metadata,
-                    )
-
+                    perf_timing.end(f"- completed in {i} iterations -")
                     return LLMResult(
                         result=post_processed_response,
                         unprocessed_result=raw_response,
-                        tool_calls=all_tool_calls,
+                        tool_calls=tool_calls,
                         prompt=json.dumps(messages, indent=2),
                         messages=messages,
                         **costs.model_dump(),  # Include all cost fields
-                        metadata=metadata,
                     )
 
+                perf_timing.end(f"- completed in {i} iterations -")
                 return LLMResult(
                     result=text_response,
-                    tool_calls=all_tool_calls,
+                    tool_calls=tool_calls,
                     prompt=json.dumps(messages, indent=2),
                     messages=messages,
                     **costs.model_dump(),  # Include all cost fields
-                    metadata=metadata,
                 )
 
             if text_response and text_response.strip():
@@ -451,55 +465,28 @@ class ToolCallingLLM:
             logging.info(
                 f"The AI requested [bold]{len(tools_to_call) if tools_to_call else 0}[/bold] tool call(s)."
             )
+            perf_timing.measure("pre-tool-calls")
             with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
                 futures = []
-                futures_tool_numbers: dict[
-                    concurrent.futures.Future, Optional[int]
-                ] = {}
-                tool_number: Optional[int]
                 for tool_index, t in enumerate(tools_to_call, 1):
                     logging.debug(f"Tool to call: {t}")
-                    tool_number = tool_number_offset + tool_index
-
-                    future = executor.submit(
-                        self._invoke_llm_tool_call,
-                        tool_to_call=t,
-                        previous_tool_calls=tool_calls,
-                        trace_span=trace_span,
-                        tool_number=tool_number,
+                    futures.append(
+                        executor.submit(
+                            self._invoke_tool,
+                            tool_to_call=t,
+                            previous_tool_calls=tool_calls,
+                            trace_span=trace_span,
+                            tool_number=tool_number_offset + tool_index,
+                        )
                     )
-                    futures_tool_numbers[future] = tool_number
-                    futures.append(future)
 
                 for future in concurrent.futures.as_completed(futures):
                     tool_call_result: ToolCallResult = future.result()
 
-                    tool_number = (
-                        futures_tool_numbers[future]
-                        if future in futures_tool_numbers
-                        else None
-                    )
-
-                    if (
-                        tool_call_result.result.status
-                        == StructuredToolResultStatus.APPROVAL_REQUIRED
-                    ):
-                        with trace_span.start_span(type="tool") as tool_span:
-                            tool_call_result = self._handle_tool_call_approval(
-                                tool_call_result=tool_call_result,
-                                tool_number=tool_number,
-                            )
-                            ToolCallingLLM._log_tool_call_result(
-                                tool_span, tool_call_result
-                            )
-
-                    tool_result_response_dict = (
-                        tool_call_result.as_tool_result_response()
-                    )
-                    tool_calls.append(tool_result_response_dict)
-                    all_tool_calls.append(tool_result_response_dict)
+                    tool_calls.append(tool_call_result.as_tool_result_response())
                     messages.append(tool_call_result.as_tool_call_message())
-                    tokens = self.llm.count_tokens(messages=messages, tools=tools)
+
+                    perf_timing.measure(f"tool completed {tool_call_result.tool_name}")
 
                 # Update the tool number offset for the next iteration
                 tool_number_offset += len(tools_to_call)
@@ -510,205 +497,121 @@ class ToolCallingLLM:
 
         raise Exception(f"Too many LLM calls - exceeded max_steps: {i}/{max_steps}")
 
-    def _directly_invoke_tool_call(
-        self,
-        tool_name: str,
-        tool_params: dict,
-        user_approved: bool,
-        tool_number: Optional[int] = None,
-    ) -> StructuredToolResult:
-        tool = self.tool_executor.get_tool_by_name(tool_name)
-        if not tool:
-            logging.warning(
-                f"Skipping tool execution for {tool_name}: args: {tool_params}"
-            )
-            return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=f"Failed to find tool {tool_name}",
-                params=tool_params,
-            )
-
-        try:
-            invoke_context = ToolInvokeContext(
-                tool_number=tool_number,
-                user_approved=user_approved,
-                llm=self.llm,
-                max_token_count=self.llm.get_max_token_count_for_single_tool(),
-            )
-            tool_response = tool.invoke(tool_params, context=invoke_context)
-        except Exception as e:
-            logging.error(
-                f"Tool call to {tool_name} failed with an Exception", exc_info=True
-            )
-            tool_response = StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
-                error=f"Tool call failed: {e}",
-                params=tool_params,
-            )
-        return tool_response
-
-    def _get_tool_call_result(
+    def _invoke_tool(
         self,
-        tool_call_id: str,
-        tool_name: str,
-        tool_arguments: str,
-        user_approved: bool,
+        tool_to_call: ChatCompletionMessageToolCall,
         previous_tool_calls: list[dict],
-        tool_number: Optional[int] = None,
+        trace_span=DummySpan(),
+        tool_number=None,
     ) -> ToolCallResult:
-        tool_params = {}
+        # Handle the union type - ChatCompletionMessageToolCall can be either
+        # ChatCompletionMessageFunctionToolCall (with 'function' field and type='function')
+        # or ChatCompletionMessageCustomToolCall (with 'custom' field and type='custom').
+        # We use hasattr to check for the 'function' attribute as it's more flexible
+        # and doesn't require importing the specific type.
+        if hasattr(tool_to_call, "function"):
+            tool_name = tool_to_call.function.name
+            tool_arguments = tool_to_call.function.arguments
+        else:
+            # This is a custom tool call - we don't support these currently
+            logging.error(f"Unsupported custom tool call: {tool_to_call}")
+            return ToolCallResult(
+                tool_call_id=tool_to_call.id,
+                tool_name="unknown",
+                description="NA",
+                result=StructuredToolResult(
+                    status=ToolResultStatus.ERROR,
+                    error="Custom tool calls are not supported",
+                    params=None,
+                ),
+            )
+
+        tool_params = None
         try:
             tool_params = json.loads(tool_arguments)
         except Exception:
             logging.warning(
                 f"Failed to parse arguments for tool: {tool_name}. args: {tool_arguments}"
             )
+        tool_call_id = tool_to_call.id
+        tool = self.tool_executor.get_tool_by_name(tool_name)
+
+        if (not tool) or (tool_params is None):
+            logging.warning(
+                f"Skipping tool execution for {tool_name}: args: {tool_arguments}"
+            )
+            return ToolCallResult(
+                tool_call_id=tool_call_id,
+                tool_name=tool_name,
+                description="NA",
+                result=StructuredToolResult(
+                    status=ToolResultStatus.ERROR,
+                    error=f"Failed to find tool {tool_name}",
+                    params=tool_params,
+                ),
+            )
 
         tool_response = None
-        if not user_approved:
+
+        # Create tool span if tracing is enabled
+        tool_span = trace_span.start_span(name=tool_name, type="tool")
+
+        try:
             tool_response = prevent_overly_repeated_tool_call(
-                tool_name=tool_name,
+                tool_name=tool.name,
                 tool_params=tool_params,
                 tool_calls=previous_tool_calls,
             )
+            if not tool_response:
+                tool_response = tool.invoke(tool_params, tool_number=tool_number)
 
-        if not tool_response:
-            tool_response = self._directly_invoke_tool_call(
-                tool_name=tool_name,
-                tool_params=tool_params,
-                user_approved=user_approved,
-                tool_number=tool_number,
+            if not isinstance(tool_response, StructuredToolResult):
+                # Should never be needed but ensure Holmes does not crash if one of the tools does not return the right type
+                logging.error(
+                    f"Tool {tool.name} return type is not StructuredToolResult. Nesting the tool result into StructuredToolResult..."
+                )
+                tool_response = StructuredToolResult(
+                    status=ToolResultStatus.SUCCESS,
+                    data=tool_response,
+                    params=tool_params,
+                )
+
+            # Log tool execution to trace span
+            tool_span.log(
+                input=tool_params,
+                output=tool_response.data,
+                metadata={
+                    "status": tool_response.status.value,
+                    "error": tool_response.error,
+                    "description": tool.get_parameterized_one_liner(tool_params),
+                    "structured_tool_result": tool_response,
+                },
             )
 
-        if not isinstance(tool_response, StructuredToolResult):
-            # Should never be needed but ensure Holmes does not crash if one of the tools does not return the right type
+        except Exception as e:
             logging.error(
-                f"Tool {tool_name} return type is not StructuredToolResult. Nesting the tool result into StructuredToolResult..."
+                f"Tool call to {tool_name} failed with an Exception", exc_info=True
             )
             tool_response = StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
-                data=tool_response,
+                status=ToolResultStatus.ERROR,
+                error=f"Tool call failed: {e}",
                 params=tool_params,
             )
 
-        tool = self.tool_executor.get_tool_by_name(tool_name)
-
+            # Log error to trace span
+            tool_span.log(
+                input=tool_params, output=str(e), metadata={"status": "ERROR"}
+            )
+        finally:
+            # End tool span
+            tool_span.end()
         return ToolCallResult(
             tool_call_id=tool_call_id,
             tool_name=tool_name,
-            description=str(tool.get_parameterized_one_liner(tool_params))
-            if tool
-            else "",
+            description=tool.get_parameterized_one_liner(tool_params),
             result=tool_response,
         )
 
-    @staticmethod
-    def _log_tool_call_result(tool_span, tool_call_result: ToolCallResult):
-        tool_span.set_attributes(name=tool_call_result.tool_name)
-        tool_span.log(
-            input=tool_call_result.result.params,
-            output=tool_call_result.result.data,
-            error=tool_call_result.result.error,
-            metadata={
-                "status": tool_call_result.result.status,
-                "description": tool_call_result.description,
-            },
-        )
-
-    def _invoke_llm_tool_call(
-        self,
-        tool_to_call: ChatCompletionMessageToolCall,
-        previous_tool_calls: list[dict],
-        trace_span=None,
-        tool_number=None,
-        user_approved: bool = False,
-    ) -> ToolCallResult:
-        if trace_span is None:
-            trace_span = DummySpan()
-        with trace_span.start_span(type="tool") as tool_span:
-            if not hasattr(tool_to_call, "function"):
-                # Handle the union type - ChatCompletionMessageToolCall can be either
-                # ChatCompletionMessageFunctionToolCall (with 'function' field and type='function')
-                # or ChatCompletionMessageCustomToolCall (with 'custom' field and type='custom').
-                # We use hasattr to check for the 'function' attribute as it's more flexible
-                # and doesn't require importing the specific type.
-                tool_name = "Unknown_Custom_Tool"
-                logging.error(f"Unsupported custom tool call: {tool_to_call}")
-                tool_call_result = ToolCallResult(
-                    tool_call_id=tool_to_call.id,
-                    tool_name=tool_name,
-                    description="NA",
-                    result=StructuredToolResult(
-                        status=StructuredToolResultStatus.ERROR,
-                        error="Custom tool calls are not supported",
-                        params=None,
-                    ),
-                )
-            else:
-                tool_name = tool_to_call.function.name
-                tool_arguments = tool_to_call.function.arguments
-                tool_id = tool_to_call.id
-                tool_call_result = self._get_tool_call_result(
-                    tool_id,
-                    tool_name,
-                    tool_arguments,
-                    previous_tool_calls=previous_tool_calls,
-                    tool_number=tool_number,
-                    user_approved=user_approved,
-                )
-
-            prevent_overly_big_tool_response(
-                tool_call_result=tool_call_result, llm=self.llm
-            )
-
-            ToolCallingLLM._log_tool_call_result(tool_span, tool_call_result)
-            return tool_call_result
-
-    def _handle_tool_call_approval(
-        self,
-        tool_call_result: ToolCallResult,
-        tool_number: Optional[int],
-    ) -> ToolCallResult:
-        """
-        Handle approval for a single tool call if required.
-
-        Args:
-            tool_call_result: A single tool call result that may require approval
-            tool_number: The tool call number
-
-        Returns:
-            Updated tool call result with approved/denied status
-        """
-
-        # If no approval callback, convert to ERROR because it is assumed the client may not be able to handle approvals
-        if not self.approval_callback:
-            tool_call_result.result.status = StructuredToolResultStatus.ERROR
-            return tool_call_result
-
-        # Get approval from user
-        approved, feedback = self.approval_callback(tool_call_result.result)
-
-        if approved:
-            logging.debug(
-                f"User approved command: {tool_call_result.result.invocation}"
-            )
-            new_response = self._directly_invoke_tool_call(
-                tool_name=tool_call_result.tool_name,
-                tool_params=tool_call_result.result.params or {},
-                user_approved=True,
-                tool_number=tool_number,
-            )
-            tool_call_result.result = new_response
-        else:
-            # User denied - update to error
-            feedback_text = f" User feedback: {feedback}" if feedback else ""
-            tool_call_result.result.status = StructuredToolResultStatus.ERROR
-            tool_call_result.result.error = (
-                f"User denied command execution.{feedback_text}"
-            )
-
-        return tool_call_result
-
     @staticmethod
     def __load_post_processing_user_prompt(
         input_prompt, investigation, user_prompt: Optional[str] = None
@@ -757,6 +660,17 @@ class ToolCallingLLM:
             logging.exception("Failed to run post processing", exc_info=True)
             return investigation, 0.0
 
+    @sentry_sdk.trace
+    def truncate_messages_to_fit_context(
+        self, messages: list, max_context_size: int, maximum_output_token: int
+    ) -> list:
+        return truncate_messages_to_fit_context(
+            messages,
+            max_context_size,
+            maximum_output_token,
+            self.llm.count_tokens_for_message,
+        )
+
     def call_stream(
         self,
         system_prompt: str = "",
@@ -764,55 +678,48 @@ class ToolCallingLLM:
         response_format: Optional[Union[dict, Type[BaseModel]]] = None,
         sections: Optional[InputSectionsDataType] = None,
         msgs: Optional[list[dict]] = None,
-        enable_tool_approval: bool = False,
-        tool_decisions: List[ToolApprovalDecision] | None = None,
     ):
         """
         This function DOES NOT call llm.completion(stream=true).
         This function streams holmes one iteration at a time instead of waiting for all iterations to complete.
         """
-
-        # Process tool decisions if provided
-        if msgs and tool_decisions:
-            logging.info(f"Processing {len(tool_decisions)} tool decisions")
-            msgs, events = self.process_tool_decisions(msgs, tool_decisions)
-            yield from events
-
-        messages: list[dict] = []
+        messages = []
         if system_prompt:
             messages.append({"role": "system", "content": system_prompt})
         if user_prompt:
             messages.append({"role": "user", "content": user_prompt})
         if msgs:
             messages.extend(msgs)
+        perf_timing = PerformanceTiming("tool_calling_llm.call")
         tool_calls: list[dict] = []
         tools = self.tool_executor.get_all_tools_openai_format(
             target_model=self.llm.model
         )
+        perf_timing.measure("get_all_tools_openai_format")
         max_steps = self.max_steps
-        metadata: Dict[Any, Any] = {}
         i = 0
         tool_number_offset = 0
 
         while i < max_steps:
             i += 1
+            perf_timing.measure(f"start iteration {i}")
             logging.debug(f"running iteration {i}")
 
             tools = None if i == max_steps else tools
-            tool_choice = "auto" if tools else None
-
-            limit_result = limit_input_context_window(
-                llm=self.llm, messages=messages, tools=tools
-            )
-            yield from limit_result.events
-            messages = limit_result.messages
-            metadata = metadata | limit_result.metadata
-
-            if (
-                limit_result.conversation_history_compacted
-                and RESET_REPEATED_TOOL_CALL_CHECK_AFTER_COMPACTION
-            ):
-                tool_calls = []
+            tool_choice = os.environ.get("HOLMES_TOOL_CHOICE", "auto") if tools else None
+            logging.debug(f"HOLMES_TOOL_CHOICE env var: {os.environ.get('HOLMES_TOOL_CHOICE', 'NOT_SET')}, using tool_choice: {tool_choice}")
+
+            total_tokens = self.llm.count_tokens_for_message(messages)  # type: ignore
+            max_context_size = self.llm.get_context_window_size()
+            maximum_output_token = self.llm.get_maximum_output_token()
+            perf_timing.measure("count tokens")
+
+            if (total_tokens + maximum_output_token) > max_context_size:
+                logging.warning("Token limit exceeded. Truncating tool responses.")
+                messages = self.truncate_messages_to_fit_context(
+                    messages, max_context_size, maximum_output_token
+                )
+                perf_timing.measure("truncate_messages_to_fit_context")
 
             logging.debug(f"sending messages={messages}\n\ntools={tools}")
             try:
@@ -829,6 +736,7 @@ class ToolCallingLLM:
                 # Log cost information for this iteration (no accumulation in streaming)
                 _process_cost_info(full_response, log_prefix="LLM iteration")
 
+                perf_timing.measure("llm.completion")
             # catch a known error that occurs with Azure and replace the error message with something more obvious to the user
             except BadRequestError as e:
                 if "Unrecognized request arguments supplied: tool_choice, tools" in str(
@@ -850,10 +758,9 @@ class ToolCallingLLM:
 
                 if incorrect_tool_call:
                     logging.warning(
-                        "Detected incorrect tool call. Structured output will be disabled. This can happen on models that do not support tool calling. For Azure AI, make sure the model name contains 'gpt-4.1' or other structured output compatible models. To disable this holmes behaviour, set REQUEST_STRUCTURED_OUTPUT_FROM_LLM to `false`."
+                        "Detected incorrect tool call. Structured output will be disabled. This can happen on models that do not support tool calling. For Azure AI, make sure the model name contains 'gpt-4o'. To disable this holmes behaviour, set REQUEST_STRUCTURED_OUTPUT_FROM_LLM to `false`."
                     )
                     # disable structured output going forward and and retry
-                    sentry_helper.capture_structured_output_incorrect_tool_call()
                     response_format = None
                     max_steps = max_steps + 1
                     continue
@@ -864,25 +771,11 @@ class ToolCallingLLM:
                 )
             )
 
-            tokens = self.llm.count_tokens(messages=messages, tools=tools)
-            add_token_count_to_metadata(
-                tokens=tokens,
-                full_llm_response=full_response,
-                max_context_size=limit_result.max_context_size,
-                maximum_output_token=limit_result.maximum_output_token,
-                metadata=metadata,
-            )
-            yield build_stream_event_token_count(metadata=metadata)
-
             tools_to_call = getattr(response_message, "tool_calls", None)
             if not tools_to_call:
                 yield StreamMessage(
                     event=StreamEvents.ANSWER_END,
-                    data={
-                        "content": response_message.content,
-                        "messages": messages,
-                        "metadata": metadata,
-                    },
+                    data={"content": response_message.content, "messages": messages},
                 )
                 return
 
@@ -891,30 +784,22 @@ class ToolCallingLLM:
             if reasoning or message:
                 yield StreamMessage(
                     event=StreamEvents.AI_MESSAGE,
-                    data={
-                        "content": message,
-                        "reasoning": reasoning,
-                        "metadata": metadata,
-                    },
+                    data={"content": message, "reasoning": reasoning},
                 )
 
-            # Check if any tools require approval first
-            pending_approvals = []
-            approval_required_tools = []
-
+            perf_timing.measure("pre-tool-calls")
             with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
                 futures = []
                 for tool_index, t in enumerate(tools_to_call, 1):  # type: ignore
-                    tool_number = tool_number_offset + tool_index
-
-                    future = executor.submit(
-                        self._invoke_llm_tool_call,
-                        tool_to_call=t,  # type: ignore
-                        previous_tool_calls=tool_calls,
-                        trace_span=DummySpan(),  # Streaming mode doesn't support tracing yet
-                        tool_number=tool_number,
+                    futures.append(
+                        executor.submit(
+                            self._invoke_tool,
+                            tool_to_call=t,  # type: ignore
+                            previous_tool_calls=tool_calls,
+                            trace_span=DummySpan(),  # Streaming mode doesn't support tracing yet
+                            tool_number=tool_number_offset + tool_index,
+                        )
                     )
-                    futures.append(future)
                     yield StreamMessage(
                         event=StreamEvents.START_TOOL,
                         data={"tool_name": t.function.name, "id": t.id},
@@ -923,72 +808,15 @@ class ToolCallingLLM:
                 for future in concurrent.futures.as_completed(futures):
                     tool_call_result: ToolCallResult = future.result()
 
-                    if (
-                        tool_call_result.result.status
-                        == StructuredToolResultStatus.APPROVAL_REQUIRED
-                    ):
-                        if enable_tool_approval:
-                            pending_approvals.append(
-                                PendingToolApproval(
-                                    tool_call_id=tool_call_result.tool_call_id,
-                                    tool_name=tool_call_result.tool_name,
-                                    description=tool_call_result.description,
-                                    params=tool_call_result.result.params or {},
-                                )
-                            )
-                            approval_required_tools.append(tool_call_result)
-
-                            yield StreamMessage(
-                                event=StreamEvents.TOOL_RESULT,
-                                data=tool_call_result.as_streaming_tool_result_response(),
-                            )
-                        else:
-                            tool_call_result.result.status = (
-                                StructuredToolResultStatus.ERROR
-                            )
-                            tool_call_result.result.error = f"Tool call rejected for security reasons: {tool_call_result.result.error}"
-
-                            tool_calls.append(
-                                tool_call_result.as_tool_result_response()
-                            )
-                            messages.append(tool_call_result.as_tool_call_message())
-
-                            yield StreamMessage(
-                                event=StreamEvents.TOOL_RESULT,
-                                data=tool_call_result.as_streaming_tool_result_response(),
-                            )
-
-                    else:
-                        tool_calls.append(tool_call_result.as_tool_result_response())
-                        messages.append(tool_call_result.as_tool_call_message())
-
-                        yield StreamMessage(
-                            event=StreamEvents.TOOL_RESULT,
-                            data=tool_call_result.as_streaming_tool_result_response(),
-                        )
+                    tool_calls.append(tool_call_result.as_tool_result_response())
+                    messages.append(tool_call_result.as_tool_call_message())
 
-                # If we have approval required tools, end the stream with pending approvals
-                if pending_approvals:
-                    # Add assistant message with pending tool calls
-                    for result in approval_required_tools:
-                        tool_call = self.find_assistant_tool_call_request(
-                            tool_call_id=result.tool_call_id, messages=messages
-                        )
-                        tool_call["pending_approval"] = True
+                    perf_timing.measure(f"tool completed {tool_call_result.tool_name}")
 
-                    # End stream with approvals required
                     yield StreamMessage(
-                        event=StreamEvents.APPROVAL_REQUIRED,
-                        data={
-                            "content": None,
-                            "messages": messages,
-                            "pending_approvals": [
-                                approval.model_dump() for approval in pending_approvals
-                            ],
-                            "requires_approval": True,
-                        },
+                        event=StreamEvents.TOOL_RESULT,
+                        data=tool_call_result.as_streaming_tool_result_response(),
                     )
-                    return
 
                 # Update the tool number offset for the next iteration
                 tool_number_offset += len(tools_to_call)
@@ -997,21 +825,6 @@ class ToolCallingLLM:
             f"Too many LLM calls - exceeded max_steps: {i}/{self.max_steps}"
         )
 
-    def find_assistant_tool_call_request(
-        self, tool_call_id: str, messages: list[dict[str, Any]]
-    ) -> dict[str, Any]:
-        for message in messages:
-            if message.get("role") == "assistant":
-                for tool_call in message.get("tool_calls", []):
-                    if tool_call.get("id") == tool_call_id:
-                        return tool_call
-
-        # Should not happen unless there is a bug.
-        # If we are here
-        raise Exception(
-            f"Failed to find assistant request for a tool_call in conversation history. tool_call_id={tool_call_id}"
-        )
-
 
 # TODO: consider getting rid of this entirely and moving templating into the cmds in holmes_cli.py
 class IssueInvestigator(ToolCallingLLM):
@@ -1044,9 +857,8 @@ class IssueInvestigator(ToolCallingLLM):
         post_processing_prompt: Optional[str] = None,
         sections: Optional[InputSectionsDataType] = None,
         trace_span=DummySpan(),
-        runbooks: Optional[RunbookCatalog] = None,
     ) -> LLMResult:
-        issue_runbooks = self.runbook_manager.get_instructions_for_issue(issue)
+        runbooks = self.runbook_manager.get_instructions_for_issue(issue)
 
         request_structured_output_from_llm = True
         response_format = None
@@ -1074,9 +886,12 @@ class IssueInvestigator(ToolCallingLLM):
         else:
             logging.info("Structured output is disabled for this request")
 
+        if instructions is not None and instructions.instructions:
+            runbooks.extend(instructions.instructions)
+
         if console and runbooks:
             console.print(
-                f"[bold]Analyzing with {len(issue_runbooks)} runbooks: {issue_runbooks}[/bold]"
+                f"[bold]Analyzing with {len(runbooks)} runbooks: {runbooks}[/bold]"
             )
         elif console:
             console.print(
@@ -1091,20 +906,29 @@ class IssueInvestigator(ToolCallingLLM):
                 "structured_output": request_structured_output_from_llm,
                 "toolsets": self.tool_executor.toolsets,
                 "cluster_name": self.cluster_name,
-                "runbooks_enabled": True if runbooks else False,
             },
         )
 
+        if instructions is not None and len(instructions.documents) > 0:
+            docPrompts = []
+            for document in instructions.documents:
+                docPrompts.append(
+                    f"* fetch information from this URL: {document.url}\n"
+                )
+            runbooks.extend(docPrompts)
+
         user_prompt = ""
+        if runbooks:
+            for runbook_str in runbooks:
+                user_prompt += f"* {runbook_str}\n"
 
-        user_prompt = add_runbooks_to_user_prompt(
-            user_prompt,
-            runbook_catalog=runbooks,
-            global_instructions=global_instructions,
-            issue_instructions=issue_runbooks,
-            resource_instructions=instructions,
+            user_prompt = f'My instructions to check \n"""{user_prompt}"""'
+
+        user_prompt = add_global_instructions_to_user_prompt(
+            user_prompt, global_instructions
         )
-        user_prompt = f"{user_prompt}\n #This is context from the issue:\n{issue.raw}"
+        user_prompt = f"{user_prompt}\n This is context from the issue {issue.raw}"
+
         logging.debug(
             "Rendered system prompt:\n%s", textwrap.indent(system_prompt, "    ")
         )
@@ -1118,5 +942,5 @@ class IssueInvestigator(ToolCallingLLM):
             sections=sections,
             trace_span=trace_span,
         )
-        res.instructions = issue_runbooks
+        res.instructions = runbooks
         return res
diff --git a/holmes/plugins/prompts/generic_ask.jinja2 b/holmes/plugins/prompts/generic_ask.jinja2
index bf607ec8..23f05090 100644
--- a/holmes/plugins/prompts/generic_ask.jinja2
+++ b/holmes/plugins/prompts/generic_ask.jinja2
@@ -5,13 +5,43 @@ Do not say 'based on the tool output' or explicitly refer to tools at all.
 If you output an answer and then realize you need to call more tools or there are possible next steps, you may do so by calling tools at that point in time.
 If you have a good and concrete suggestion for how the user can fix something, tell them even if not asked explicitly
 
+CRITICAL: After calling tools, you MUST provide a clear, natural language answer to the user's question. Do not just output tool calls - always follow up with the actual answer or information the user requested.
+
+CRITICAL JSON PROHIBITION: NEVER output raw JSON tool calls as your final response. Tool calls are internal operations. Your final response must ALWAYS be natural language text that directly answers the user's question. For example:
+- If user asks "how many pods?" and tools show 14 pods, respond: "There are 14 pods in the namespace."
+- NEVER respond with: {"name": "kubectl_get_by_kind_in_namespace", "arguments": {...}}
+
+CRITICAL CONCISENESS REQUIREMENT: Keep your answers direct and concise. When users ask specific diagnostic questions:
+- Give the core answer first, then brief supporting details if needed
+- If asked "what's wrong with X?", lead with the specific issue: "The pod was killed due to out of memory"
+- Avoid lengthy explanations unless specifically requested
+- Be direct and actionable rather than verbose
+
+IMPORTANT: Your response workflow should be:
+1. Call necessary tools
+2. Wait for tool results 
+3. Provide a clear, concise answer based on the tool results
+4. Do NOT stop after just making tool calls
+5. NEVER output JSON - only natural language answers
+
 If you are unsure about the answer to the user's request or how to satisfy their request, you should gather more information. This can be done by asking the user for more information.
 Bias towards not asking the user for help if you can find the answer yourself.
 
+{% include '_current_date_time.jinja2' %}
+
 Use conversation history to maintain continuity when appropriate, ensuring efficiency in your responses.
 
 {% include '_general_instructions.jinja2' %}
 
+{% include '_runbook_instructions.jinja2' %}
+
+# Accuracy Requirements
+
+* When counting items, count carefully. Don't guess or estimate.
+* Double-check numbers against tool output.
+* If you say "X items", make sure you actually found X items.
+* Be precise with numerical data.
+
 # Style guide
 
 * Reply with terse output.
@@ -34,8 +64,11 @@ Relevant logs:
 
 Validation error led to unhandled Java exception causing a crash.
 
+User: How many pods are in the test-1 namespace?
+(Call tool kubectl_get_by_kind_in_namespace kind=pods namespace=test-1)
+
+AI: 14 pods in test-1 namespace.
+
 {% if system_prompt_additions %}
 {{ system_prompt_additions }}
 {% endif %}
-
-{% include '_current_date_time.jinja2' %}
diff --git a/holmes/plugins/toolsets/investigator/core_investigation.py b/holmes/plugins/toolsets/investigator/core_investigation.py
index ddac7f56..2f99fad9 100644
--- a/holmes/plugins/toolsets/investigator/core_investigation.py
+++ b/holmes/plugins/toolsets/investigator/core_investigation.py
@@ -3,39 +3,20 @@ import os
 from typing import Any, Dict
 
 from uuid import uuid4
-
 from holmes.core.todo_tasks_formatter import format_tasks
 from holmes.core.tools import (
-    StructuredToolResult,
-    StructuredToolResultStatus,
-    Tool,
-    ToolInvokeContext,
-    ToolParameter,
     Toolset,
     ToolsetTag,
+    ToolParameter,
+    Tool,
+    StructuredToolResult,
+    ToolResultStatus,
 )
 from holmes.plugins.toolsets.investigator.model import Task, TaskStatus
 
-TODO_WRITE_TOOL_NAME = "TodoWrite"
-
-
-def parse_tasks(todos_data: Any) -> list[Task]:
-    tasks = []
-
-    for todo_item in todos_data:
-        if isinstance(todo_item, dict):
-            task = Task(
-                id=todo_item.get("id", str(uuid4())),
-                content=todo_item.get("content", ""),
-                status=TaskStatus(todo_item.get("status", "pending")),
-            )
-            tasks.append(task)
-
-    return tasks
-
 
 class TodoWriteTool(Tool):
-    name: str = TODO_WRITE_TOOL_NAME
+    name: str = "TodoWrite"
     description: str = "Save investigation tasks to break down complex problems into manageable sub-tasks. ALWAYS provide the COMPLETE list of all tasks, not just the ones being updated."
     parameters: Dict[str, ToolParameter] = {
         "todos": ToolParameter(
@@ -47,11 +28,7 @@ class TodoWriteTool(Tool):
                 properties={
                     "id": ToolParameter(type="string", required=True),
                     "content": ToolParameter(type="string", required=True),
-                    "status": ToolParameter(
-                        type="string",
-                        required=True,
-                        enum=["pending", "in_progress", "completed"],
-                    ),
+                    "status": ToolParameter(type="string", required=True),
                 },
             ),
         ),
@@ -80,28 +57,39 @@ class TodoWriteTool(Tool):
         content_width = max(max_content_width, len("Content"))
         status_width = max(max_status_display_width, len("Status"))
 
+        # Build table
         separator = f"+{'-' * (id_width + 2)}+{'-' * (content_width + 2)}+{'-' * (status_width + 2)}+"
         header = f"| {'ID':<{id_width}} | {'Content':<{content_width}} | {'Status':<{status_width}} |"
-        tasks_to_display = []
+
+        # Log the table
+        logging.info("Updated Investigation Tasks:")
+        logging.info(separator)
+        logging.info(header)
+        logging.info(separator)
 
         for task in tasks:
             status_display = f"{status_icons[task.status.value]} {task.status.value}"
             row = f"| {task.id:<{id_width}} | {task.content:<{content_width}} | {status_display:<{status_width}} |"
-            tasks_to_display.append(row)
+            logging.info(row)
 
-        logging.info(
-            f"Task List:\n{separator}\n{header}\n{separator}\n"
-            + "\n".join(tasks_to_display)
-            + f"\n{separator}"
-        )
+        logging.info(separator)
 
-    def _invoke(self, params: dict, context: ToolInvokeContext) -> StructuredToolResult:
+    def _invoke(self, params: Dict) -> StructuredToolResult:
         try:
             todos_data = params.get("todos", [])
 
-            tasks = parse_tasks(todos_data=todos_data)
+            tasks = []
+
+            for todo_item in todos_data:
+                if isinstance(todo_item, dict):
+                    task = Task(
+                        id=todo_item.get("id", str(uuid4())),
+                        content=todo_item.get("content", ""),
+                        status=TaskStatus(todo_item.get("status", "pending")),
+                    )
+                    tasks.append(task)
 
-            logging.debug(f"Tasks: {len(tasks)}")
+            logging.info(f"Tasks: {len(tasks)}")
 
             self.print_tasks_table(tasks)
             formatted_tasks = format_tasks(tasks)
@@ -113,7 +101,7 @@ class TodoWriteTool(Tool):
                 response_data += "No tasks currently in the investigation plan."
 
             return StructuredToolResult(
-                status=StructuredToolResultStatus.SUCCESS,
+                status=ToolResultStatus.SUCCESS,
                 data=response_data,
                 params=params,
             )
@@ -121,13 +109,14 @@ class TodoWriteTool(Tool):
         except Exception as e:
             logging.exception("error using todowrite tool")
             return StructuredToolResult(
-                status=StructuredToolResultStatus.ERROR,
+                status=ToolResultStatus.ERROR,
                 error=f"Failed to process tasks: {str(e)}",
                 params=params,
             )
 
     def get_parameterized_one_liner(self, params: Dict) -> str:
-        return "Update investigation tasks"
+        todos = params.get("todos", [])
+        return f"Write {todos} investigation tasks"
 
 
 class CoreInvestigationToolset(Toolset):
@@ -137,11 +126,12 @@ class CoreInvestigationToolset(Toolset):
         super().__init__(
             name="core_investigation",
             description="Core investigation tools for task management and planning",
-            enabled=True,
+            enabled=False,  # Modified for KAITO compatibility - allow config override
             tools=[TodoWriteTool()],
             tags=[ToolsetTag.CORE],
-            is_default=True,
+            is_default=False,  # Modified for KAITO compatibility - allow config override
         )
+        logging.info("Core investigation toolset loaded")
 
     def get_example_config(self) -> Dict[str, Any]:
         return {}
diff --git a/super-minimal-config.yaml b/super-minimal-config.yaml
new file mode 100644
index 00000000..923fcd39
--- /dev/null
+++ b/super-minimal-config.yaml
@@ -0,0 +1,25 @@
+toolsets:
+  kubernetes/core:
+    enabled: true
+  core_investigation:
+    enabled: false
+  kubernetes/logs:
+    enabled: false
+  bash:
+    enabled: false
+  helm/core:
+    enabled: false
+  aks/core:
+    enabled: true
+  aks/node-health:
+    enabled: false
+  kubernetes/kube-prometheus-stack:
+    enabled: false
+  kubernetes/live-metrics:
+    enabled: false
+  datadog/rds:
+    enabled: false
+  internet:
+    enabled: false
+  runbook:
+    enabled: false
\ No newline at end of file
